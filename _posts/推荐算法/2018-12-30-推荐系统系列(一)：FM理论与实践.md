---
title: 推荐系统系列(一)：FM理论与实践
date: 2018-12-30
categories:
- 推荐算法
tags:
- 推荐系统
---

# 背景

在推荐领域CTR（click-through rate）预估任务中，最常用到的baseline模型就是LR（Logistic Regression）。对数据进行特征工程，构造出大量单特征，编码之后送入模型。这种线性模型的优势在于，运算速度快可解释性强，在特征挖掘完备且训练数据充分的前提下能够达到一定精度。但这种模型的缺点也是较为明显的：

<!-- more -->

* 模型并未考虑到特征之间的关系 $ y = w_0 + \sum_{i=1}^n w_i * x_i$。在实践经验中，对特征进行交叉组合往往能够更好地提升模型效果。  
* 对于多取值的categorical特征进行one-hot编码，具有高度稀疏性，带来维度灾难问题。

FM（Factorization Machine）模型就是针对在特征组合过程中遇到的上述问题而提出的一种高效的解决方案[1]。由于FM优越的性能表现，后续出现了一系列FM变种模型，从浅层模型到深度推荐模型中都有FM的影子。

# 分析

## FM定义

FM以特征组合进行切入点，在公式定义中引入特征交叉项，弥补了一般线性模型未考虑特征间关系的缺憾。公式如下（FM模型可拓展到高阶，但为简化且不失一般性，这里只讨论二阶交叉）[1]：

$$ y = w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^nw_{ij}x_i x_j \tag{1} $$

与一般线性模型相比，公式（1）仅多了一个二阶交叉项，模型参数多了 $\frac{n(n+1)}{2}$ 个。虽然这种显式交叉的方式能够刻画特征间关系，但是对公式求解带来困难。

因为大量特征进行one-hot表示之后具有高度稀疏性的问题，所以公式（1）中的 $x_i x_j$ 同样会产生大量的0值。参数学习不充分，直接导致 $w_{ij}$ 无法通过训练得到。（解释：令 $x_i x_j = X$，则 $\frac{\partial y}{\partial w_{ij}} = X$，又因 $X = 0$，所以 $w_{ij}^{new} = w_{ij}^{old} + \alpha X = w_{ij}^{new}$，梯度为0参数无法更新。）

导致这种情况出现的根源在于：特征过于稀疏。我们期望的是找到一种方法，使得 $w_{ij}$ 的求解不受特征稀疏性的影响。

## 公式改写

为了克服上述困难，需要对FM公式进行改写，使得求解更加顺利。受 矩阵分解 的启发，对于每一个特征 $x_i$ 引入辅助向量（隐向量）$V_i = (v_{i1}，v_{i2}，...，v_{ik})$，然后利用 $V_i V_j^T$ 对 $w_{ij}$ 进行求解。即，做如下假设： $w_{ij} \approx V_i V_j^T$。

引入隐向量的好处是：

* 二阶项的参数量由原来的 $\frac{n(n-1)}{2}$ 降为 kn。

* 原先参数之间并无关联关系，但是现在通过隐向量可以建立关系。如，之前 $w_{ij}$ 与 $w_{ik}$ 无关，但是现在 $w_{ij} = <V_i，V_j>，w_{ik} = <V_i，V_k>$ 两者有共同的 $V_i$，也就是说，所有包含 $x_i x_j$ 的非零组合特征（存在某个 $j \neq i$，使得 $x_i x_j \neq 0$）的样本都可以用来学习隐向量 $V_i$，这很大程度上避免了数据稀疏性造成的影响。[2]

现在可以将公式（1）进行改写：

$$ y = w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n\langle V_i,V_j\rangle x_ix_j \tag{2} $$


