---
title: 推荐算法综述
date: 2018-12-29
categories:
- 推荐算法
tags:
- 推荐系统
---

# 背景

本文主要介绍推荐系统中传统机器学习算法与深度学习算法，写这篇文章的主要目的是对业界主流推荐算法的一些总结，让读者对主流推荐算法的底层实现有一定了解，从而在业务实践过程中更好地理解算法，运用算法。

<!-- more -->

文章总共分3部分，第1部分为传统机器学习算法的介绍，第2部分是深度学习基础知识介绍，第3部分是深度学习推荐算法的介绍。

其中，传统机器学习算法总共介绍了10种推荐算法，深度学习介绍了14种方法

# 传统机器学习算法

本章节将介绍10种常见的推荐算法，并且列举一些实际的例子，详细介绍如下：

### 基于CF的推荐算法

#### 算法简介

CF（协同过滤）简单来形容就是利用兴趣相投的原理进行推荐，协同过滤主要分两类，一类是基于物品的协同过滤算法，另一种是基于用户的协同过滤算法，这里主要介绍基于物品的协同过滤算法。

给定一批用户，及一批物品，记 $V_i$ 表示不同用户对物品的评分向量，那么物品 $i$ 与物品 $j$ 的相关性为：

$$ sim(i, j) = cos(\vec{V_i}, \vec{V_j}) = \frac{\vec{V_i} \cdot \vec{V_j}}{\| \vec{V_i} \| \|\vec{V_j}\|} $$

上述公式是利用余弦公式计算相关系数，相关系数的计算还有：杰卡德相关系数、皮尔逊相关系数等

计算用户 $u$ 对某一物品的偏好，记用户 $u$ 对物品 $i$ 的评分为 $score(u,i)$，用户 $u$ 对物品 $i$ 的协同过滤得分为 $rec(u,j)$。

$$ rec(u, j) = \sum_{i=1}^n score(u, j) \cdot sim(i, j) $$

#### 业务实践

以购物篮子为例，业务问题：根据用户的历史购买商品记录，给用户推荐一批商品。协同过滤算法实现方法如下。

* Step1：计算物品之间的相关系数

&emsp;&emsp;&emsp;记 $buyers_i$ 表示用户购买商品的向量，记 $buyers_i = (\ldots, b_{u,i}, \ldots), u \in U$，其中 $U$ 表示全库用户集合，$b_{u, i}$ 表示用户 $u$ 对商品 $i$ 的得分，定义如下：

$$ b_{u,i} = \left\{
\begin{array}{rcl}
0       &      & {表示没有购买}\\
\frac{1}{\alpha t}       &      & {其中t表示购买距离现在天数，\alpha 表示时间衰减系数}
\end{array} \right. 
$$

&emsp;&emsp;&emsp;那么商品 $i$ 与商品 $j$ 的相关系数如下：

$$ sim(i,j) = \frac{buyers_i \cdot buyers_j }{\| buyers_i\| \| buyers_j\|} $$

&emsp;&emsp;&emsp;上述公式是是利用余弦公式计算相关性，含义是商品的用户购买向量夹角越小越相似。此外也可以运用皮尔逊、杰卡德、自定义公式计算相关性，这里不一一列举。

* Step2：计算用户对商品的协同过滤得分

&emsp;&emsp;&emsp;给定一个用户 $u$，设该用户历史购买商品记录的向量为 $history_i = (\ldots, h_{u,i}, \ldots), i \in I$，其中 $I$ 表示所有商品的集合。

$$ h_{u, i} = \frac{1}{\alpha t}$$

&emsp;&emsp;&emsp;计算给定一个物品j的协同过滤得分为:

$$ rec(u,j) = \sum_{i=1}^n h_{u,i} \cdot sim(i,j)$$

* step3：给用户推荐商品

&emsp;&emsp;&emsp;通过Step2计算用户对全库商品的协同过滤得分，取得分top 10展示给用户

### 基于关联规则的推荐算法

#### 算法简介

基于关联规则的推荐是根据历史数据统计不同规则出现的关系，形如：X->Y，表示X事件发生后，Y事件会有一定概率发生，这个概率是通过历史数据统计而来。

对于一个规则X->Y，有两个指标对该规则进行衡量。一个是支持度，表示在所有样本数据中，同时包含X和Y样本的占比。另一个是置信度，表示在所有包含X的样本中，包含Y的样本占比。

在关联推荐算法中，最主要的是如何找到最大频繁项，业界主要的做法有两种，分别为Apriori算法和FP树。但在互联网海量的用户特征中，使用这些算法挖掘频繁集计算复杂度非常高，下面我们介绍一种在业务当中简单实用的关联规则算法。

#### 业务实践

同样是以购物篮子为例，业务场景是：根据用户历史购物记录，给用户推荐商品。下面我们介绍如何构建简单的关联规则推荐算法。

* Step1：数据准备

&emsp;&emsp;&emsp;首先收集用户展示购买记录，并且关联用户在展示时刻的特征数据。设总样本数量为n条，数据格式如下：

| 样本序号 | 用户 | 特征 | 商品 | 用户是否购买（0表示否，1表示是）|
| 1 | $u_1$ | $f_{1,1}, \, f_{1,2}, \, \ldots$| $i_1$ | $b_n$ |
| $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ |
| n | $u_n$ | $f_{n,1}, \, f_{n,2}, \, \ldots$ | $i_n$ | $b_n$ |

&emsp;&emsp;&emsp;其中用户特征可以是用户历史购买的商品ID，也可以是用户属性特征，例如：年龄、性别、居住地等等。

* Step2：特征交叉

&emsp;&emsp;&emsp;在上表中，对于同一个样本所有特征两两交叉，生成长度为2的特征规则，合并原来的长度为1的特征规则，得到关联规则数据输入表如下：

![avatar](https://upload-images.jianshu.io/upload_images/18360757-f4f63d1c2f974461?imageMogr2/auto-orient/strip|imageView2/2/w/601/format/webp)

&emsp;&emsp;&emsp;上述表中只用长度为1（原始特征）和2（原始特征两两交叉）的规则作为后面rule的候选集，不做长度为3的规则主要的考虑点是降低规则空间复杂度。

* Step3：生成关联规则

&emsp;&emsp;&emsp;首先把上表的特征展开，使得一个特征一条记录，如下表：

| 样本序号 | 用户 | 特征 | 商品 | 用户是否购买（0表示否，1表示是）|
| 1 | $u_1$ | $f_{1,1}$ | $i_1$ | $b_n$ |
| 1 | $u_1$ | $f_{1,2}$ | $i_1$ | $b_n$ |
| $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ |
| 1 | $u_1$ | $f_{1,1} \& f_{1,2}$ | $i_1$ | $b_n$ |
| $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ |
| k | $u_k$ | $f_{k,1} \& f_{k,2}$ | $i_k$ | $b_k$ |
| $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ |

&emsp;&emsp;&emsp;计算每个规则的支持度，置信度，提升度。首先作变量声明：

&emsp;&emsp;&emsp;$f$ -> $i$：表示具备特征 $f$ 的用户购买商品 $i$ 的事件  
&emsp;&emsp;&emsp;$s_{f,i}$：&nbsp;表示规则$f$ -> $i$的支持度  
&emsp;&emsp;&emsp;$c_{f,i}$：&nbsp;表示规则$f$ -> $i$的置信度

&emsp;&emsp;&emsp;$s_{f,i}$ 计算方法为：统计表3中中同时满足特征 = $f$，商品 = $i$，用户是否购买 = 0的记录条数记为 $notbuyers_{f,i}$

$$ c_{f,i} = \frac{buyers_{f,i}}{buyers_{f,i} + notbuyers_{f,i}} $$

&emsp;&emsp;&emsp;规则选择，规则可以通过以下条件进行过滤。

&emsp;&emsp;&emsp;条件1：大于等于某个值，参考值取20-100  
&emsp;&emsp;&emsp;条件2：对所有规则的支持度做降序，取75位数为参考值，$s_{f,i}$ 大于等于这个值  
&emsp;&emsp;&emsp;条件3：对所有规则的置信度做降序，取75位数为参考值，$c_{f,i}$ 大于等于这个值

* Step4：给用户推荐商品

&emsp;&emsp;&emsp;给定一个用户 $u$ 和一个商品 $i$，通过上述方法生成用户 $u$ 的特征集合记为 $F$。我们用该用户特征集合下，所有对 $i$ 有效特征的均值衡量用户 $u$ 对该物品的购买可能性 $p(u,i)$

$$ p(u,i) = avg_{f \in F}(c_{f,i}) $$

&emsp;&emsp;&emsp;通过上述公式对全库商品求top 10得分的商品推荐给用户。在实际计算当中，并非会进行全库计算，而是采用特征索引技术进行减少大量冗余计算。

### 基于bayes的推荐算法

#### 原理介绍

Bayes(贝叶斯)定理是关于随机事件A和B的条件概率相互转化的一则定理，贝叶斯公式如下：

![avatar](https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/pic/item/1ad5ad6eddc451da062e7cb7bafd5266d1163295.jpg)

上述公式中，$P(B_i\|u)$ 表示的含义是在发生了事件 $u$ 的情况下，发生事件 $B_i$ 的概率，$P(B_i)$ 表示事件 $B_i$ 的发生概率，$P(u)$ 表示事件 $u$ 的发生概率。

如何利用上述定理进行个性化推荐，下面我们举个业务实践的例子。

#### 业务实践

以应用商店中应用推荐为例，业务场景：当用户进入应用商店，根据用户已安装应用列表给用户推荐应用。

* Step1：问题分解

&emsp;&emsp;&emsp;给定一个用户 $u$，给该用户推荐应用 $B$，根据贝叶斯公式用户安装概率为：

$$ P(B|u) = \frac{P(B)P(u|B)}{P(u)} $$

&emsp;&emsp;&emsp;设用户的安装列表为 $\{A_1, \, A_2, \, \ldots, \, A_n\}$，把用户 $u$ 看作是事件 $\{A_1, \, A_2, \, \ldots, \, A_n\}$，为了简化问题，假设 $A_k$ 相互独立，那么

$$ P(u|B) = P(A_1|B)P(A_2|B) \ldots P(A_n|B) = \prod_{i=1}^n P(A_i|B)  $$

&emsp;&emsp;&emsp;上述式子可以化为


$$ P(B|u) = \frac{P(B) \prod_{i=1}^n P(A_i|B)}{P(u)} $$

&emsp;&emsp;&emsp;在推荐场景中，是对一个用户计算不同应用的得分，然后作降序进行推荐。而对于同一个用户 $P(u)$ 是不变的，所以我们可以用以下公式作为排序依据

$$ sortScore(u, B) = P(B) \prod_{i=1}^n P(A_i|B) $$

&emsp;&emsp;&emsp;全库的应用集合记为 $I$，所以在贝叶斯推荐模型中主要参数有两个集合，分别为

$$ \{P(B)|B \in I\} $$

$$ \{ P(A_i|B) | B \in I, \, A_i \in I, \, A_i \neq B \} $$

* Step2：数据准备

&emsp;&emsp;&emsp;首先收集历史的用户在应用商店中应用展示记录，并且关联用户在展示时刻的安装列表，数据格式如下：

| 样本序号 | 用户 | 已安装应用 | 展示应用 | 用户是否安装（0表示否，1表示是）|
| 1 | $u_1$ | $A_1, \, A_2, \, \ldots$| $B_1$ | $b_n$ |
| $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ | $\ldots$ |

* Step3：模型参数计算

&emsp;&emsp;&emsp;参数集合 $ \{P(B)\|B \in I\} $ 的计算：给定一个应用 $B$. 根据表1，首先选中 “展示应用=B” 的样本数，记为 $showNums_B$，然后计算 “展示应用=B” 且 “用户是否安装=1” 的样本数记为 $installNums_B$，那么

$$ P(B) = \frac{installNums_B}{ showNums_B } $$

&emsp;&emsp;&emsp;参数 $ \{ P(A_i\|B) \| B \in I, \, A_i \in I, \, A_i \neq B \} $，给定一个应用 $B$ 及 $A_i$，根据表，首先计算 “$A_i \in $已安装列表” 且 “展示应用=B” 的样本数，记为 $showNums_{A_i,B}$，然后计算 “$A_i \in $已安装列表” 且 “展示应用=B” 且 “用户是否安装=1” 的样本数，记为 $installNums_{A_i,B}$，那么

$$ P(A_i|B) = \frac{installNums_{A_i,B}}{showNums_{A_i,B}}$$

&emsp;&emsp;&emsp;在计算 $ P(A_i\|B)$ 可能会遇到样本不足的情况，导致计算出异常值，为了避免这类情况，需要根据经验加个最少安装数限制，这里我们定义为最少安装次数为100，那么：

$$ P(A_i|B) = \left\{
\begin{array}{rcl}
P(A_i)       &      & {installNums_{A_i,B} < 100}\\
\frac{installNums_{A_i,B}}{showNums_{A_i,B}}       &      & {installNums_{A_i,B} \geq 100}
\end{array} \right. 
$$

&emsp;&emsp;&emsp;其中 $P(A_i)$ 是表示在所有用户中，安装了应用 $A_i$ 用户的占比。

* Step4：给用户推荐应用

&emsp;&emsp;&emsp;给定一个用户 $u$，及一批候选推荐应用池，我们通过上述方法计算用户 $u$ 对候选池中每个应用的得分 $sortScore(u,B)$，根据这个值做降序，取top 10的应用推荐给用户。

### 基于KNN的推荐算法

#### 算法简介

KNN（K最近邻分类算法）是一种在机器学习中比较简单的算法，它的原理如下：对于一个需要分类的物品A，定义某一种刻画物品之间距离方法，找出该物品最邻近k个有已知类别的物品，这k物品中出现最多的类别即为物品A的类别。如下图：

![avatar](https://img-blog.csdnimg.cn/20190113190408164.png)

在KNN算中，最核心的一点是怎么定义物品之间的距离，这里我们简单列举几种计算物品距离的方法：欧式距离、曼哈顿距离、切比雪夫距离、杰卡德系数、夹角余弦、皮尔逊系数。

下面介绍KNN在实际业务中的运用。

#### 业务实践

**业务场景1**：以应用商店为例，在用户下载完一个应用时，触发一个“大家还下载”的推荐，下面介绍如何运用knn算法实现这个场景的推荐：

首先定义应用的维度向量，一种简单的方法是离散化所有特征，然后进行one-hot编码，得到所有维度取值0/1的向量V，例如：可以把每个用户当做一个维度，如果第n个用户安装了应用A，那么应用A在第n个维度取值为1，否则为0，运用欧式距离可以得到应用A与应用B的距离公式：

$$ d_{A,B} = |\vec{V_A} - \vec{V_B}| $$

给定一个应用A，通过上述公式取距离最小的4个应用出来，在用户下载完应用A以后给该用户推荐这4个应用

**业务场景2**：网络购物中，在“猜你喜欢”场景推荐一批物品给用户

通过用户的历史购物清单，运用杰卡德公式计算用户与用户的相关系数

$$sim(u,v) = \frac{ |A_u \bigcap A_v| }{|A_u \bigcup A_v|} \, \, \, \, A_x 表示购买了物品x的用户集合$$

那么用户u与用户v的距离定义为：

$$ d_{u,v} = 1 - sim(u,v) $$

给定一个用户u，首先找出这个用户最邻近的k个用户，然后在这k个用户中按照购买的用户数对物品进行降序，去除用户u已经购买的物品，取top 10个物品推荐给用户。

### 决策树算法

#### 算法简介

决策树一种经典的机器学习分类算法，主要的代表算法有ID3、C4.5、CARD，原理可以简单理解为通过对数据总结、归纳得到一系列分类规则，举个简单的例子：

![avatar](https://img2018.cnblogs.com/blog/1522247/201908/1522247-20190811094824649-1052242257.png)

在决策树中，一个叶子节点表示一条决策规则（通常叶子节点的类目等于该节点样本最多的类目），决策树算法的目标是得到准确率高的规则，而决策规则的准确率可以用叶子节点的复杂度来衡量。

#### 复杂度计算

下面列举2种常用复杂度的计算方法, 假设有样本集X，总共的类目有n个，pi表示第i个类目的占比。

（1）信息熵：

$$ H(X) = - \sum_{i=1}^n p_i log(p_i)$$

上式中，信息熵的值越高，复杂度越高，样本的不确定性越大。

（2）基尼指数：

$$ Gini(X) = 1 - \sum_{i=1}^n p_i^2 $$

上式中，基尼指数越大，复杂度越高，样本的不确定性也就越大。

#### 裂分指标

在决策树的生成过程中，每一个节点的裂分都需要考虑选择哪个属性裂分使得系统的复杂度降低越多。不同算法选用的裂分方法有所不同。

（1）ID3：信息增益

$$G(A) = H(X) - \frac{\sum_{i=1}^n |X_i|H(X_i)}{|X|} $$

 其中 $H(X)$ 表示裂分前系统的复杂度，$\frac{\sum_{i=1}^n \|X_i\|H(X_i)}{\|X\|}$ 表示裂分后系统的复杂度。该值越大表示裂分方式使得系统更为有序。
 
（2）C4.5：信息增益率

$$ GR(A) = \frac{G(A)}{SplitInfo_A(X)} $$

$$ SplitInfo_A(X) = - \sum_{i=1}^m p_{A,i} log(p_{A,i}) \, \, \, \, p_{A,i} 表示A属性第i个取值占比 $$

其中 $SplitInfo_A(X)$ 表示的意思是属性A的复杂度，该公式除了考虑系统纯度的增量的同时，也考虑了属性A的复杂度。该值越大表示裂分方式使得系统更为有序。（在ID3算法中，由于选择的是信息增益计算系统纯度增量，往往会选择复杂度高的属性进行裂分，复杂度高的属性取值分段会有很多，导致裂分后某些节点只有少量样本而不具备用于预测的统计意义，C4.5基于这个问题加以改进）

（3）CARD：基尼系数

$$ Gini(X|A) = \frac{|X_1|Gini(X_1)}{|X|} + \frac{|X_2|Gini(X_2)}{|X|} $$

CARD算法生成的决策树是一个二叉树，每次裂分只裂分两个节点，$Gini(X\|A)$ 表示裂分后的复杂度，该值越高样本无序性越大，$X_1,X_2$ 是 $X$ 的裂分后的两个样本集（裂分方法为遍历所有裂分可能，找出 $Gini(X\|A)$ 最小的那个点）。该值越小表示裂分方式使得系统更为有序。

#### 决策树生成

输入：$D = \{(x_1,c_1), (x_2,c_2), \ldots, (x_n,c_n)\}$

裂分指标：选择一种裂分指标（信息增益、信息增益率、Gini系数）

节点裂分终止条件：选择节点最小样本数及最大深度

* Step1：选择一个可裂分的节点 $D_i$，循环计算所有属性的裂分指标，选取最优的指标使得系统最为有序那个属性作为裂分点，得到数据集 $D_{i+1},D_{i+2},…$

* Step2：所有叶子节点是否都达到了裂分的终止条件，是则执行Step3，否则执行step1

* Step3：减枝

* Step4：返回决策树T

#### 业务实践

业务场景：以应用商店中应用个性化推荐为例

* Step1：构造用户画像，收集用户历史下载应用记录、已安装应用记录、用户社会属性（年龄、性别、学历、所在城市）

* Step2：构造应用画像，应用画像包括应用ID，应用类型、应用标签、应用安装量排名、应用CTR等

* Step3：样本收集，收集用户历史曝光下载应用记录（字段：用户ID、应用ID、是否下载），并通关用户ID、应用ID与用户画像、应用画像关联起来得到样本数据，得到样本数据（用户ID，应用ID，用户画像，应用画像，是否下载）

* Step4：构造模型训练样本，定义用户画像与应用画像不同类型特征的交叉规则生成模型特征，运用定义好的交叉规则对所有样本生成模型特征，得到模型训练样本（模型特征，是否下载）

* Step5：模型训练，模型训练样本训练CARD算法，得到预测模型

* Step6：模型使用，给定一个用户和应用，根据上述方法生成用户的用户画像及应用的应用画像，然后运用定义好的交叉特征规则生成模型特征，把模型特征代入模型得到预测值

### 随机森林算法

#### 算法简介

随机森林（RF）是决策树与bagging结合一种分类回归算法，它由多颗决策树构成的一个bagging决策系统。当运用RF进行预测时，首先把需要把样本数据输入到每一棵决策树，每个树得到一个叶子节点，预测的时候，如果是回归问题则统计所有树叶子节点的均值，如果是分类问题则求所有树叶子节点类目出现最多的那个类。

RF每棵决策树的构建方式如下：

* Step1：用M表示数据总特征维度，N表示样本数量，m表示特征抽样维度

* Step2：有放回随机抽取N个样本作为这个树的训练样本

* Step3：对训练样本构建决策树，每次裂分前随机抽取m个特征出来，裂分特征在这m个特征中选择一个最优的裂分特征

* Step4：不作减枝直到不能裂分为止

![avatar](https://img2018.cnblogs.com/blog/1522247/201908/1522247-20190811095037506-1708534947.png)

#### 业务实践
 
在实际的业务运用中与决策树类似，在前面介绍的决策树业务实践中可以直接用RF算法替代决策树，构造的方法如上所述，重复地随机抽样样本及抽样特征构造多颗决策树，决策树的棵数需要结合分类精度及模型复杂程度判断。

### 基于矩阵分解的推荐算法

#### 算法介绍

在推荐算法中，主要解决的问题是找到用户对物品的偏好得分。矩阵分解算法它的基本思想是认为用户对物品的偏好是外在表现，内在是用户对主题的偏好，而主题对不同物品又有不同的权重，通过用户->主题->物品这条链路才形成用户对物品的偏好。

矩阵分解的公式如下：

$$ U=PQ $$

其中 U 表示用户对不同物品的偏好矩阵，P 表示用户对不同主题的偏好矩阵，Q 表示不同主题对应用的权重

#### 模型求解

在实际的业务实践中，往往是已知用户对物品的部分偏好得分，求解用户对未知物品的偏好得分。以应用商店广告场景为例：已知用户在端内的物品曝光点击记录，求解用户对不同广告偏好得分。

* Step1：根据样本数据构造矩阵 U

&emsp;&emsp;&emsp;根据样本数据，用户对曝光物品有点击记为1，没有点击记为0，没有曝光过的物品不赋值（记为-），示例如下：

![avatar](https://img2018.cnblogs.com/blog/1522247/201908/1522247-20190811095047670-1318190373.png)

* Step2：求解矩阵P和矩阵Q

&emsp;&emsp;&emsp;设矩阵U的大小为 N×M，主题数定义为 K，那么矩阵的大小是 N×K，Q矩阵的大小是 K×M，构造损失函数，如下：

$$ log(P,Q) = \sum_{i=1}^N \sum_{j=1, u_{i,j} \in \{0,1\}}^M (u_{i,j} - p_i q_j)^2 $$

其中 $u_{i,j}$ 表示矩阵 $U$ 的第 i 行第 j列元素，$p_i$ 表示矩阵 P 的第 i 行，$q_j$ 表示矩阵 Q 的第 j 列，通过梯度下降法可以求解矩阵P和矩阵Q

* Step3：预测用户对没有曝光过物品的偏好得分

&emsp;&emsp;&emsp;给定一个用户 i，需要预测该用户对物品 j 的偏好得分，公式如下：

$$ \hat{u_{i,j}} = p_j q_j $$

Step4：如何给用户推荐物品

&emsp;&emsp;&emsp;给定一个用户，通过Step3的公式计算该用户对所有物品的偏好得分，取该用户没有曝光过得分排名前10的物品进行推荐。

### 基于BP的推荐算法

#### 算法介绍

BP算法是神经网络的一种算法，BP算法网络是有多层网络构成，信号的传播是由第一层网络开始正向传播到下一层网络。以3层神经网络为例，网络结构示例如下：

![avatar](https://img2018.cnblogs.com/blog/1522247/201908/1522247-20190811095115956-1355751636.png)

以3层神经网络关系如下：

$L_1 = f(X * w_1 + b_1) $  
$L_2 = f(L_1 * w_2 + b_2) $  
$L_3 = f(L_2 * w_3 + b_3) $

向量 X 是模型输入变量向量，$w_i$ 是 $L_{i-1}$ 层与 $L_i$ 的连接权重矩阵，$b_i$ 是 $L_i$ 的偏置向量。函数 $f$ 是一个激活函数，目前业界常用的激活函数有relu、sigmod、tanh，传统BP神经网络函数一般采用sigmod函数，如果采用该函数，那么：

![avatar](https://img2018.cnblogs.com/blog/1522247/201908/1522247-20190811095133481-678244111.png)

#### 模型求解

以个性化推荐场景中点击率预估为例，上述模型参数有 $w_1, w_2, w_3, b_1, b_2, b_3$，我们通过梯度下降法求解这些参数，首先收集样本，取历史用户推荐的数据及用户对推荐反馈的数据作为样本。变量定义如下：

* nums 表示收集样本的数量。

* $(X_i,y_i)$ 表示用户第 i 个样本的数据，$X_i$ 表示样本的特征，$y_i$ 表示点击情况(0表示没有点击，1表示点击)。

* $Y_i$ 表示上述模型构造中的输出值，是关于 $w_1, w_2, w_3, b_1, b_2, b_3$ 的变量。

损失函数：常用的的定义有两种，一种是交叉熵，另一种均方差，以均方差为例：

$$los(w_1, w_2, w_3, b_1, b_2, b_3) = \frac{1}{n} \sum_{i=1}^n (y_i - Y_i)^2 $$

通过上述损失函数，运用梯度下降法求解模型参数 $w_1, w_2, w_3, b_1, b_2, b_3$。

关于**交叉熵和均方差**的差异见[**文章**](https://blog.csdn.net/liuweiyuxiang/article/details/90707375)

### 基于W2V的推荐算法

#### 算法简介

W2V是在2013年由Google开源了一款用于词向量计算的工具，该算法提出的场景主要是解决NLP中词向量化的问题，传统对词向量的方法是one-hot编码，one-hot编码存在主要有两点，第一点维度极高无法直接作为模型的输入变量，第二点是词与词之间没有相关性。W2V的诞生解决了这两个问题，W2V是通过神经网络对词进行低纬度的向量化。

W2V有两种模型，一个是CBOW模型，另一个是Skip-gram模型。两个模型都是对词进行向量化，区别在于：CBOW是以一个词为输出目标，以该词邻近的词为输入；Skip-gram是以一个词为输入，以该词邻近的词为输出目标。示例如下：

![avatar](https://img2018.cnblogs.com/blog/1522247/201908/1522247-20190811095236887-880839011.png)

以CBOW模型为例，该模型的结构图如下：

![avatar](https://img2018.cnblogs.com/blog/1522247/201908/1522247-20190811095245449-1066875477.png)

各层关系为：

* Input层：以一个词为输出目标，以该词邻近的词向量为输入。

* Projection层：把Input层的所有向量叠加求和。

* Output层：首先对语料库中的所有词建立哈夫曼树编码（不使用one-hot编码，one-hot编码太稀疏）。然后为每在每个哈夫曼树节点建立一个逻辑斯蒂分类模型，模型的输入都是Projection的输出。

#### 模型训练

模型的参数，模型的参数包括所有词的词向量 和哈夫曼树中每个节点的逻辑斯蒂回归参数。

哈夫曼树中的每个节点都是一个逻辑斯蒂回归函数，以输出的词作为叶子节点路径中的每个节点的分类（路径走左分支为1，右分支非0）作为训练目标。例如：上图中输出词假设为“足球”，那么路径如下：

![avatar](https://img2018.cnblogs.com/blog/1522247/201908/1522247-20190811095257140-1609754.png)
