---
title: 推荐系统系列(四)：PNN理论与实践
date: 2018-12-30
categories:
- 推荐算法
tags:
- 推荐系统
---

# 背景

上一篇文章介绍了FNN [2]，在FM的基础上引入了DNN对特征进行高阶组合提高模型表现。但FNN并不是完美的，针对FNN的缺点上交与UCL于2016年联合提出一种新的改进模型PNN（Product-based Neural Network）。

<!--more-->

PNN同样引入了DNN对低阶特征进行组合，但与FNN不同，PNN并没有单纯使用全连接层来对低阶特征进行组合，而是设计了Product层对特征进行更细致的交叉运算。在《推荐系统系列(三)：FNN理论与实践》 中提到过，在不考虑激活函数的前提下，使用全连接的方式对特征进行组合，等价于将所有特征进行加权求和。PNN的作者同样意识到了这个问题，认为“加法”操作并不足以捕获不同Field特征之间的相关性。原文如下 [1]：

```
the “add” operations of the perceptron layer might not be useful to explore the interactions of categorical data in multiple fields.
```

有研究表明“product”操作比“add”操作更有效，而且FM模型的优势正是通过特征向量的内积来体现的。基于此，PNN作者设计了product layer来对特征进行组合，包含内积与外积两种操作。实验表明，PNN有显著提升，而product layer也成为了深度推荐模型中的经典结构。

# 分析

## PNN结构

PNN的网络架构如下图所示：

![avatar](/images/rec/rec-65.png)

从上往下进行分析，最上层输出的是预估的CTR值，$\hat{y}=\sigma(W_3l_2+b_3)$，公式符号与原Paper保持一致。

第二层隐藏层：$l_2=relu(W_2l_1+b_1)$

第一层隐藏层：$l_1=relu(l_p + l_z + b_1)$

PNN核心在于计算 $l_z，l_p$。首先，定义矩阵点积运算 $A \bigodot B \triangleq \sum_{i,j}A_{i,j}B_{i,j}$

则：

$$ \begin{align}
l_z=(l_{z}^1,l_{z}^2,\dots,l_{z}^n,\dots,l_{z}^{D_1}), \qquad l_z^n=W_z^n \bigodot z \notag \\
\end{align} \tag{1} $$

$$ \begin{align}
l_p=(l_{p}^1,l_{p}^2,\dots,l_{p}^n,\dots,l_{p}^{D_1}), \qquad l_p^n=W_p^n \bigodot p \notag \\
\end{align} \tag{2} $$

结合公式（1）（3），得：

$$ \begin{align}
l_z^n=W_z^n \bigodot z=\sum_{i=1}^N\sum_{j=1}^M(W_z^n)_{i,j}z_{i,j} \notag \\
\end{align} \tag{5} $$

公式（3）中，$f_i \in \mathbb{R}^M$ 表示经过embedding之后的特征向量，embedding过程与FNN保持一致。联系PNN结构图与公式（1）（3）可以看出，这个部分的计算主要是为了保留低阶特征，对比FNN丢弃低阶特征，只对二阶特征进行更高阶组合，显然PNN是为了弥补这个缺点。

公式（4）中，$p_{i,j}=g(f_i,f_j)$ 表示成对特征交叉函数，定义不同的交叉方式也就有不同的PNN结构。在论文中，函数 $g(f_i,f_j)$ 有两种表示，第一种为向量内积运算，即IPNN（Inner Product-based Neural Network）；第二种为向量外积运算，即OPNN（Outer Product-based Neural Network）。

### IPNN分析

定义 $p_{i,j}=g(f_i,f_j)=\langle f_i,f_j \rangle$，将公式（2）进行改写，得：

$$ \begin{align}
l_p^n=W_p^n \bigodot p=\sum_{i=1}^N\sum_{j=1}^N(W_p^n)_{i,j}p_{i,j}=
\sum_{i=1}^N\sum_{j=1}^N(W_p^n)_{i,j}\langle f_i,f_j \rangle \notag \\
\end{align} \tag{6} $$

* 分析IPNN的product layer计算空间复杂度：

结合公式（1）（5）可知，$l_z$ 计算空间复杂度为 $O(D_1NM)$。结合公式（2）（6）可知，计算 $p$ 需要 $O(N^2)$ 空间开销，$l_p^n$ 需要 $O(N^2)$ 空间开销，所以 $l_p$计算空间复杂度为 $O(D_1NN)$。所以，product layer 整体计算空间复杂度为 $O(D_1N(M + N))$。
