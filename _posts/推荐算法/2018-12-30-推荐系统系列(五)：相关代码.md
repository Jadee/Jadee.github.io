---
title: 推荐系统系列(五)：相关代码
date: 2018-12-30
categories:
- 推荐算法
tags:
- 推荐系统
---

# 环境配置

pandas==0.21

<!-- more -->

# FM

## eda.py

```
# -*- coding:utf-8 -*-
from collections import defaultdict

# u.item feat_name:
item_feat_names = ['item_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action',
                   'Adventure', 'Animation', 'Children','Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',
                   'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
# u.user feat_names:
user_feat_names = ['user_id', 'age', 'gender', 'occupation', 'zip_code']

# 统计每个特征字段取值
def featStat(file_path, mode='item'):
    feat_dict = defaultdict(set)
    feat_names = item_feat_names if mode == 'item' else user_feat_names
    encode = "ISO-8859-1" if mode == 'item' else 'utf-8'
    with open(file_path, 'r', encoding=encode) as f:
        line = f.readline()
        while line:
            feats = line.strip().split("|")
            for i in range(len(feats)):
                feat_dict[feat_names[i]].add(feats[i].strip())
            line = f.readline()
    for k,v in feat_dict.items():
        print(k, len(v))

# print("==== item feat stat ====")
# featStat('./data/u.item')
# print("==== user feat stat ====")
# featStat('./data/u.user', mode='user')

# ==== item feat stat ====
# item_id 1682
# title 1664
# release_date 241
# video_release_date 1
# IMDb_URL 1661
# unknown 2
# Action 2
# Adventure 2
# Animation 2
# Children 2
# Comedy 2
# Crime 2
# Documentary 2
# Drama 2
# Fantasy 2
# Film-Noir 2
# Horror 2
# Musical 2
# Mystery 2
# Romance 2
# Sci-Fi 2
# Thriller 2
# War 2
# Western 2
# ==== user feat stat ====
# user_id 943
# age 61
# gender 2
# occupation 21
# zip_code 795

print("==== item feat stat ====")
featStat('./data/tmp.u.item')
print("==== user feat stat ====")
featStat('./data/tmp.u.user', mode='user')

# ==== item feat stat ====
# item_id 4
# title 4
# release_date 1
# video_release_date 1
# IMDb_URL 4
# unknown 1
# Action 2
# Adventure 2
# Animation 2
# Children 2
# Comedy 2
# Crime 1
# Documentary 1
# Drama 2
# Fantasy 1
# Film-Noir 1
# Horror 1
# Musical 1
# Mystery 1
# Romance 1
# Sci-Fi 1
# Thriller 2
# War 1
# Western 1
# ==== user feat stat ====
# user_id 3
# age 3
# gender 2
# occupation 3
# zip_code 3

# label distribution
import pandas as pd
header = ['user_id', 'item_id', 'rating', 'timestamp']
base = pd.read_csv('./data/ua.base', sep='\t', names=header)
test = pd.read_csv('./data/ua.test', sep='\t', names=header)

def labelDist(df):
    label = df.rating
    dist = defaultdict(int)
    for lb in label.values:
        dist[lb] += 1
    for lb, cnt in dist.items():
        print(lb, cnt, cnt/len(df))
print("==== base label distribution ====")
labelDist(base)
print("==== test label distribution ====")
labelDist(test)

# ==== base label distribution ====
# 5 19048 0.2103124654963012
# 3 24721 0.2729491001435354
# 4 30858 0.34070884398807555
# 1 5568 0.061477310367671414
# 2 10375 0.11455228000441647
# ==== test label distribution ====
# 4 3316 0.35164369034994697
# 3 2424 0.25705196182396606
# 2 995 0.10551431601272535
# 5 2153 0.22831389183457051
# 1 542 0.05747613997879109
```
## utils.py

```
import pandas as pd
import random

user_info_path = './data/u.user'
item_info_path = './data/u.item'
base_path = './data/ua.base'
test_path = './data/ua.test'

def loadData():
    user_header = ['user_id', 'age', 'gender', 'occupation', 'zip_code']
    user_info = pd.read_csv(user_info_path, sep='|', names=user_header)
    user_info['age'] = pd.cut(user_info['age'], bins=[0,10,20,30,40,50,60,70,80,90,100],
                              labels=['0-10','10-20','20-30','30-40','40-50','50-60','60-70','70-80','80-90','90-100'])
    user_id = user_info[['user_id']]
    user_info = user_info.drop(columns=['zip_code'])
    user_info = pd.get_dummies(user_info, columns=['user_id', 'age', 'gender', 'occupation'])
    user_info = pd.concat([user_id, user_info], axis=1)
    print(user_info.shape)

    item_header = ['item_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure',
                   'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
                   'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
    item_info = pd.read_csv(item_info_path, sep='|', names=item_header, encoding="ISO-8859-1")
    item_info = item_info.drop(columns=['title', 'video_release_date', 'IMDb_URL'])
    item_id = item_info[['item_id']]
    item_info = pd.get_dummies(item_info, columns=['item_id', 'release_date', 'unknown', 'Action', 'Adventure',
                   'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
                   'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])
    item_info = pd.concat([item_id, item_info], axis=1)
    print(item_info.shape)

    header = ['user_id', 'item_id', 'rating', 'timestamp']
    base = pd.read_csv(base_path, sep='\t', names=header)
    base = base.drop(columns=['timestamp'])
    base = pd.merge(base, user_info, how='left', on='user_id')
    base = pd.merge(base, item_info, how='left', on='item_id')
    base = base.drop(columns=['user_id', 'item_id'])
    print(base.shape)
    # print(base.head())
    # print(base.dtypes)
    test = pd.read_csv(test_path, sep='\t', names=header)
    test = test.drop(columns=['timestamp'])
    test = pd.merge(test, user_info, how='left', on='user_id')
    test = pd.merge(test, item_info, how='left', on='item_id')
    test = test.drop(columns=['user_id', 'item_id'])
    print(test.shape)
    # print(test.head())
    # print(test.dtypes)
    return base, test

def shuffleBatch(x_batch, y_batch):
    assert len(x_batch) == len(y_batch)
    length = len(x_batch)
    index = [i for i in range(length)]
    random.shuffle(index)
    x_batch_shuffle = [x_batch[i] for i in index]
    y_batch_shuffle = [y_batch[i] for i in index]
    return x_batch_shuffle, y_batch_shuffle

def getBatchData(data, batch_size=32):
    rating = data.rating
    data = data.drop(columns=['rating'])
    start, end = 0, 0
    while True:
        start = end % data.shape[0]
        end = min(data.shape[0], start + batch_size)
        x_batch, y_batch = [], []
        for i in range(start, end):
            label = 1 if rating.iloc[i] >= 4 else 0
            y_batch.append(label)
            single_sample = data.iloc[i, :].values
            x_batch.append(single_sample)

        x_batch_shuffle, y_batch_shuffle = shuffleBatch(x_batch, y_batch)
        yield x_batch_shuffle, y_batch_shuffle

# loadData()
```

# FFM

## utils.py

```
import pandas as pd
import random

user_info_path = './data/u.user'
item_info_path = './data/u.item'
base_path = './data/ua.base'
test_path = './data/ua.test'

def loadData():
    user_header = ['user_id', 'age', 'gender', 'occupation', 'zip_code']
    user_info = pd.read_csv(user_info_path, sep='|', names=user_header)
    user_info['age'] = pd.cut(user_info['age'], bins=[0,10,20,30,40,50,60,70,80,90,100],
                              labels=['0-10','10-20','20-30','30-40','40-50','50-60','60-70','70-80','80-90','90-100'])
    user_id = user_info[['user_id']]
    user_info = user_info.drop(columns=['user_id', 'zip_code'])
    user_info = pd.get_dummies(user_info, columns=['age', 'gender', 'occupation'])
    user_info = pd.concat([user_id, user_info], axis=1)
    # print(user_info.shape)

    item_header = ['item_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure',
                   'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
                   'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
    item_info = pd.read_csv(item_info_path, sep='|', names=item_header, encoding="ISO-8859-1")
    item_info = item_info.drop(columns=['title', 'video_release_date', 'IMDb_URL'])
    item_id = item_info[['item_id']]
    item_info = item_info.drop(columns=['item_id'])
    item_info = pd.get_dummies(item_info, columns=['release_date'])
    item_info = pd.concat([item_id, item_info], axis=1)
    # print(item_info.shape)

    header = ['user_id', 'item_id', 'rating', 'timestamp']
    base = pd.read_csv(base_path, sep='\t', names=header)
    base = base.drop(columns=['timestamp'])
    base = pd.merge(base, user_info, how='left', on='user_id')
    base = pd.merge(base, item_info, how='left', on='item_id')
    base = base.drop(columns=['user_id', 'item_id'])
    print(base.shape)
    # print(base.head())
    # print(base.dtypes)
    test = pd.read_csv(test_path, sep='\t', names=header)
    test = test.drop(columns=['timestamp'])
    test = pd.merge(test, user_info, how='left', on='user_id')
    test = pd.merge(test, item_info, how='left', on='item_id')
    test = test.drop(columns=['user_id', 'item_id'])
    print(test.shape)
    # print(test.head())
    # print(test.dtypes)

    return base, test

def shuffleBatch(x_batch, y_batch):
    assert len(x_batch) == len(y_batch)
    length = len(x_batch)
    index = [i for i in range(length)]
    random.shuffle(index)
    x_batch_shuffle = [x_batch[i] for i in index]
    y_batch_shuffle = [y_batch[i] for i in index]
    return x_batch_shuffle, y_batch_shuffle

def getBatchData(data, batch_size=32):
    rating = data.rating
    data = data.drop(columns=['rating'])
    start, end = 0, 0
    while True:
        start = end % data.shape[0]
        end = min(data.shape[0], start + batch_size)
        x_batch, y_batch = [], []
        for i in range(start, end):
            label = 1 if rating.iloc[i] == 5 else 0
            y_batch.append(label)
            single_sample = data.iloc[i, :].values
            x_batch.append(single_sample)

        x_batch_shuffle, y_batch_shuffle = shuffleBatch(x_batch, y_batch)
        yield x_batch_shuffle, y_batch_shuffle

def getFieldMap(data_columns):
    fields = ['age', 'gender', 'occupation', 'release_date']
    item_types = ['unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime',
                  'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',
                  'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
    field_map = {}
    data_columns.remove('rating')
    for index, feat in enumerate(data_columns):
        if feat in item_types:
            field_map[index] = len(fields)
        else:
            for i, f in enumerate(fields):
                if feat.startswith(f):
                    field_map[index] = i
    return field_map

# loadData()
```

# FNN & PNN

## utils.py

```
import pandas as pd
import random

user_info_path = './data/u.user'
item_info_path = './data/u.item'
base_path = './data/ua.base'
test_path = './data/ua.test'

FIELD_LENS = []

def groupByField(data):
    field_names = ['user_id', 'age', 'gender', 'occupation', 'item_id', 'release_date', 'unknown', 'Action', 'Adventure',
                   'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
                   'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
    data_columns = data.columns.values
    field_dict, field_lens = {}, {}
    for field in field_names:
        field_dict[field] = []
        for column in data_columns:
            if column == 'rating':
                continue
            if column.startswith(field):
                field_dict[field].append(column)
        field_lens[field] = len(field_dict[field])
    new_data_columns = []
    fill = False
    if len(FIELD_LENS) == 0:
        fill = True
    for field in field_names:
        new_data_columns += field_dict[field]
        if fill:
            FIELD_LENS.append(field_lens[field])

    return data[['rating']+new_data_columns]


def loadData():
    user_header = ['user_id', 'age', 'gender', 'occupation', 'zip_code']
    user_info = pd.read_csv(user_info_path, sep='|', names=user_header)
    user_info['age'] = pd.cut(user_info['age'], bins=[0,10,20,30,40,50,60,70,80,90,100],
                              labels=['0-10','10-20','20-30','30-40','40-50','50-60','60-70','70-80','80-90','90-100'])
    user_id = user_info[['user_id']]
    user_info = user_info.drop(columns=['zip_code'])
    user_info = pd.get_dummies(user_info, columns=['user_id', 'age', 'gender', 'occupation'])
    user_info = pd.concat([user_id, user_info], axis=1)
    # print(user_info.shape)

    item_header = ['item_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure',
                   'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
                   'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']
    item_info = pd.read_csv(item_info_path, sep='|', names=item_header, encoding="ISO-8859-1")
    item_info = item_info.drop(columns=['title', 'video_release_date', 'IMDb_URL'])
    item_id = item_info[['item_id']]
    item_info = pd.get_dummies(item_info, columns=['item_id', 'release_date', 'unknown', 'Action', 'Adventure',
                   'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
                   'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])
    item_info = pd.concat([item_id, item_info], axis=1)
    # print(item_info.shape)

    header = ['user_id', 'item_id', 'rating', 'timestamp']
    base = pd.read_csv(base_path, sep='\t', names=header)
    base = base.drop(columns=['timestamp'])
    base = pd.merge(base, user_info, how='left', on='user_id')
    base = pd.merge(base, item_info, how='left', on='item_id')
    base = base.drop(columns=['user_id', 'item_id'])
    print(base.shape)
    # print(base.head())
    # print(base.dtypes)
    test = pd.read_csv(test_path, sep='\t', names=header)
    test = test.drop(columns=['timestamp'])
    test = pd.merge(test, user_info, how='left', on='user_id')
    test = pd.merge(test, item_info, how='left', on='item_id')
    test = test.drop(columns=['user_id', 'item_id'])
    print(test.shape)
    # print(test.head())
    # print(test.dtypes)

    return groupByField(base), groupByField(test)

def shuffleBatch(x_batch, y_batch):
    assert len(x_batch) == len(y_batch)
    length = len(x_batch)
    index = [i for i in range(length)]
    random.shuffle(index)
    x_batch_shuffle = [x_batch[i] for i in index]
    y_batch_shuffle = [y_batch[i] for i in index]
    return x_batch_shuffle, y_batch_shuffle

def sliceByField(single_sample):
    sample = []
    index = 0
    for feat_num in FIELD_LENS:
        tmp = []
        for i in range(feat_num):
            tmp.append(single_sample[index+i])
        sample.append(tmp)
        index += feat_num
    return sample

def getBatchData(data, batch_size=32):
    rating = data.rating
    data = data.drop(columns=['rating'])
    start, end = 0, 0
    while True:
        start = end % data.shape[0]
        end = min(data.shape[0], start + batch_size)
        x_batch, y_batch = [], []
        for i in range(start, end):
            label = 1 if rating.iloc[i] >= 4  else 0
            y_batch.append(label)
            single_sample = data.iloc[i, :].values
            slice_by_field = sliceByField(single_sample)
            x_batch.append(slice_by_field)

        x_batch_shuffle, y_batch_shuffle = shuffleBatch(x_batch, y_batch)
        yield x_batch_shuffle, y_batch_shuffle

# loadData()
```
