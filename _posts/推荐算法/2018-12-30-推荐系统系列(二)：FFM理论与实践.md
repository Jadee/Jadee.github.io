---
title: 推荐系统系列(二)：FFM理论与实践
date: 2018-12-30
categories:
- 推荐算法
tags:
- 推荐系统
---

# 背景

在CTR/CVR预估任务中，除了FM模型[2] 之外，后起之秀FFM（Field-aware Factorization Machine）模型同样表现亮眼。FFM可以看作是FM的升级版，Yuchi Juan于2016年提出该模型，但其诞生是受启于Rendle在2010年发表的另一个模型PITF [3]（FM也是Rendle在2010年发表的），其论文原文 [1] 中写道：

<!--more-->

```
The idea of FFM originates from PITF proposed for recommender systems with personalized tags.
```

在各种深度推荐模型出来之前，FM/FFM模型在各大推荐相关的竞赛中大放异彩。今天，我们就来好好梳理一下FFM的原理以及如何将理论转化为实践。

# 分析

## FFM公式定义

相较于FM模型，FFM模型引入了域（Field）的概念（该想法来源于PITF [3]），可看做是对特征进行分组。例如，对于性别特征而言，通常有两种取值 female、male。对值进行one-hot编码之后性别特征会拆分为两个独立的特征 $x_{female}$ 和 $x_{male}$。显然，这两个特征具有共同的性质：都属于性别。所以可以将这两个特征归在同一个Field下，即有相同的Field编号。不同域的特征之间，往往具有明显的差异性。对比FM中的做法，每个特征有且仅有一个隐向量，在对特征 $x_i$ 与其他特征进行交叉时，始终使用同一个隐向量 $V_i$。 这种无差别式交叉方式，并没有考虑到不同特征之间的共性（同域）与差异性（异域）。

FFM公式化定义如下：

$$ y = w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n\langle V_{i,f_j},V_{j,f_i}\rangle x_ix_j \tag{1} $$

其中 $f$ 为域（Field）映射函数，$f_i$ 表示为 $x_i$ 特征对应的Field编号。

将公式（1）对比FM可以发现，二者之间的差异仅在于二阶交叉对应的隐向量。设数据集中Field的数目为 $F$，那么对于每个特征 $x_i$ 拥有 $F$ 个对应的隐向量，分别应用于与不同域特征进行交叉时。设隐向量维度为 $k$，FFM二阶交叉项参数为 $nkF$。

## 求解

由于引入了Field，公式（1）不能像FM那样进行改写，所以FFM模型进行 推断 时的时间复杂度为 $O(kn^2)$。

为了方便推导各参数的梯度，隐向量表示为 $V_{i,f_j}=(v_{i,f_j}^1,v_{i,f_j}^3,\cdots,v_{i,f_j}^k)$。公式（1）展开：

$$ y = w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n\sum_{q=1}^{k}v_{i,f_j}^{q}v_{j,f_i}^{q}x_ix_j \tag{2} $$

当参数为 $w_0$ 时，$\frac{\partial y}{\partial w_0} = 1$。

当参数为 $w_i$ 时，$\frac{\partial y}{\partial w_i} = x_i$。

当参数为 $v_{i,f_j}^q$ 时，其他参数视为常量，只考虑公式（2）交叉项。由于Field数量以及映射关系 $f$ 取决于数据集，这种情况参数梯度的数学统一表达式稍微复杂点（但当确定 $f$ 之后很好计算），所以这里就暂且按下不表。

## 性能评估

上述小节并未得到统一的参数梯度表达式，但估计模型训练时的时间复杂度，仍需评估更新 $v_{i,f_j}^q$ 参数的时间复杂度。尽管没有梯度公式，但可以通过夹逼定理来确定该参数的更新时间复杂度。两种极端情况：1）F = 1；2）F = n；参数更新时间复杂度位于二者之间。

1）F = 1 时，所有特征均属于同一个Field，此时FFM退化为FM。可以将 $f$ 暂时省略，公式（2）可以表示为

$$ \begin{align}
y 
={} & w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n\sum_{q=1}^{k}v_{i}^{q}v_{j}^{q}x_ix_j \notag \\
={} & w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n-1}\sum_{q=1}^{k}v_i^q\sum_{j=i+1}^{n}v_{j}^{q}x_ix_j \notag \\
\end{align} \tag{3} $$

有：

$$ \begin{align}
\frac{\partial{y}}{\partial{v_{i,f_j}^q}}
={} & \frac{\partial{y}}{\partial{v_{i}^q}} \notag \\
={} & \sum_{j=i+1}^{n}v_j^qx_ix_j \notag \\
\end{align} \tag{4} $$

所以，更新参数 $v_{i,f_j}^q$  所需时间复杂度为 $O(n)$。这只是二阶项中 $nkF$ 个参数中的其中一个，所以更新二阶项参数总时间复杂度为 $O(kn^2)$。

2）F = n 时，每个特征的Field都不相同。不失一般性，可以设 $f_i = i$，此时公式（2）可以表示为：

$$ \begin{align}
y 
={} & w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^n\sum_{q=1}^{k}v_{i,j}^{q}v_{j,i}^{q}x_ix_j \notag \\
\end{align} \tag{5} $$

有：

$$ \begin{align}
\frac{\partial{y}}{\partial{v_{i,f_j}^q}}
={} & \frac{\partial{y}}{\partial{v_{i,j}^q}} \notag \\
={} & v_{j,i}^qx_ix_j \notag \\
\end{align} \tag{4} $$

所以，更新参数 $v_{i,f_j}^q$  所需时间复杂度为 $O(1)$。这只是二阶项中 $nkF$ 个参数中的其中一个，所以更新二阶项参数总时间复杂度为 $O(kn^2)$。

综上，更新二阶项参数所需时间复杂度为 $O(kn^2)$，因为 $w_0$ 与 $w_i$ 参数更新时间复杂度为 $O(1)$，所以FFM训练的时间复杂度为 $O(kn^2)$。

**总结：FFM 训练/推断 时间复杂度都为 $O(kn^2)$。**

## 优缺点

优点：

* 在高维稀疏性数据集中表现很好。

* 相对FM模型精度更高，特征刻画更精细。

缺点：

* 时间开销大。FFM时间复杂度为  $O(kn^2)$，FM时间复杂度为  $O(kn)$。

* 参数多容易过拟合，必须设置正则化方法，以及早停的训练策略。  
&nbsp;关于FFM过拟合可以参考：[FPENN：解决FFM模型过拟合的问题](https://zhuanlan.zhihu.com/p/83575678)

## 注意事项

FFM对于数据集的要求 [1]：

```
1) FFMs should be effective for data sets that contain categorical features and are transformed to binary features.
2) If the transformed set is not sparse enough, FFMs seem to bring less benefit.
3) It is more difficult to apply FFMs on numerical data sets.
```

1）含有类别特征的数据集，且需要对特征进行二值化处理。

2）越是稀疏的数据集表现效果相较于其他模型更优。

3）FFM比较难应用于纯数值类型的数据集。

数据预处理 [4]：

与FM一样，最好先进行特征归一化，再进行样本归一化。

超参数对于模型的影响 [1]：

首先需要注意的是，FFM的隐向量维度远小于FM的隐向量维度，即 $k_{FFM} \lt k_{FM}$ 。

1）隐向量维度 $k$ 对于模型的影响不大。

![avatar](/images/rec/rec-59.png)

2) 正则化系数 $\lambda$ 如果太大，容易导致模型欠拟合，反之，容易过拟合。

![avatar](/images/rec/rec-60.png)

3）在论文中，使用的是Adagrad优化器，全局学习率 $\eta$ 也是超参数。如果 $\eta$ 在一个较小的水平，则可以表现最佳。过大，容易导致过拟合。过小，容易欠拟合。

![avatar](/images/rec/rec-61.png)

模型训练加速 [1,4]：

1）梯度分布计算；2）自适应学习率；3）多核并行计算；4）SSE3指令并行编程；

# 实验

与FM一致使用 MovieLens100K 数据集，将评分大于3的样本置为正样本1，其他置为负样本0，构造一个二分类任务。使用 Crossentropy 损失函数，最后使用了 Adam 优化算法。
```
论文中使用的 logisticloss 将样本构造为-1、1的二分类，同时使用的是 Adagrad优化算法 [1]
```

核心代码：
```
# -*- coding:utf-8 -*-
import tensorflow as tf
from utils import *
from sklearn.metrics import accuracy_score, roc_auc_score
class FFM(object):
    def __init__(self, vec_dim, feat_num, field_num, lr, lamda):
        self.vec_dim = vec_dim
        self.feat_num = feat_num
        self.field_num = field_num
        self.lr = lr
        self.lamda = lamda

        self._build_graph()

    def _build_graph(self):
        self.add_input()
        self.inference()

    def add_input(self):
        self.x = tf.placeholder(tf.float32, shape=[None, self.feat_num], name='input_x')
        self.y = tf.placeholder(tf.float32, shape=[None], name='input_y')

    def inference(self):
        with tf.variable_scope('linear_part'):
            w0 = tf.get_variable(name='bias', shape=[1], dtype=tf.float32)
            self.W = tf.get_variable(name='linear_w', shape=[self.feat_num], dtype=tf.float32)
            self.linear_part = w0 + tf.reduce_sum(tf.multiply(self.x, self.W), axis=1)
        with tf.variable_scope('interaction_part'):
            self.V = tf.get_variable(name='interaction_w', shape=[self.feat_num, self.field_num, self.vec_dim], dtype=tf.float32)
            self.interaction_part = tf.constant(0, dtype=tf.float32)
            for i in range(self.feat_num):
                for j in range(i+1, self.feat_num):
                    self.interaction_part += \
                        tf.reduce_sum(tf.multiply(self.V[i, field_map[j]], self.V[j, field_map[i]])) * \
                        tf.multiply(self.x[:, i], self.x[:, j])

        self.y_logits = self.linear_part + self.interaction_part
        self.y_hat = tf.nn.sigmoid(self.y_logits)
        self.pred_label = tf.cast(self.y_hat > 0.5, tf.int32)
        self.loss = -tf.reduce_mean(self.y*tf.log(self.y_hat+1e-8) + (1-self.y)*tf.log(1-self.y_hat+1e-8))
        self.reg_loss = self.lamda*(tf.reduce_mean(tf.nn.l2_loss(self.W)) + tf.reduce_mean(tf.nn.l2_loss(self.V)))
        self.total_loss = self.loss + self.reg_loss

        self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.total_loss)
        
EPOCH = 10
STEP_PRINT = 200
STOP_STEP = 1200

LEARNING_RATE = 1e-4
BATCH_SIZE = 32
LAMDA = 1e-2

VEC_DIM = 12
FIELD_NUM = 5
print("load data...")
base, test = loadData()
FEAT_NUM = base.shape[1]-1
print("load data over!")
print("get field map...")
field_map = getFieldMap(base.columns.values.tolist())

def eval_acc(pred_label, y):
    acc = accuracy_score(y, pred_label.flatten())
    return acc
def eval_auc(y_hat, y):
    auc = roc_auc_score(y, y_hat.flatten())
    return auc

def getValTest():
    """split validation data/ test data"""
    for valTest_x, valTest_y in getBatchData(test, batch_size=8000):
        val_x, val_y = valTest_x[:4000], valTest_y[:4000]
        test_x, test_y = valTest_x[4000:], valTest_y[4000:]
        return val_x, val_y, test_x, test_y

def run():
    print("get val/test data...")
    val_x, val_y, test_x, test_y = getValTest()
    print("build model...")
    model = FFM(VEC_DIM, FEAT_NUM, FIELD_NUM, LEARNING_RATE, LAMDA)
    print("lefs train...")
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        # saver = tf.train.Saver()
        step = 0
        best_metric = 0
        last_improved = 0
        stop = False
        print("====== let's train =====")
        for epoch in range(EPOCH):
            print("EPOCH: {}".format(epoch+1))
            for x_batch, y_batch in getBatchData(base, BATCH_SIZE):
                feed_dict = {model.x: x_batch, model.y:y_batch}
                sess.run(model.train_op, feed_dict=feed_dict)
                if step % STEP_PRINT == 0:

                    train_y_hat, train_pred_label, train_loss = sess.run([model.y_hat, model.pred_label], feed_dict=feed_dict)
                    train_acc = eval_acc(train_pred_label, y_batch)
                    val_y_hat, val_pred_label, val_loss = sess.run([model.y_hat, model.pred_label, model.total_loss],
                                                             feed_dict={model.x:val_x, model.y:val_y})
                    val_acc, val_auc = eval_acc(val_pred_label, val_y), eval_auc(val_y_hat, val_y)
                    improved_str = ''
                    if val_auc > best_metric:
                        best_metric = val_auc
                        last_improved = step
                        # saver.save(sess=sess, save_path='./ckpt/best')
                        improved_str = '*'

                    msg = 'Iter: {0:>6}, Train acc: {1:>6.4}, Val acc: {2:>6.4}, Val auc: {3:>6.4}, Val loss: {4:6.6}, Flag: {5}'
                    print(msg.format(step, train_acc, val_acc, val_auc, val_loss, improved_str))
                step += 1
                if step-last_improved > STOP_STEP:
                    print("No optimization for a long time, auto-stopping...")
                    stop = True
                    break
            if stop:
                break

        print("====== let's test =====")
        # saver.restore(sess=sess, save_path='./ckpt/best')
        test_y_hat, test_pred_label, test_loss = sess.run([model.y_hat, model.pred_label, model.total_loss],
                                                feed_dict={model.x: test_x, model.y: test_y})
        test_acc, test_auc = eval_acc(test_pred_label, test_y), eval_auc(test_y_hat, test_y)
        msg = 'Test acc: {0:>6.4}, Test auc: {1:>6.4}'
        print(msg.format(test_acc, test_auc))

print("lets run...")
run()

```

感想： FFM 训练速度真的很慢。

# reference

[1] Juan, Yuchin, et al. "Field-aware factorization machines for CTR prediction." Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 2016.

[2] Rendle, S. (2010, December). Factorization machines. In 2010 IEEE International Conference on Data Mining (pp. 995-1000). IEEE.

[3] Rendle, Steffen, and Lars Schmidt-Thieme. "Pairwise interaction tensor factorization for personalized tag recommendation." Proceedings of the third ACM international conference on Web search and data mining. ACM, 2010.

[4] <https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html>

[5] <https://www.jianshu.com/p/781cde3d5f3d>

[6] <https://zhuanlan.zhihu.com/p/38241764>

[7] <https://zhuanlan.zhihu.com/p/64113429>

[8] <https://zhuanlan.zhihu.com/p/50692817>

