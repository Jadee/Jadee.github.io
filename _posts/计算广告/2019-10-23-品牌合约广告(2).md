---
title: 品牌合约广告(2)
date: 2019-10-23
categories: 计算广告
tags:
- 计算广告
- 合约广告
---

# 项目背景

品牌广告投放分配优化，目的是设计一种合理的分配机制，为每个请求对应的资源位展示分配对应的展示广告，以完成品牌广告订单的展示量要求，并在此前提下最大化广告转化效果点击率。

不同于一般效果广告只追求点击率最大化的要求，品牌广告的首要目标是完成广告主的合约保量需求，因此针对效果广告中点击率优化的神经网络、强化学习等算法在品牌广告分配模型中的往往很难直接落地应用。

# 项目目标

针对品牌广告合约保量的要求，在品牌广告保量的前提下，或者说是在保证品牌广告的订单完成率不低于一定阈值的前提下，最大化合约广告的点击率。
# 算法方案

基于已有的SHALE算法框架进行求解，对于最大化点击率的目标，在模型的目标函数中增加最大化click项，建立分配模型：

##分配模型

$$ \min \;\;\; \frac{1}{2}\sum_{j,i\in\Gamma(j)}s_i\frac{V_j}{\theta_j}(x_{ij}-\theta_j)^2-\sum_{j,i\in\Gamma(j)}s_ix_{ij}-\lambda\sum_{j,i\in\Gamma(j)}s_ix_{ij}c_{ij}\\ $$

$$
\begin{align} s.t. \;\;\; & \sum_{i\in\Gamma(j)}s_ix_{ij}\le d_j,\;\;\forall j \\
& \sum_{j\in\Gamma(i)}x_{ij}\le1,\;\;\forall i \\
& s_ix_{ij}\le f_j,\;\;\forall i,j \\
& x_{ij} \ge 0, \;\; \forall i,j \\
\end{align}
$$

目标函数中第一项为平滑项，目的是实现广告在各supply上的均匀投放，避免一味追求保量或效果最大出现集中小部分supply投放的情况；第二项是pv投放总量，即最大化pv总量；第三项是click总量，权重值用于在保量和效果两个目标之间实现均衡。

第一个约束限制了每个demand都不能出现超过预定量的投放，这一约束避免了为到达更多的click数量而超量投放的情况，而由于目标函数中有最大化投放总量这一项，因此仍然可以保证缺量风险最小这一目标；第二个约束为供给侧约束，对于淘系品牌展示广告，一个supply节点上最多只能投放一个广告。

## 算法原理

采用拉格朗日对偶方法，根据KKT条件，最优投放概率 $x_{ij}=\min\{\frac{f_j}{s_i},\max\{0,\theta_j(1+\frac{1+\lambda c_{ij}-\alpha_j-\beta_i}{v_j})\}\}$，其中 $\alpha_j$ 和 $\beta_i$ 分别为模型中第一个和第二个约束对应的拉格朗日对偶变量。为得到最优解 $x_ij$，采用坐标下降法分别在demand和supply两个维度上对 $\alpha_j$ 和 $\beta_i$ 进行交替迭代求解，定义 $g(z) = max\{0, \theta_j(1 + \frac{z}{v_j}) \}$，下面给出算法实现的具体步骤。

### 离线训练步骤

stage One：

A. 初始化：  
  1) 对每个j，$\alpha_j^0=1+\lambda\max_{i\in\Gamma(j)}\{c_{ij}\}$   
  2) 对每个i，求解 $\beta_i$，使其满足 $\sum_{j\in\Gamma(i)}g(1+\lambda c_{ij}-\alpha_j-\beta_i)=1$，若无解或解小于0，$\beta_i = 0$

B. 迭代步骤：
  1) 对每个j，求解 $\alpha_j$，使其满足 $\sum_{i\in\Gamma(j)}s_ig(1+\lambda c_{ij}-\alpha_j-\beta_i)=d_j$，若无解或解小于0，$\alpha_j = 0$
  2）对每个i，求解 $\beta_i$，使其满足 $\sum_{j\in\Gamma(i)}g(1+\lambda c_{ij}-\alpha_j-\beta_i)=1$，若无解或解小于0，$\beta_i = 0$

C. 重复迭代步骤B中的1)和2），直到目标函数收敛或达到指定迭代次数.

stage Two:

A. 初始化：对每个i，令 $\tilde{s_i} = s_i$

B. 对每个 $j$，按照allocation order执行以下操作：  
  1) 求解 $zeta_j$，使其满足方程 ${\sum_{i\in\Gamma(j)}\min\{\tilde s_i,s_ig_{ij}(1+\lambda c_{ij}-\zeta_j-\beta_i)\}}=d_j$；若方程无解，$\zeta_j= -\infty$；  
  2) 对符合定向的每个 $i$，更新 $\tilde s_i$，$\tilde s_i=\tilde s_i-\min\{\tilde s_i,s_ig_{ij}(1+\lambda c_{ij}-\zeta_j-\beta_i)\}$
  
### 在线投放概率计算步骤

线上对于一个到来的请求 $i$，对应召回订单队列 $d = \{d_1, d_2, \cdots, d_n \}$，其每个订单投放概率的计算过程为：

1. 求解 $\beta_i$，使其满足 $\sum_{j\in\Gamma(i)}g(1+\lambda c_{ij}-\alpha_j-\beta_i)=1$，若无解或解小于0，$\beta_i = 0$

2. 令 $\tilde x_{ij} = 1$，对召回队列中的每个订单j，按照优先级顺序依次执行以下操作：  
  * 计算投放概率 $x_{ij}=\min\{ \tilde x_{ij}, \max\{0,\theta_j(1+\frac{1+\lambda c_{ij}-\alpha_j-\beta_i}{v_j})\}\}$  
  * $\tilde x_{ij} = $\tilde x_{ij} - x_ij$
  
## 算法优化

针对模型和数据的特点以及算法实现过程中遇到的具体问题，对上述基本算法的实现细节上进行了一系列优化。

### 大规模节点近似方法

stage One迭代中的第一步，更新的过程，是在supply节点上进行的，每个supply上对应的广告是规模很小，换言之求解这个线性方程的时间是可以接受的，但是在第二步迭代中，求解的过程是利用每个supply计算的结果进行计算的，这个节点的规模最大是亿级别的规模，无论是内存还是计算资源都是无法吃下这么大的资源的。

通过近似的方法求解该方程，不直接进行求解，通过多迭代的方式，近似逼近该目标解，这样的好处是每次计算轮数较多，但是计算的规模较小，从而做到加速计算结果。进行运算的过程中既可以支持进行随机采样加速计算也可以全量的方式进行优化求解。这种方式的求解的算法原理如下面所示：

定义第t轮迭代后的投放总量为：

$d_j(\alpha^t)=\sum_{i\in\Gamma(j)}s_i\max\{0,\theta_j(1+\frac{1+\lambda c_{ij}-\alpha_j^t-\beta_i^t}{v_j})\}$

显然有 $d_j(\alpha^0)=\sum_{i\in\Gamma(j)}s_i\max\{0,\theta_j(1+\frac{1+\lambda c_{ij}-\alpha_j^0-\beta_i^0}{v_j})\}\le\sum_{i\in\Gamma(j)}s_i\max\{0,\theta_j\}\le d_j$ 

而在第t轮 $d_j(\alpha^t) \leq d_j$ 成立的前提下：

$$ \begin{align} 
d_j - d_j(\alpha^t) &= \sum_{i\in\Gamma(j)}s_i\max\{0,\theta_j(1+\frac{1+\lambda c_{ij}-\alpha_j^{t+1}-\beta_i^t}{v_j})\} - \sum_{i\in\Gamma(j)}s_i\max\{0,\theta_j(1+\frac{1+\lambda c_{ij}-\alpha_j^t-\beta_i^t}{v_j})\} \\
& \le \sum_{i\in\Gamma(j)}s_i\theta_j(1+\frac{\alpha_j^t-\alpha_j^{t+1}}{v_j})
\end{align}
$$

可以得到 $\alpha_j^{t+1} \le \alpha_j^t - v_j(1-\frac{d_j(\alpha^t)}{d_j})$，即 $\alpha_j$ 单调递减，而为了保证迭代步骤2）中方程等号成立，有 $\beta_i$ 单调递增，即 $\beta_i^{t+1} \geq \beta_i^{t}$ 。

于是在第t+1轮迭代中，有 

$$d_j(\alpha^{t+1})=\sum_{i\in\Gamma(j)}s_i\max\{0,\theta_j(1+\frac{1+\lambda c_{ij}-\alpha_j^{t+1}-\beta_i^{t+1}}{v_j})\} \le \sum_{i\in\Gamma(j)}s_i\max\{0,\theta_j(1+\frac{1+\lambda c_{ij}-\alpha_j^{t+1}-\beta_i^t}{v_j})\}$$

即 $d_j(\alpha^{t+1})\le d_j$

所以只要从初始值 $\alpha_j^0=1+\lambda\max_{i\in\Gamma(j)}\{c_{ij}\}$ 开始迭代，迭代每一步都有 $d_j(\alpha^{t+1})\le d_j$，即每一次迭代的结果都为满足约束条件的可行解。因此可以通过控制迭代轮数来进行结果精确度与时间成本之间的均衡。

根据上面得到的 $\alpha_j^{t+1} \le \alpha_j^t - v_j(1-\frac{d_j(\alpha^t)}{d_j})$，可以直接使用 $\alpha_j^{t+1} = \alpha_j^t - v_j(1-\frac{d_j(\alpha^t)}{d_j})$ 进行 $\alpha_j$ 的近似更新，这样做避免了迭代步骤1）中大规模方程的直接求解，对内存和计算时间的要求大幅度降低，而且可以对所有demand进行并行计算。

### 线性单调方程求解方法

对于离线训练迭代步骤2) 的线性方程 $\sum_{j\in\Gamma(i)}g(1+\lambda c_{ij}-\alpha_j-\beta_i)=1$ 中 $\beta_i$ 的求解，定义：

$b_i = -\beta_i, k_{j}=\frac{\theta_j}{v_j}, A_{ij} = \theta_j + k_j(1+\lambda c_{ij}-\alpha_j), F_{ij} = \frac{f_j}{s_i}$，有

$x_{ij} = f_j(b_i)=\min\{F_{ij},\max\{0,k_jb_i+A_{ij}\}\}$，原方程求解等价于 $\sum_{j\in\Gamma(i)}f_j(b_i)=1$

对于函数 $f_j(b_i)=\min\{F_{ij},\max\{0,k_jb_i+A_{ij}\}\}$，由于 $k_{ij} \geq 0$，其函数值单调非递减，且函数图象由三条折线构成，斜率分别为0，$k_j$，0，折线中两个拐点的坐标为 $(-\frac{A_{ij}}{k_j}, 0)$ 和 $(\frac{F_{ij}-A_{ij}}{k_j}, F_ij)$。

令 $b_{1j} =-\frac{A_{ij}}{k_j}$，$b_{2j}=\frac{F_{ij}-A_{ij}}{k_j}$，观察函数图象，三个区间内的函数斜率分别为0，$k_j$，0.

对于方程 $f_j(b_i) = y$，通过判断y 与 0 和 $F_{ij}$的关系，即得到方程的解：

1）当 $y<0$ 或 $y > F_{ij}$ 时，方程无解；  
2）当 $0\le y\le F_{ij}$ 时， 方程的解为 $b_i = \ b_{1j}+\frac{y-0}{k_j}$

对于方程 $\sum_{j\in\Gamma(i)}f_j(b_i)=1$ 的求解，首先定义结构体b{val, sign, kj}，

其中val为拐点的值，$k_j$ 为对应 $f_j(b_i)$ 的斜率，sign表明该点是b1还是b2，拐点b1对应的sign为0 斜率增量为 $k_j$，拐点b2对应的sign为1 斜率增量为 $-k_j$。

把所有 $j\in\Gamma(i)$ 对应 $f_j(b_i)$ 的的拐点 $b_{1j}$ 和 $b_{2j}$ 先从小到大排序，再依次遍历每个区间，即可找到方程 $\sum_{j\in\Gamma(i)}f_j(b_i)=1$的解，复杂度为o(nlogn)。

具体步骤:
对于所有 $j\in\Gamma(i)$，按照 $f_j(b_i)$ 的拐点 $b_{1j}$ 和 $b_{2j}$ 从小到大排序，得到结构体数组B，向量长度为len.

```python
sum_f = 0;
sum_k = 0; 
x = 0;
for (i = 0; i < len; i++){
	sum_f += (B[i].val - x) * sum_k;
  if(sum_f > y){
    if(sum_k == 0){
			return B[i].val;
    } else {
      return B[i].val - (sum_f - y)/sum_k;
    }
    x = B[i].val;
	}
  if(B[i].sign == 0)
    sum_k += B[i].k;
  else
    sum_k -= B[i].k;
}
```

### 平滑项降权及初始值修正

由于在线投放策略中，在计算出投放概率后还会通过pacing策略来控制投放速度，可以认为pacing策略已经起到了保证投放平滑的作用，因此可以降低SHALE模型目标函数中的平滑项权重。为达到这一目的，我们把平滑项的表达式改写为 $\frac{1}{2}\sum_{j,i\in\Gamma(j)}s_i\frac{V_j}{\theta_j}(x_{ij}-1)^2$，即把原来尽量按供需比均匀投放的目标改为趋于概率1投放，以实现更高的投放量及更好的投放效果。

在调整了平滑项目标之后，大规模近似迭代方法中参数 $\alpha_j$ 的迭代公式变为 $$\alpha_j^{t+1} = \alpha_j^t - v_j \theta_j(1-\frac{d_j(\alpha^t)}{d_j})$

由于 $\theta_j$ 的值较小，每一步迭代的更新量较小，迭代轮数较多，离线训练的速度明显变慢。

根据大规模节点近似方法中的迭代公式推导过程可以发现，为保证每一轮迭代都为满足模型约束的可行解，需要使不等式成立，初始条件是让。换句话说，只要保证了初始条件成立，就可以使得每轮近似迭代的结果都在可行域范围内。
为了提升离线训练的速度，我们用对全量数据集进行随机抽样，在抽样数据中使用上述线性单调方程求解的方法对进行精确求解，将其结果作为全量数据训练的初始值下界，这时为了保证成立以进行接下来的迭代更新，而将原来的初始值作为初始值上界，采用二分逼近的方法对进行检验和修正，使其值仍然在可行域范围内。
