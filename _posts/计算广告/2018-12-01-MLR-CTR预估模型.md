---
title: MLR-CTR预估模型
date: 2018-12-01
categories: 计算广告
tags:
- 计算广告
- CTR模型
---

# 背景

点击率(Click-Through Rate, CTR)预估是互联网主流应用(广告、推荐、搜索等)的核心算法问题，包括google、facebook、百度等业界巨头对这个问题一直进行着持续的研究和投入。

<!-- more -->

广告领域的CTR预估问题，面临的是超高维离散特征空间中模式发现的挑战：如何拟合现有数据的规律，同时又具备推广性。在MLR之前业界的主流算法是广义线性模型LR(logistic regression，逻辑斯特回归)+人工特征工程。这种解法需要引入大量的领域知识来人工设计特征以及特征之间的交叉组合。**MLR算法**创新地提出并实现了直接在原始空间学习特征之间的非线性关系，基于数据自动发掘可推广的模式，相比于人工来说效率和精度均有了大幅提升。[相关论文](https://arxiv.org/abs/1704.05194)

# 算法特性

## 基础特性

MLR可以看做是对LR的一个自然推广，它采用分而治之的思路，用分片线性的模式来拟合高维空间的非线性分类面，超参数分片数可以较好地平衡模型的拟合与推广能力。下图中，MLR模型用4个分片可以完美地拟合出数据中的菱形分类面。

![avatar](/images/计算广告/ad-24.png)

MLR算法适合于工业级的大规模稀疏数据场景问题，如广告CTR预估。背后的优势体现在两个方面：

* 端到端的非线性学习。从模型端自动挖掘数据中蕴藏的非线性模式，省去了大量的人工特征设计，这 使得MLR算法可以端到端地完成训练，在不同场景中的迁移和应用非常轻松。  
* 稀疏性。MLR在建模时引入了L1和L21范数正则，可以使得最终训练出来的模型具有较高的稀疏度， 模型的学习和在线预测性能更好。当然，这也对算法的优化求解带来了巨大的挑战。具体细节参见我 们的论文。

## 高级特性

在具体的实践应用中，阿里妈妈精准定向机器学习和排序团队进一步发展了MLR算法的多种高级特性，包括：

* **结构先验**：基于领域知识先验，灵活地设定空间划分与线性拟合使用的不同特征结构。例如精准定向 广告中验证有效的先验为：以user特征空间划分、以ad特征为线性拟合。  
* **线性偏置**：这个特性提供了一个较好的方法解决CTR预估问题中的bias特征，如位置、资源位等等。  
* **模型级联**：MLR支持与LR模型的级联式联合训练，这有点类似于wide&deep learning。在我们的实践 经验中，一些强feature配置成级联模式有助于提高模型的收敛性。  
* **增量训练**：实践证明，MLR通过结构先验进行pretrain，然后再增量进行全空间参数寻优训练，会获得 进一步的效果提升。

# 算法分析

**MLR**整体的思想就是把复杂的整个定义域上的拟合问题，分成小块，每一块上把它近似成一个线性模型来解决，由于局部数据的稳定性，如果划分合理，拟合优秀，那么就可以进行更匹配的拟合，问题就会得到更精确的解。这个思想，也就是常说的“分治”的思想(divide and conquer)。这个比semi-lazy的思路更自然和简单，计算复杂性也好很多。(semi-lazy的思想是用预测用例在训练数据中的k临近来训练一个专门的拟合模型，也是基于局部性原理来思考的，但是计算和存储复杂度都很高，超参数设置不易，且还要求训练数据的分布符合要求)。

mlr中的分块是抽象的，可以是任意的形式，比如硬性分块(分割)，概率分块(类似混合分布)，或者是某些feature的组合来确定的分块(这种意义上来说，感觉rf和gbdt也可以看做是mlr特殊的的一例)

## MLR和其他非线性模型的不同

mlr面对的问题是**“feature规模很大，数据非常稀疏(sparse)情况下的线性不可分”**问题。广告ctr预估正是这样一个问题。之前的方案LR+特征工程，用核函数的方式带来非线性扩展，但是这样就非常依赖特征工程的质量，经验要求高，复杂度也非常高。

* MF本身提供了基于2个特征关系的考虑，并通过类似矩阵分解的形式，提供了适应非线性+稀疏数据的方法，缺点是只能做2维的非线性化，对高维feature不适用，刻画复杂函数的能力有限。  
* FM虽然升了维，但是还是可以做到线性的复杂度，这是FM的很大的优势；但是基础的FM只提供了2个feature之间的关系拟合(说白了就是最高二次函数的拟合)，模型刻画复杂关系的能力还是受到了限制，这一点不如mlr。mlr比FM有更强的特征选择能力，目测看这种能力是由2，1正则项和1阶正则项的组合带来的。  
* SVM是n^2级别的，大规模就不考虑了。  
* GBDT也是很好的非线性，但是也是在小规模feature+很多样本的情况下有很好的效果，随着feature的增加，效果越来越难以保证(全部是低相关性特征，很难在一定树高度下给出好预测；如果增加高度，续联数据又不够，就会变成记忆形，很容易陷入过拟合)

## MLR数学描述

模型的基础数学描述如下：

$$ p(y=1|x)=g(\sum_{j=1}^m\sigma(u_j^Tx)\eta(w_j^Tx)) \tag{1} $$

当一个预测用例到达时，公式中的 $\sigma(u_j^Tx)$ 这一部分相当于将这个用例分配(或者叫对应,映射)到一个区域，而 $\eta(w_j^Tx)$ 这一部分，相当于在这个映射到的区域做一个线性预测。之后把这个用例在所有m个区域上的预测“加权组合”起来，在通过 $g()$ 这个函数，得到用例最终的预测值。

在论文中，作者一般用下面这个公式来实例化上面的抽象公式：

$$ p(y=1|x)=\sum_{i=1}^m\frac {e^{u_i^Tx}}{\sum_{j=1}^me^{u_i^Tx}}*\frac {1}{1+e^{-w_i^Tx}} \tag {2} $$

这个公式相当于相当于：当接到一个预测用例后，先利用softmax看看用例在某个区域的概率，之后在这个区域用后面的LR来预测y的label，之后把所有区域的结果按照用例落在所属区域的概率加权求和，最终得到在给定x的情况下y=1的概率是多少。论文后面的模型公式，基本都是基于这个公式来实例化问题的。

MLR的正式名称是Piece-wise Linear Models(基于分片的线性模型?)，期望是提供高阶非线性函数的模拟能力，能对sparse的高维度feature的数据又和好的拟合，同时用于实际应用的需要，模型希望在防止过拟合的同时得到稀疏解，这部分就要正则来帮忙。总体上看，我们设定了损失函数，在选择合适的正则，就可以提供衡量模型效果的object函数，通过优化object，最终可以得到模型最合适的参数。

mlr最终的object function(损失函数+正则项)如下：

$$ argmin_\theta f(\theta)=loss(\theta)+\lambda||\theta||_{2,1}+\beta||\theta||_1 \tag{3} $$

$$ loss(\theta)=-\sum_{t=1}^n[y_tlog(p(y_t=1|x_t,\theta))+(1-y_t)log(p(y_t=0|x_t,\theta))] \tag{4} $$

loss function选择的是负log似然函数(neg-log-likelihood loss function)：

$$ ||\theta||_{2,1}=\sum_{i=1}^d\sqrt{\sum_{j=1}^{2m}\theta_{ij}^2} \tag{5} $$

$$ ||\theta||_1=\sum_{ij}|\theta_{ij}| \tag{6} $$

这里用了2个正则，第一个是“L2，1”正则，具体的计算方式可以看公式(4)；另外一个就是很常见的“L1”正则。由于模型的关系，对于某个feature k来说，如果任意分片上的线性模型的这个维度的参数不为0，那么这个参数就不能删除了。所以除了“L1”外，作者又加入了“L2，1”正则的restrict，来带来某个参数的2m个参数都趋向于0，从而带来稀疏性，让模型变得简单。

OK，有了objective function，我们就可以最优化损失啦！但是仔细一看，这个obj在整个定义域上是非凸，非平滑的。。。非凸就罢了，这不平滑就是说并不是处处可导啊，那就不能用传统意义上的随机梯度下降啦，因为有的地方对 $\theta$ 来说是没有梯度定义的。

论文提出了一个优化的方法，并给出了这个方法理论证明和公式推导，具体如下：

## 优化方法及证明

### 第一个问题(Lemma 1的证明)

首先，作者定义了一个叫做“方向导数”的东西，即 $f'(\theta,d)$ ，打算用它在遇到一些不光滑点 $\theta$ 时，来帮助找到下降方向d。这里，作者提供了 $f(\theta,d)$ 在定义域上处处可导的证明。

具体的证明如下：

$$ \begin{split} f'(\theta;d) = lim_{\alpha\rightarrow0} \cfrac {f(\theta+\alpha d)-f(\theta
)} {\alpha} \\ = lim_{\alpha\rightarrow0} \cfrac {loss(\theta+\alpha d)-loss(\theta
)} {\alpha} \\ + lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{2,1} -|| f(\theta
)||_{2,1}} {\alpha} \\ + lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{1} -|| f(\theta
)||_{1}} {\alpha} \end{split}  \tag{7} $$ 

1. 这里的第一个部分loss是处处可导的，所以有

$$ lim_{\alpha\rightarrow0} \cfrac {loss(\theta+\alpha d)-loss(\theta
)} {\alpha}=\triangledown{loss(\theta^T)d} \tag{8} $$

2. 对于第二部分，当 $||\theta_{i.}||_{2,1}\neq0$ 时，针对 $\theta$的导数存在，所以

$$ \begin{align} lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta_{ij}+\alpha d_{ij}||_{2,1} -|| f(\theta_{ij}
)||_{2,1}} {\alpha} &= \lambda (\sum_{i=1}^d\sqrt{\sum_{j=1}^{2m}\theta_{ij}^2})'*d_{ij} \\
&=\lambda \frac {2\theta_{ij}}{2\sum_{i=1}^d\sqrt{\sum_{j=1}^{2m}\theta_{ij}^2}}*d_{ij} \\ 
&= \lambda \frac {\theta_{ij}}{\sum_{i=1}^d\sqrt{\sum_{j=1}^{2m}\theta_{ij}^2}}*d_{ij} \tag{9} 
\end{align}$$

写成向量的形式就是：

$$ \lambda \frac {\theta_{i.}^Td_{i.}}{||\theta_{i.}||_{2,1}} \tag{10} $$

当 $||\theta_{i.}||_{2,1}=0$ 时，那说明所有的 $\theta_{ij}$ 都等于0，其中 $1<=j<=2m$，这时，我们根据定义 $lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{2,1} -|| f(\theta)||_{2,1}} {\alpha}$ 可以得到：

$$ \begin{align}lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{2,1} -|| f(\theta)||_{2,1}} {\alpha} \\
&= lim_{\alpha\rightarrow0} \lambda \cfrac {||\alpha d_{i.}||_{2,1}} {\alpha} \\ &= \lambda ||d_{i.}||_{2,1} \tag{11} \end{align}$$

由此可得：

$$ lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{2,1} -|| \theta||_{2,1}} {\alpha} \\ 
= \sum_{||\theta_{i.}||_{2,1}\neq0} \lambda \cfrac {\theta_{i.}^Td_{i.}}{||\theta_{i.}||_{2,1}} + \sum_{||\theta_{i.}||_{2,1}=0} \lambda ||d_{i.}||_{2,1} \tag{11} $$

3. 第三部分，跟第二部分类似，当 $||\theta_{ij}||_1\neq 0$ 时，针对 $\theta$ 的导数存在，所以 $lim_{\alpha\rightarrow0} \beta \cfrac {||\theta+\alpha d||_1 - ||\theta||_1}{\alpha}$ 这对 $\theta_{ij}$ 的导数等于：

$$ \begin{align} lim_{\alpha\rightarrow0} \beta \cfrac {||\theta_{ij}+\alpha d_{ij}||_1 - ||\theta_{ij}||_1}{\alpha} \\ &=
\beta (\sum_{ij} |\theta_{ij}|)'*d_{ij} \\ &=
\beta sign(\theta_{ij})*d_{ij} \tag{12} \end{align} $$

当 $||\theta_{ij}||_1 = 0$ 时：

$$ \begin{align} lim_{\alpha\rightarrow0} \beta \cfrac {||\theta_{ij}+\alpha d_{ij}||_1 - ||\theta_{ij}||_1}{\alpha} \\ &=
lim_{\alpha\rightarrow0} \beta \cfrac {||\alpha d_(ij)||_1}{\alpha} \\ &=
\beta |d_{ij}| \tag{13} \end{align}$$

由此可得：

$$ lim_{\alpha\rightarrow0} \beta \cfrac {||\theta+\alpha d||_1 -|| \theta||_1} {\alpha} \\ 
= \sum_{||\theta_{ij}||_1\neq0} \beta {sign(\theta_{ij})d_{ij}} + \sum_{||\theta_{ij}||_1=0} \beta |d_{ij}| \tag{14} $$

综上所述，可以得到，无论 $\theta$ 和 $d$ 取何值，$f'(\theta;d)$ 在定义域上都存在，证毕。

### 第二个问题：为什么可以用 $f'(\theta;d)$ 来代替 $f(\theta)$ 来求下降方向 $d$ ?

由于obj函数 $f(\theta)$ 并不是在所有的 $\theta$ 下都有导数，所以寻找“最快下降方向”的本身并不能简单的通过求解负梯度来解决。在一般的 $\theta$ 下，我们可以通过 $-f'(\theta)$ 直接找到“下降”方向 $d$，但当 $f'(\theta)$ 在 $\theta$ 下导数无定义时，我们就无法直接求导来确定下降方向。




