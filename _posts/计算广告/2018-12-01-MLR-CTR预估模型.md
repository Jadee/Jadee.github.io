---
title: MLR-CTR预估模型
date: 2018-12-01
categories: 计算广告
tags:
- 计算广告
- CTR模型
---

# 背景

点击率(Click-Through Rate, CTR)预估是互联网主流应用(广告、推荐、搜索等)的核心算法问题，包括google、facebook、百度等业界巨头对这个问题一直进行着持续的研究和投入。

<!-- more -->

广告领域的CTR预估问题，面临的是超高维离散特征空间中模式发现的挑战：如何拟合现有数据的规律，同时又具备推广性。在MLR之前业界的主流算法是广义线性模型LR(logistic regression，逻辑斯特回归)+人工特征工程。这种解法需要引入大量的领域知识来人工设计特征以及特征之间的交叉组合。**MLR算法**创新地提出并实现了直接在原始空间学习特征之间的非线性关系，基于数据自动发掘可推广的模式，相比于人工来说效率和精度均有了大幅提升。[相关论文](https://arxiv.org/abs/1704.05194)

# 算法特性

## 基础特性

MLR可以看做是对LR的一个自然推广，它采用分而治之的思路，用分片线性的模式来拟合高维空间的非线性分类面，超参数分片数可以较好地平衡模型的拟合与推广能力。下图中，MLR模型用4个分片可以完美地拟合出数据中的菱形分类面。

![avatar](https://images2017.cnblogs.com/blog/975503/201707/975503-20170730210258646-1171778731.png)

MLR算法适合于工业级的大规模稀疏数据场景问题，如广告CTR预估。背后的优势体现在两个方面：

* 端到端的非线性学习。从模型端自动挖掘数据中蕴藏的非线性模式，省去了大量的人工特征设计，这 使得MLR算法可以端到端地完成训练，在不同场景中的迁移和应用非常轻松。  
* 稀疏性。MLR在建模时引入了L1和L21范数正则，可以使得最终训练出来的模型具有较高的稀疏度， 模型的学习和在线预测性能更好。当然，这也对算法的优化求解带来了巨大的挑战。具体细节参见我 们的论文。

## 高级特性

在具体的实践应用中，阿里妈妈精准定向机器学习和排序团队进一步发展了MLR算法的多种高级特性，包括：

* **结构先验**：基于领域知识先验，灵活地设定空间划分与线性拟合使用的不同特征结构。例如精准定向 广告中验证有效的先验为：以user特征空间划分、以ad特征为线性拟合。  
* **线性偏置**：这个特性提供了一个较好的方法解决CTR预估问题中的bias特征，如位置、资源位等等。  
* **模型级联**：MLR支持与LR模型的级联式联合训练，这有点类似于wide&deep learning。在我们的实践 经验中，一些强feature配置成级联模式有助于提高模型的收敛性。  
* **增量训练**：实践证明，MLR通过结构先验进行pretrain，然后再增量进行全空间参数寻优训练，会获得 进一步的效果提升。

# 算法分析

**MLR**整体的思想就是把复杂的整个定义域上的拟合问题，分成小块，每一块上把它近似成一个线性模型来解决，由于局部数据的稳定性，如果划分合理，拟合优秀，那么就可以进行更匹配的拟合，问题就会得到更精确的解。这个思想，也就是常说的“分治”的思想(divide and conquer)。这个比semi-lazy的思路更自然和简单，计算复杂性也好很多。(semi-lazy的思想是用预测用例在训练数据中的k临近来训练一个专门的拟合模型，也是基于局部性原理来思考的，但是计算和存储复杂度都很高，超参数设置不易，且还要求训练数据的分布符合要求)。

mlr中的分块是抽象的，可以是任意的形式，比如硬性分块(分割)，概率分块(类似混合分布)，或者是某些feature的组合来确定的分块(这种意义上来说，感觉rf和gbdt也可以看做是mlr特殊的的一例)

## MLR和其他非线性模型的不同

mlr面对的问题是**“feature规模很大，数据非常稀疏(sparse)情况下的线性不可分”**问题。广告ctr预估正是这样一个问题。之前的方案LR+特征工程，用核函数的方式带来非线性扩展，但是这样就非常依赖特征工程的质量，经验要求高，复杂度也非常高。

* MF本身提供了基于2个特征关系的考虑，并通过类似矩阵分解的形式，提供了适应非线性+稀疏数据的方法，缺点是只能做2维的非线性化，对高维feature不适用，刻画复杂函数的能力有限。  
* FM虽然升了维，但是还是可以做到线性的复杂度，这是FM的很大的优势；但是基础的FM只提供了2个feature之间的关系拟合(说白了就是最高二次函数的拟合)，模型刻画复杂关系的能力还是受到了限制，这一点不如mlr。mlr比FM有更强的特征选择能力，目测看这种能力是由2，1正则项和1阶正则项的组合带来的。  
* SVM是n^2级别的，大规模就不考虑了。  
* GBDT也是很好的非线性，但是也是在小规模feature+很多样本的情况下有很好的效果，随着feature的增加，效果越来越难以保证(全部是低相关性特征，很难在一定树高度下给出好预测；如果增加高度，续联数据又不够，就会变成记忆形，很容易陷入过拟合)

## MLR数学描述

模型的基础数学描述如下：

$$ p(y=1|x)=g(\sum_{j=1}^m\sigma(u_j^Tx)\eta(w_j^Tx)) \tag{1} $$

当一个预测用例到达时，公式中的 $\sigma(u_j^Tx)$ 这一部分相当于将这个用例分配(或者叫对应,映射)到一个区域，而 $\eta(w_j^Tx)$ 这一部分，相当于在这个映射到的区域做一个线性预测。之后把这个用例在所有m个区域上的预测“加权组合”起来，在通过 $g()$ 这个函数，得到用例最终的预测值。

在论文中，作者一般用下面这个公式来实例化上面的抽象公式：

$$ p(y=1|x)=\sum_{i=1}^m\frac {e^{u_i^Tx}}{\sum_{j=1}^me^{u_i^Tx}}*\frac {1}{1+e^{-w_i^Tx}} \tag {2} $$

这个公式相当于相当于：当接到一个预测用例后，先利用softmax看看用例在某个区域的概率，之后在这个区域用后面的LR来预测y的label，之后把所有区域的结果按照用例落在所属区域的概率加权求和，最终得到在给定x的情况下y=1的概率是多少。论文后面的模型公式，基本都是基于这个公式来实例化问题的。

MLR的正式名称是Piece-wise Linear Models(基于分片的线性模型?)，期望是提供高阶非线性函数的模拟能力，能对sparse的高维度feature的数据又和好的拟合，同时用于实际应用的需要，模型希望在防止过拟合的同时得到稀疏解，这部分就要正则来帮忙。总体上看，我们设定了损失函数，在选择合适的正则，就可以提供衡量模型效果的object函数，通过优化object，最终可以得到模型最合适的参数。

mlr最终的object function(损失函数+正则项)如下：

$$ argmin_\theta f(\theta)=loss(\theta)+\lambda||\theta||_{2,1}+\beta||\theta||_1 \tag{3} $$

$$ loss(\theta)=-\sum_{t=1}^n[y_tlog(p(y_t=1|x_t,\theta))+(1-y_t)log(p(y_t=0|x_t,\theta))] \tag{4} $$

loss function选择的是负log似然函数(neg-log-likelihood loss function)：

$$ ||\theta||_{2,1}=\sum_{i=1}^d\sqrt{\sum_{j=1}^{2m}\theta_{ij}^2} \tag{5} $$

$$ ||\theta||_1=\sum_{ij}|\theta_{ij}| \tag{6} $$

这里用了2个正则，第一个是“L2，1”正则，具体的计算方式可以看公式(4)；另外一个就是很常见的“L1”正则。由于模型的关系，对于某个feature k来说，如果任意分片上的线性模型的这个维度的参数不为0，那么这个参数就不能删除了。所以除了“L1”外，作者又加入了“L2，1”正则的restrict，来带来某个参数的2m个参数都趋向于0，从而带来稀疏性，让模型变得简单。

OK，有了objective function，我们就可以最优化损失啦！但是仔细一看，这个obj在整个定义域上是非凸，非平滑的。。。非凸就罢了，这不平滑就是说并不是处处可导啊，那就不能用传统意义上的随机梯度下降啦，因为有的地方对 $\theta$ 来说是没有梯度定义的。

论文提出了一个优化的方法，并给出了这个方法理论证明和公式推导，具体如下：

## 优化方法及证明

### 第一个问题(Lemma 1的证明)

首先，作者定义了一个叫做“方向导数”的东西，即 $f'(\theta,d)$ ，打算用它在遇到一些不光滑点 $\theta$ 时，来帮助找到下降方向d。这里，作者提供了 $f(\theta,d)$ 在定义域上处处可导的证明。

具体的证明如下：

$$ \begin{split} f'(\theta;d) = lim_{\alpha\rightarrow0} \cfrac {f(\theta+\alpha d)-f(\theta
)} {\alpha} \\ = lim_{\alpha\rightarrow0} \cfrac {loss(\theta+\alpha d)-loss(\theta
)} {\alpha} \\ + lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{2,1} -|| f(\theta
)||_{2,1}} {\alpha} \\ + lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{1} -|| f(\theta
)||_{1}} {\alpha} \end{split}  \tag{7} $$ 

1. 这里的第一个部分loss是处处可导的，所以有

$$ lim_{\alpha\rightarrow0} \cfrac {loss(\theta+\alpha d)-loss(\theta
)} {\alpha}=\triangledown{loss(\theta^T)d} \tag{8} $$

2. 对于第二部分，当 $\|\|\theta_{i.}\|\|_{2,1} \neq 0$ 时，针对 $\theta$的导数存在，所以

$$ \begin{align} lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta_{ij}+\alpha d_{ij}||_{2,1} -|| f(\theta_{ij}
)||_{2,1}} {\alpha} &= \lambda (\sum_{i=1}^d\sqrt{\sum_{j=1}^{2m}\theta_{ij}^2})'*d_{ij} \\
&=\lambda \frac {2\theta_{ij}}{2\sum_{i=1}^d\sqrt{\sum_{j=1}^{2m}\theta_{ij}^2}}*d_{ij} \\ 
&= \lambda \frac {\theta_{ij}}{\sum_{i=1}^d\sqrt{\sum_{j=1}^{2m}\theta_{ij}^2}}*d_{ij} \tag{9} 
\end{align}$$

写成向量的形式就是：

$$ \lambda \frac {\theta_{i.}^Td_{i.}}{||\theta_{i.}||_{2,1}} \tag{10} $$

当 $\|\|\theta_{i.}\|\|_{2,1} = 0 $ 时，那说明所有的 $\theta_{ij}$ 都等于0，其中 $1<=j<=2m$，这时，我们根据定义 $lim_{\alpha\rightarrow0} \lambda \cfrac {\|\|\theta+\alpha d\|\|_{2,1} - \|\| f(\theta)\|\|_{2,1}} {\alpha}$ 可以得到：

$$ \begin{align}lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{2,1} -|| f(\theta)||_{2,1}} {\alpha} \\
&= lim_{\alpha\rightarrow0} \lambda \cfrac {||\alpha d_{i.}||_{2,1}} {\alpha} \\ &= \lambda ||d_{i.}||_{2,1} \tag{11} \end{align}$$

由此可得：

$$ lim_{\alpha\rightarrow0} \lambda \cfrac {||\theta+\alpha d||_{2,1} -|| \theta||_{2,1}} {\alpha} \\ 
= \sum_{||\theta_{i.}||_{2,1}\neq0} \lambda \cfrac {\theta_{i.}^Td_{i.}}{||\theta_{i.}||_{2,1}} + \sum_{||\theta_{i.}||_{2,1}=0} \lambda ||d_{i.}||_{2,1} \tag{11} $$

3. 第三部分，跟第二部分类似，当 $\|\|\theta_{ij}\|\|_1\neq 0$ 时，针对 $\theta$ 的导数存在，所以 $lim_{\alpha\rightarrow0} \beta \cfrac {\|\|\theta+\alpha d\|\|_1 - \|\| \theta \|\|_1}{\alpha}$ 这对 $\theta_{ij}$ 的导数等于：

$$ \begin{align} lim_{\alpha\rightarrow0} \beta \cfrac {||\theta_{ij}+\alpha d_{ij}||_1 - ||\theta_{ij}||_1}{\alpha} \\ &=
\beta (\sum_{ij} |\theta_{ij}|)'*d_{ij} \\ &=
\beta sign(\theta_{ij})*d_{ij} \tag{12} \end{align} $$

当 $\|\| \theta_{ij} \|\|_1 = 0$ 时：

$$ \begin{align} lim_{\alpha\rightarrow0} \beta \cfrac {||\theta_{ij}+\alpha d_{ij}||_1 - ||\theta_{ij}||_1}{\alpha} \\ &=
lim_{\alpha\rightarrow0} \beta \cfrac {||\alpha d_(ij)||_1}{\alpha} \\ &=
\beta |d_{ij}| \tag{13} \end{align}$$

由此可得：

$$ lim_{\alpha\rightarrow0} \beta \cfrac {||\theta+\alpha d||_1 -|| \theta||_1} {\alpha} \\ 
= \sum_{||\theta_{ij}||_1\neq0} \beta {sign(\theta_{ij})d_{ij}} + \sum_{||\theta_{ij}||_1=0} \beta |d_{ij}| \tag{14} $$

综上所述，可以得到，无论 $\theta$ 和 $d$ 取何值，$f'(\theta;d)$ 在定义域上都存在，证毕。

### 第二个问题：为什么可以用 $f'(\theta;d)$ 来代替 $f(\theta)$ 来求下降方向 $d$ ?

由于obj函数 $f(\theta)$ 并不是在所有的 $\theta$ 下都有导数，所以寻找“最快下降方向”的本身并不能简单的通过求解负梯度来解决。在一般的 $\theta$ 下，我们可以通过 $-f'(\theta)$ 直接找到“下降”方向 $d$，但当 $f'(\theta)$ 在 $\theta$ 下导数无定义时，我们就无法直接求导来确定下降方向。

怎么办？一种方法是，我们可以看看在所有方向 $d'$ 上，哪个方向可以使 $f$ 的值下降，然后找到所有使 $f(\theta)$ 下降的方向d中，某个方向 $\hat d$ ，使obj的下降的最快，但是“所有方向”的探测基本是不可能的；除此之外，有没有什么方式，可以找到一个“差不多”的方向 $d'$，使得obj可以在这个 $d'$ 方向上也得到不错的下降，之后我们就可以在 $\theta$ 时，用d'来做下降方向呢？

在原论文中，作者在解决obj函数非凸，非光滑的求解问题时，给出了如下的，$f'(\theta)$ 在 $\theta$ 下导数无定义时的解决方案：“Since the negative-gradients of our objective function do not exists for all Θ, we take the direction d which minimizes the directional derivative of f with Θ as a replace.”，翻译过来就是**“由于obj函数的负梯度并不是在全部的 $\theta$ 上存在，我们选取可以最小化参数是 $\theta$ 时的方向梯度导数的方向 $d$ 来替代（也就是选择最小化 $f'(\theta，d)$ 的 $d$ 来作为此时 $f(\theta)$ 的迭代方向）。”**

那么问题来了，为什么在这里，我们可以使用**“使方向梯度函数最小的 $d$ ”**来代替原始的obj函数的迭代方向 $d$？这一点，以下是一些想法分析。

从数学上来看，导数定义式

$$ \begin{align} f'(\theta;d) &= lim_{\alpha\rightarrow0} \cfrac {f(\theta+\alpha d)-f(\theta
)} {\alpha}\\ 
f'(\theta) &= lim_{\alpha\rightarrow0} \cfrac {f(\theta+\alpha)-f(\theta
)} {\alpha} \tag{15} \end{align}$$

按照链式法则，最终可以得到，$f$ 的方向导数，在形式上，和f一般的导数只有一点不同，就是**方向导数等于一般导数在乘以这个方向本身**，即

$$ f'(\theta,d)=f'(\theta)*d \tag{16} $$

我们将函数 $ f(\theta+\alpha d) $ 在 $\theta$ 处做一阶的taylor展开，可以得到

$$ f(\theta+\alpha d) \thickapprox f(\theta)+\cfrac {\alpha}{2}f'(\theta)d \\
=> f(\theta+\alpha d) - f(\theta) \thickapprox \cfrac {\alpha}{2}f'(\theta)d \tag{17} $$

其中 $\frac{\alpha}{2}$ 是标量(正的)，再一看，哇，右边部分除了这个这个标量，不就剩下 $f'(\theta)d$，也就是 $f'(\theta，d)$，我们知道 $f(\theta)$ 的导数不一定存在，但是上面证明了 $f(\theta，d)$ 的导数都存在啊！所以我们这里直接把 $f'(\theta，d)$ 换过来，就有

$$ f(\theta+\alpha d) - f(\theta) \thickapprox \cfrac {\alpha}{2}f'(\theta,d) \tag{18} $$

这样，如果我们找到了一个方向d，沿着各个方向可以使得 $f'(\theta，d) \lt 0$ ，那么就有 $f(\theta+\alpha d)$ 所以，我们就可以通过 $d$ 对 $f'(\theta，d)$ 的影响来找到让 $f$ 下降的方向。如果一个方向 $d$ 使得 $f'(\theta，d) \lt 0$ 的话，我们就称d是一个下降方向。

这里可以看到，可能有好多的 $d$ 都能让 $f'(\theta，d) \lt 0$，那最终我们选哪个 $d$ 呢？当然是选让 $f$ 下降最多的！！！所以这里，我们就选取 $min_df'(\theta,d)$ 来进行迭代了。

在这里，论文作者通过巧妙地运用替换，使得一个非光滑的函数，可以通过求导的方式获类似“最速下降”方向，这一点十分令人佩服。也可能是我孤陋寡闻，个人感觉对 $f'(\theta，d)$ 的构造，还是非常精妙的。

### 第三个问题(如何得到下降方向的解析解)

有了对前两个问题的理解，我们来看看第三个问题，如何的到下降方向d的解析解。

第二个问题已经给出了，要求obj函数的下降方向，我们只需解等价的最小化方向梯度 $f'(\theta，d)$ 的问题即可。

作者在这里给出了obj等于损失函数 $loss(\theta)$ 为可导(平滑)函数，加上用 $L_{2,1}$ 和 L_{1}的优化目标正则项的显示的最优下降方向d的解析解(Proposition 2)。

令 $s=-\triangledown loss(\theta)_{ij}-\lambda \cfrac {\theta_{ij}}{||\theta_{i.}||_{2,1}}，\nu=max\{|-\triangledown loss(\theta)_{ij}|-\beta,0\}sign(-\triangledown loss(\theta)_{ij})$，则：

$$ d_{ij}=\begin{cases} 
s-\beta sign(\theta_{ij}),&\theta_{ij}\neq0\\
max\{|s|-\beta,0\}sign(s),&\theta_{ij}=0,||\theta_{i.}||_{2,1}\neq0\\
\cfrac  {max\{||\nu||_{2,1}-\lambda,0\}}{||\nu||_{2,1}}\nu ,&||\theta_{i.}||_{2,1}=0
\end{cases} \tag{19} $$

下面推导一下公式:

我们要找到期望的 $d$，使obj下降的“最多”，同时又要 $d$ 的大小不能太大(否则迭代在理论上不能在保证一定会收敛），这其实就是个有限制的最优化问题，翻译成数学模型就是下面的公式描述：

$$min_d f(\theta) \\ s.t. ||d||^2 \leq C $$ 

这里要求d的模不能大于C。我们已经知道可以用求 $f'(\theta，d)$ 的最小值，来取代求 $f(\theta)$的最小值，于是公式可写成：

这里要求d的模不能大于C。我们已经知道可以用求 $f'(\theta，d)$ 的最小值，来取代求 $f(\theta)$

$$min_d f'(\theta, d) \\s.t. ||d||^2 \leq C $$

一般解有约束的最优化问题，我么会先用lagrange乘子法，把它转化成无约束的最优化问题，这里通过拉格朗日乘子法，把原问题转化成了如下无约束最优化问题：

$$ L(d,\mu)=f'(\theta,d)+\mu(||d||^2+C) \tag{22} $$ 

其中 $\mu>0$

我们要求 $L$ 的驻点，也就是 $L' = 0$ 时 $\theta$，于是有

$$ L'(\theta,d)=\cfrac {{\rm d}f'(\theta,d)}{{\rm d}d}+2\mu d=0\\
 => 2\mu d=-\cfrac {{\rm d}f'(\theta, d)}{{\rm d}d} \tag{23} $$
 
这里，我们先把问题一得到的 $f'(\theta，d)$ 在任意的 $\theta$ 的公式写一下，方便一会的查阅。

$$ f'(\theta,d)=\triangledown{loss(\theta^T)d} \\ + \sum_{||\theta_{i.}||_{2,1}\neq0} \lambda \cfrac {\theta_{i.}^Td_{i.}}{||\theta_{i.}||_{2,1}} + \sum_{||\theta_{i.}||_{2,1}=0} \lambda ||d_{i.}||_{2,1} \\ + \sum_{||\theta_{ij}||_1\neq0} \beta {sign(\theta_{ij})d_{ij}} + \sum_{||\theta_{ij}||_1=0} \beta |d_{ij}| \tag{24} $$

根据 $\theta$ 的不同，我们来分情况讨论：

a. 当 $\theta_{ij}\neq0$ 时，根据第一个问题的推导，我们带入在 $\theta_{ij}\neq0$ 时的 $f'(\theta，d)$ 得到

$$2\mu d_{ij}=-\triangledown{loss(\theta^T)}_{ij}-\lambda\cfrac {\theta_{ij}}{||\theta_{i.}||_{2,1}}-\beta sign(\theta_{ij} ) \tag{25} $$

b. 当 $\theta_{ij}=0，||\theta_{i.}||_{2,1}\neq0 $ 时，有

$$ 2\mu d_{ij}=-\triangledown{loss(\theta^T)}_{ij}-\lambda\cfrac {\theta_{ij}}{||\theta_{i.}||_{2,1}}-\beta sign(d_{ij} ) \tag{26} $$

由于除了在左边有 $d_{ij}$，公式的右侧还有个 $sign(d_{ij})$，不好直接处理，我们来分情况讨论。我们知道 $\beta \gt 0$，令 $s = -\triangledown{loss(\theta^T)}_{ij}-\lambda\cfrac {\theta_{ij}}{||\theta_{i.}||_{2,1}}$，我们进行如下分析：

* 当 $s>\beta>0$ 时，那么无论 $sign(d_{ij})$ 是大于0还是小于0，公式右边都会是大于0的，这时候 $d_{ij}$ 一定会大于0，可以得到 $2\mu d_{ij}=s-\beta$

* 当 $\beta>s>0$ 时，那么如果 $d_{ij} < 0$，那么左边大于0，右边小于0，矛盾；同理如果 $d_{ij} > 0$，有左边大于0，右边小于0，也矛盾；由于 $s>0$，所以也不可能 $d_{ij} = 0$，这种情况下，无解，也就是说没方向 $d$ 能让 $f$ 下降了，那么 $d$ 就取0即可。

* 当 $ s = 0 $时，很简单，直接可以得到 $d_{ij} = 0$。

* 当 $0 > s > −\beta$ 时，那么如果 $d_{ij} < 0$，那么左边小于0，而右边 $\beta - \|s\|$ 大于0，矛盾；同理如果 $d_{ij} > 0$，有左边大于0，右边小于0，也矛盾；由于 $ s < 0 $，所以也不可能 $d_{ij} = 0$，这种情况下，无解，也就是说没方向 $d$ 能让 $f$ 下降了，那么 $d$ 就取0即可。

* 当 $0 > −\beta > s$ 时，那么无论 $sign(d_{ij})$ 是大于0还是小于0，公式右边都会是小于0的，这时候 $d_{ij}$ 一定会小于0，可以得到 $2\mu d_{ij}=s+\beta=-(|s|-\beta)$

综上所述，当 $\theta_{ij}=0，\|\| \theta_{i.} \|\|_{2,1}\neq0 $ 时，可以得到：

$$ 2\mu d_{ij}=max\{|s|-\beta,0\}sign(s) \\
 = max\{|-\triangledown{loss(\theta^T)}_{ij}-\lambda\cfrac {\theta_{ij}}{||\theta_{i.}||_{2,1}}|-\beta,0\}sign(-\triangledown{loss(\theta^T)}_{ij}-\lambda\cfrac {\theta_{ij}}{||\theta_{i.}||_{2,1}}) \tag{27} $$
 
c. 当 $\theta_{ij}=0 \: and \: \|\| \theta_{i.} \|\|_{2,1} = 0 $ 时，有

$$ 2\mu d_{ij}=-\triangledown{loss(\theta )}_{ij}-\lambda\cfrac {d_{ij}}{||d_{i.}||_{2,1}}-\beta sign(d_{ij} ) \\
 => (2 \mu + \cfrac {\lambda}{||d_{i.}||_{2,1}})d_{ij} = -\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij} ) \tag{28} $$
 
我们看到，想从上面的关于 $d_{ij}$ 的等式，解出 $d_{ij}$ 的解析解非常困难的，因为除了 $d_{ij}$ 之外，里面的 $\|\| d_{i.} \|\|_{2,1}$ 和 $sign(d_{ij})$ 都会受到 $d_{ij}$ 的影响，每当 $d_{ij}$ 变化时，另外两个也更着变化，尤其是 $sign(d_{ij})$，还是非平滑的，很难直接解方程。所以这里想要解出 $d_{ij}$ ，我们要想办法，去掉 $\|\| d_{i.} \|\|_{2,1}$等影响，论文作者的解决思路基本也是顺着这个思想走的。

首先，我们先考虑去掉 $sign(d_{ij})$ 的影响。这里，我们令 $\nu_{ij} =  -\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij})$，由于 $\mu > 0，\lambda > 0，\|\| d_{i.} \|\|_{2,1} > 0$，所以 $2 \mu + \cfrac {\lambda}{ \|\| d_{i.} \|\|_{2,1} } > 0 $，由此可以得到 $d_{ij}$ 和 $-\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij})$，也就是 $\nu_{2,1}$ 必须同号，否则的话 $d_{ij}$ 无解，也就是没有方向使obj下降，于是$d_{ij}$ 取0即可, 和b的情况相似，我们分情况讨论一下：

* 当 $d_{ij}>0$ 时，要求 $\nu_{ij}=-\triangledown{loss(\theta)}_{ij} - \beta sign(d_{ij})>0  => -\triangledown{loss(\theta )}_{ij}<-\beta $

* 当 $d_{ij}=0$ 时，要求 $\nu_{ij}=-\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij})=0  => -\triangledown{loss(\theta )}_{ij}=0$

* 当 $d_{ij}<0$ 时，要求 $\nu_{ij}=-\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij})<0  => \triangledown{loss(\theta )}_{ij}>\beta$

综合起来，可以得到

$$ \begin{align}\nu_{ij} &= \begin{cases}(|-\triangledown{loss(\theta)}_{ij}|-\beta )*sign(-\triangledown{loss(\theta^T)}_{ij}),&|-\triangledown{loss(\theta)}_{ij}|>\beta\\
 0,&otherwise \end{cases} \\ 
 &=> \nu_{ij}=max(|-\triangledown{loss(\theta )}_{ij}|-\beta )*sign(-\triangledown{loss(\theta )}_{ij}),0) \tag{29} \end{align}$$
 
这样，通过替换，就可以把 $sign(d_{ij})$ 的影响去除掉。然后，我们看如何去除 $\|\| d_{i.} \|\|_{2,1}$ 的影响，继续对公式进行推导

$$ (2 \mu + \cfrac {\lambda}{||d_{i.}||_{2,1}})d_{ij} = -\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij} ) \\
 => (2 \mu ||d_{i.}||_{2,1}+\lambda )d_{ij} = (-\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij} ))||d_{i.}||_{2,1} \\
 => (2 \mu ||d_{i.}||_{2,1}+\lambda )d_{ij} = \nu_{ij} ||d_{i.}||_{2,1} \tag{30} $$
 
这个是只针对 $d_{ij}$，我们把它推广到 $d_{i}$ 上，也就是向量模式。对于合法的 $j$，我们都有

$$ (2 \mu ||d_{i.}||_{2,1}+\lambda )d_{i.} = \nu_{i.} ||d_{i.}||_{2,1} \tag{31} $$

其中 $(2 \mu \|\| d_{i.} \|\|_{2,1} + \lambda )$ 和 $\|\| d_{i.} \|\|_{2,1}$ 都是标量，$d_{ij}$ 和 \$nu {i.}$ 是向量，我们对两边都去 $L_{2,1}$范数，有：

$$ (2 \mu ||d_{i.}||_{2,1}+\lambda )*||d_{i.}||_{2,1} = ||\nu_{ij}||_{2,1}*||d_{i.}||_{2,1} \\
 => (2 \mu ||d_{i.}||_{2,1}+\lambda ) = ||\nu_{ij}||_{2,1} \\ 
 => 2 \mu ||d_{i.}||_{2,1} = ||\nu_{ij}||_{2,1} - \lambda  \tag{32} $$
 
因为 $\|\| d_{i.} \|\|_{2,1} \geq 0$， 所以：

* 当 $\|\| \nu_{ij} \|\|_{2,1} - \lambda < 0$ 时，无解，于是没有方向 $d$ 使得函数下降，取 0，即 $d_{ij} = 0，\forall \: j$，此时 $d_{i}.$ 的各个元素都是0，也即 $d_{i.}=[0\cdot\cdot\cdot0]^T$

* 当 $\|\| \nu_{ij} \|\|_{2,1} - \lambda > 0$ 时，无解，此时 $2 \mu \|\| d_{i.} \|\|_{2,1} = \|\| \nu_{ij} \|\|_{2,1} - \lambda$ 

综上所述，最终可以得到：

$$ 2 \mu ||d_{i.}||_{2,1}=max(||\nu_{i.}||_{2,1}-\lambda , 0) \tag{33} $$

这样 $\|\| d_{i.} \|\|_{2,1}$ 和 $sign(d_{ij})$ 就都能通过 $\nu$来替换了，把它带入最原始的公式

$$ \begin{align} (2 \mu + \cfrac {\lambda}{||d_{i.}||_{2,1}})d_{ij} = -\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij} ) 
\\ &=> (2 \mu ||d_{i.}||_{2,1}+ \lambda)d_{ij} = (-\triangledown{loss(\theta )}_{ij} - \beta sign(d_{ij} ))*||d_{i.}||_{2,1}
\\ &=> (2 \mu ||d_{i.}||_{2,1}+ \lambda)d_{ij} = \nu_{ij} ||d_{i.}||_{2,1}
\\ &=> ||\nu_{i.}||_{2,1} d_{ij} = \nu_{ij}* \cfrac {max(||\nu_{i.}||_{2,1}-\lambda,0)}{2\mu }
\\ &=> 2 \mu d_{i,j} = \cfrac {max(||\nu_{i.}||_{2,1}-\lambda,0)}{||\nu_{i. }||_{2,1}}\nu_{ij} \tag{34} \end{align}$$

到此，我们推导出了所有情况下 $d_{ij}$ 的解析解：

$$ 2 \mu d_{ij}=\begin{cases} 
s-\beta sign(\theta_{ij}),&\theta_{ij}\neq0\\
max\{|s|-\beta,0\}sign(s),&\theta_{ij}=0,||\theta_{i.}||_{2,1}\neq0\\
\cfrac  {max\{||\nu_{i.}||_{2,1}-\lambda,0\}}{||\nu_{i.}||_{2,1}}\nu_{ij} ,&||\theta_{i.}||_{2,1}=0
\end{cases}   \tag{35} $$

其中 $s=-\triangledown loss(\theta)_{ij}-\lambda \cfrac {\theta_{ij}}{ \|\| \theta_{i.}\|\|_{2,1}}$，$\nu_{ij }=max\{\|-\triangledown loss(\theta)_{ij}\|-\beta,0\}sign(-\triangledown loss(\theta)_{ij})$ 

由于 $2\mu$ 是标量，不影响方向，后续跟新时相当于对学习率的调整，所以这里可以直接舍弃，所以最终有：

$$ d_{ij}=\begin{cases} 
s-\beta sign(\theta_{ij}),&\theta_{ij}\neq0\\
max\{|s|-\beta,0\}sign(s),&\theta_{ij}=0,||\theta_{i.}||_{2,1}\neq0\\
\cfrac  {max\{||\nu_{i.}||_{2,1}-\lambda,0\}}{||\nu_{i.}||_{2,1}}\nu_{ij} ,&||\theta_{i.}||_{2,1}=0
\end{cases}   \tag{36} $$

证毕。

论文中提到，和一般的针对特殊问题的特殊解法，这种通过寻找“梯度方向导数”来解决非凸，非光滑的方法，是一种通用的方法，非常有价值。

## 扩展到2阶拟牛顿法(L-BFGS)

找到了梯度方向 $d^k$，我们就可以按照一般sgd的方式求解obj函数的最优值，也可以通过转换，用二阶的拟牛顿法，来更新参数。作者在这里提到了通过 近似obj函数的hessian矩阵的拟，来用limited-memory quase-newton method（L-BFGS）的方法来更新模型参数，用来加速收敛。
通过类似OWLQN方法来进行二阶优化比较常见，这里不再做详细的说明，感兴趣的同学可以自行查阅相关资料即可，这里推荐柳超博士的[《逻辑回归：从入门到精通》](http://blogs.softwareclue.com/wp-content/uploads/2016/05/LR_intro.pdf)中对二阶方法的部分，从思想来源，到原理，再到推导，写的很详细，赞一个。

作者在这里还加入了更新中对参数sign的限制，要求 $\theta_{ij}^{(k)}\neq0$，要求更新后的 $\theta_{ij}^{(k+1)}$ 和 $\theta_{ij}^{(k)}$一致；只有 $\theta_{ij}^{(k)} = 0$时，才根据 $d_{ij}^{(k)}$ 来决定新的 $\theta_{ij}^{(k+1)}$的象限。

具体的更新方向，和保持 $H_k$ 半正定的trick，通过line search找到最佳的学习率 $\alpha$等，详见论文，这里不再赘述。

# 实现和结论

具体的实现，篇幅关系，不在本文讨论。具体的效果结论，作者除了讨论了相对于以往LR的效果上的进步之外，还分析了分块数m对auc的影响

![avatar](/images/计算广告/ad-25.png)

正则的稀疏能力

![avatar](/images/计算广告/ad-26.png)

使用feature trick后对计算的影响

![avatar](/images/计算广告/ad-25.png)

# Reference

1. <https://blog.csdn.net/hellozhxy/article/details/82562170>


