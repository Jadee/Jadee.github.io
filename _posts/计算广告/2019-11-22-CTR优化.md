---
title: CTR优化
date: 2019-11-22
categories: 计算广告
tags:
- 计算广告
- CTR
---

# 优化方向

<!-- more -->

## 样本

* 丰富样本多样性

## 特征

* 丰富特征细腻度 

## 模型

* 提升模型学习能力

### 正负样本比例完全失调

常用方法：采样（会导致一定程度过拟合）

优化思路：帮助模型集中于训练更加困难的样本

传统使用交叉熵来作模型训练的loss。公式如下所示:

![avatar](https://upload-images.jianshu.io/upload_images/2198384-f4fe88740103b4b6.png?imageMogr2/auto-orient/strip|imageView2/2/w/415/format/webp)

引入focal loss定义如下：

$$ FL (𝑝_𝑡) = −\alpha_𝑡 (1 − 𝑝_𝑡 )^{\gamma} log⁡(𝑝_𝑡) $$ 

1. 其中 $\alpha_t$ 是一个范围是 0-1 的参数，用来控制正负样本贡献在总 loss 的权重

2. $(1 − 𝑝_𝑡 )^{\gamma}$ 叫做调制系数，它有如下两个性质：

  * 以正样本为例。当样本是易分样本时，$p_t$ 接近1，调制系数比较小，表示置一个小权重给这个样本；相反，当样本是难分样本时，$p_t$ 接近 0.5（如果分错 $p_t$ 小于 0.5），使得调制系数比较大，示置一个大权重给这个样本
  
  * 当 $\gamma = 0$ 是，focal loss 等价于交叉熵，$\gamma$ 越大，调制系数会引起难易样本权重区分越大。
  
相关论文：[Focal Loss for Dense Object Detection](https://www.jianshu.com/p/204d9ad9507f)

 
