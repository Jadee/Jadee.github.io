---
title: DIEN兴趣演化模型介绍
date: 2018-11-21
categories: 计算广告
tags:
- 计算广告
- CTR
---

# 相关链接

相关论文链接和开源代码地址如下：

<https://arxiv.org/abs/1809.03672>

<https://github.com/mouna99/dien>

<!-- more -->

# 背景
在展示广告领域，CPC (cost per click) 是最常见的计费方式之一。这种计费方式下广告主会为每一次点击付费，同时系统需要预估每次展现广告的点击率来决定展示哪一个广告。点击率预估模型的性能不仅会影响平台的收入同时也会关系到用户的体验，因此点击率预估是非常核心的一项任务。

展示广告场景下，点击率预估模型的目标是预估给定广告、场景、用户的情况下该用户会点击该广告的概率，即 $p(y = 1 \| ad, scene, user)$ 。进年来随着深度学习的发展，点击率预估模型也逐渐从传统的LR、FM、MLR[1] 等模型迁移到深度模型如Wide&Deep[2]、DeepFM[3]等。Wide&Deep、DeepFM等模型都可以看做是一种Embedding&MLP的范式（如下图），先通过embedding将输入的特征ID等高维的one-hot向量投影到一个低维稠密的embedding空间最后concat成一个向量表达，再通过几层多层感知机来根据输入的向量表达预测点击率。这些模型更多是面向了通用的问题采用了普适的设计。

![avatar](/images/计算广告/ad-15.png)

但是在电商场景下，会更多的关注用户个性化需求，需要更多地通过用户的历史行为去建模用户、理解用户需求，类似Wide&Deep、DeepFM的模型就相对缺乏对这一块的显式建模。之前我们的工作深度兴趣网络DIN[4] (Deep Interest Net)提出在电商场景下，用户会同时存在多种多样的兴趣，但是当用户在面对一个具体商品的时候只有部分和此商品相关的兴趣会影响用户的行为（点击or不点击等等）。DIN提出了一个兴趣激活的模块（如下图），利用attention的机制根据被预测的商品 candidate C 激活用户相关的历史行为，然后用这些相关的行为表达的加权求和来表达用户和此商品 C 相关的兴趣。相比以往的模型需要用一个固定的向量表达用户所有的兴趣，DIN通过兴趣激活模块能根据具体的候选商品C表达用户与此次预测相关的兴趣，这样的设计降低了模型表达用户兴趣的难度。

![avatar](/images/计算广告/ad-16.png)

然而DIN也有其不足。

* 首先，DIN的设计中直接将一个具体的用户行为（如点击一个具体的商品）当做了用户的兴趣。用户的兴趣其实是一个更为抽象的概念，而某个历史的行为只是抽象的兴趣一个具体的体现。比如用户购买了某款连衣裙，其背后可能是因为触发了用户对颜色、品牌、风格、当季相关的隐藏兴趣，而用户的兴趣并不仅仅局限于这一款具体的商品。因此我们设计了一个兴趣提取模块用于从用户具体的行为中抽取出每一个行为背后用户隐藏的兴趣。

* 其次DIN缺乏对用户兴趣演化的构建。在淘宝这样综合性的电商网站场景，用户的历史行为是很丰富多样的，隐藏在这背后的用户兴趣也是多元的。假设可以用一个兴趣空间来表达所有的兴趣， 某个用户的一段历史行为序列是：

$$ S = [A1，A2，B1，B2，A3，A4，B3，B4] $$

其中 $A，B \in I$ 表达假设的兴趣空间内两种不同的兴趣：

$$ [A1，A2，A3，A4] \in A$$

$$ [B1，B2，B3，B4] \in B$$

可以认为是在不同兴趣上的行为采样点。

由于用户可能具有很多不同的兴趣，因此用户的历史行为可以被看做是很多个兴趣的很多采样点混合在一起的综合序列。DIN中提出用单纯序列建模如RNNs（GRU, RNN, LSTM）来建模这样一个综合序列是不太合理的，不同于语音或者文本信号有非常强的序列信息，电商场景下用户的行为序列信号中会有非常频繁且随机的跳转噪声。比如 $S$ 中的 $A2$ 到 $B1$ 的跳转可能是非常随机的（例如某用户正在选窗帘突然想购买个减肥饮品；又如：用户选择旅游产品的时间跨度会很长，在此期间可能能会买些别的商品)。

然而我们又明确的知道有信息隐含在这些序列当中，比如用户某方面兴趣的演化。举个例子，用户的穿衣风格可能是在逐渐变化的，这个变化的过程会在用户的历史行为中体现出来，如果能去掉历史行为中其他暂时不相关的行为，就能提取出用户的穿衣兴趣演化过程。 同时ctr预估任务中，模型只需要关注用户是否会点击给定的某个商品，换句话说，对于某一个具体的候选商品 candidate C，只需要建模和它相关的兴趣演化过程。形式化地，假设：

$$ candidate C \in A$$

就只需要关心 $[A1，A2，A3，A4] \in A$ 这个子序列的演化。

因此，兴趣演化模型（DIEN）由两个核心工作组成：

* 一是兴趣提取模块用于建模隐藏在用户具体历史行为后的抽象用户兴趣；

* 二是建模广告相关的用户兴趣的演化过程和趋势的兴趣演化模块。

# 兴趣演化模型

## 模型结构

![avatar](/images/计算广告/ad-17.png)

由下往上，其中广告特征，场景特征以及用户的一般画像特征都和大多数做法一样，即取完 Embedding 后直接 concat 一起，然后送入全连接层中。用户的历史行为则在通过 Embedding 层后需要先经过兴趣提取层获取每个行为背后的兴趣表达 $h_i$，然后通过给定的广告 ad 与每个兴趣状态 $h_i$ 做 attention 操作来获得每个兴趣状态与该广告 ad 的相关程度 $a_i$，最后所有时刻的 $a_i$ 和 $h_i$ 通过兴趣演化层得到用户最终的广告相关的兴趣表达 $h'(T)$，这个兴趣表达和其他特征的 Embedding 会 concat 在一起送入全连接层中。

## 兴趣提取模块

兴趣提取模块由一层GRU构成，其输入是用户历史行为序列 $B = [b_1，b_2，\ldots，b_T]$，每次行为是一个time step。DIEN设计希望第一层GRU的每一个time step分别表达这个时刻用户的兴趣。因此在每个time step引入了辅助 loss 来帮助刻画每个兴趣隐状态 $h_i$，即要求:

$$ f(h_i，e_{i+1}) = 1 $$

$$ f(h_i，e_{i+1}^{'}) = 0 $$

其中 $e_{i+1}$ 是 $x_{i+1}$ 对应的 Embedding，$e_{i+1}^{'}$ 是 $i + 1$ 时刻随机采样的未发生的行为所对应的Embedding，$f$ 可以是一个简单的全连接网络抑或是一个内积运算。

兴趣提取模块使用GRU的隐状态来表达用户隐藏在行为背后的兴趣，并且使用辅助loss来约束这个隐状态的学习（即通过给定每个隐状态以及一个行为能够准确的预测出用户是否会发生这次行为）：从兴趣提取的角度来讲负采样辅助loss能够约束GRU的每个隐状态更好的表达用户此刻的兴趣。如果不加入这个loss，所有的监督信号都源于最后的点击样本。

点击率预估大多数情况下都会采用某个具体场景的样本，而希望通过某个具体场景样本的反馈信号能提取到用户每一个行为状态背后的兴趣是不现实的，辅助loss的设计用一种优雅的方式引入了用户的全网行为反馈信息，同时不会引入多场景之间的点击bias以及造成多场景耦合；从优化的角度来讲负采样辅助loss可以在gru的长序列建模中减少梯度反向传播的难度；最后负采样辅助loss能提供更多语义信息来帮助Embedding层的学习，能够学习到更好的Embedding表达。

## 兴趣演化模块

由于用户的兴趣具有多样性，用户的历史行为序列是多种兴趣的采样混合在一起的综合序列，这个序列会存在很多跳变的点（如背景所介绍），因此很难用一个序列建模的方式把这个综合序列的趋势学习好。好在ctr任务中需要关注的是给定一个广告用户会不会点击它，因此可以从给定广告出发，只将和广告相关的那些兴趣态连成一个子序列进行建模，这样就能建模和广告相关的兴趣的演化趋势了。

首先这里用广告 $ad$ 与兴趣提取模块产生的兴趣态表达序列 $H = [h_1，h_2，\ldots，h_T]$ 进行attention操作，得到每个兴趣态与广告 $ad$ 的相关程度 $a_{i}:

$$ a_i = \frac{exp(h_i W e_{ad})}{\sum_{j = 0}^T exp(h_i W e_{ad})} $$

其中 $e_{ad}$ 代表广告的Embedding，$W$ 是参数矩阵。由于attention的方式得到的是一个soft的相关权重序列，与广告相关度高的兴趣态其attention score 较大，反之较小。为了能够达到“只将和广告相关的那些兴趣态连成一个子序列进行建模”这样的效果，我们需要设计一种序列建模方式，它能利用attention score的大小来决定序列建模过程中hidden state被更新的力度，即我们希望与广告更相关的那些兴趣态能够更大力度的更新演化序列的hidden state，与广告不相关的兴趣态能够较小力度甚至不更新演化序列的hidden state。

我们使用一种改进的gru结构来实现这个目的。标准的 GRU 更新公式是：

$$ u_t = \sigma(W^u i_t + U^u h_{t-1} + b^u) $$

$$ r_t = \sigma(W^r i_t + U^r h_{t-1} + b^r) $$

$$ \hat{h}_t = tanh(W^h i_t + r_t \circ U^h h_{t-1} + b^h) $$

$$ h_t = (1 - u_t) \circ h_{t-1} + u_t \circ \hat{h}_t $$

从上述公式可以看到，更新门 $u_t$ 如果等于0向量，那么 $h_t$ 就能保持不更新，即等同于 $h_{t-1}$；反之，隐藏层状态就会受当前时刻的输入影响而得到更新。 因此直观地，这里我们可以使用attention score $\alpha_t$ 来作用于 $u_t$, 让 $u_t$ 的scale受限于兴趣态与广告的相关度，即：

$$ u_t^{'} = u_t * \alpha_t $$ 

$$ h_t = (1 - u_t^{'}) \circ h_{t-1} + u_t^{'} \circ \hat{h}_t $$

以此来达到“与广告越相关的行为更新隐状态向量越多”的目的。

# 实验

具体实验记录详见论文

# 生产化

离线实验结果可以看到兴趣演化模型的提升是显著的，但是这样复杂结构的模型其计算复杂度也非常高，加之精准定向的广告请求对并发度和延时的要求非常苛刻，为了能够达到在线实时服务的要求，我们主要使用了以下的优化手段：

* 首先在工程实现上，我们对模型中的计算进行充分的合并以减少gpu对kernel的调用次数；  
* 将可以并行的计算尽量的并行起来（比如gru的三个门可以拼在一起一次算完）；  
* 不同用户的请求可以batch在一起充分的利用gpu；使用blaze对计算进行加速等；  
* 其次在模型本身上我们也进行了一系列的工作来减小模型的inference压力，我们采用了之前在AAAI2018中提出的Rocket Launching[5]的方法让一个小模型在训练过程中跟随大模型学习。

最终用更小的参数量拟合到和大模型差不多的效果；使用半精度压缩模型的Embedding参数，大大缩小模型的大小，减小在线服务的内存压力，等等。通过这些努力，我们将模型的inference延时从38.2ms压缩到5ms，qps达到800+，达到上线要求。

# 相关工作

[1] Kun Gai, Xiaoqiang Zhu, et al. 2017. Learning Piece-wise Linear Models fromLarge Scale Data for Ad Click Prediction. arXiv preprint arXiv:1704.05194 (2017).  
[2] Cheng, H.-T.; Koc, L.; Harmsen, J.; Shaked, T.; Chandra, T.;Aradhye, H.; Anderson, G.; Corrado, G.; Chai, W.; Ispir, M.;et al. 2016. Wide & deep learning for recommender systems.In Proceedings of the 1st Workshop on Deep Learning forRecommender Systems, 7–10. ACM.  
[3] Guo, H.; Tang, R.; Ye, Y.; Li, Z.; and He, X. 2017. Deepfm:a factorization-machine based neural network for ctr predic-tion. In Proceedings of the 26th International Joint Confer-ence on Artificial Intelligence, 2782–2788.  
[4] Zhou, G.; Zhu, X.; Song, C.; Fan, Y.; Zhu, H.; Ma, X.; Yan,Y.; Jin, J.; Li, H.; and Gai, K. 2018c. Deep interest net-work for click-through rate prediction. In Proceedings ofthe 24th ACM SIGKDD International Conference on Knowl-edge Discovery & Data Mining, 1059–1068. ACM.  
[5] Zhou, G.; Fan, Y.; Cui, R.; Bian, W.; Zhu, X.; and Gai, K.2018b. Rocket launching: A universal and efficient frame-work for training well-performing light net. In Proceedingsof the 32nd AAAI Conference on Artificial Intelligence.
