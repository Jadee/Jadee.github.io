---
title: OCPC 强化学习技术应用初探
date: 2019-03-01
categories: 计算广告
tags:
- 计算广告
- 机制设计
---

# 问题背景

在广告业务中，广告主会对广告给出一个固定出价，作用在指定的场景和定向类型下。如果广告主能够根据每一条流量的价值进行单独出价，可以带来两点好处：

<!-- more -->

* 广告主可以在各自的高价值（如点击、成交）流量上提高出价，而在普通流量上降低出价，如此容易获得较好的 ROI；
* 流量细分后，平台能够提升广告与访客间的匹配效率，体现为 CTR、GMV 等用户指标提升，而在大盘 ROI 不变的情况下，商业指标 RPM 也能相应提升。

在之前我们有对智能调价方面进行了深入而卓有成效的技术探索，具体详见上一篇文章。本文是在已有的智能调价系统基础上的改进工作，在正式介绍之前，我们很有必要从目标和做法两方面回顾一下原系统。

智能调价系统的目标可以简单概括为：提升大盘 RPM、GMV，保障每个广告主的 ROI。我们的改进工作仍旧以此为目标。

做法上，原系统主要分为三个步骤

1. 每个广告根据预估 GMV 决定自己的调价，结合预估 CTR 得到一次排序；  
2. 计算每个广告在保障 ROI 前提下的最大调价值；  
3. 在保障步骤 1 排序的前提下，将每个广告的出价尽量调节到步骤 2 的最大调价值上。  
这个做法将 GMV、RPM 两指标的优化分成单独的阶段，用保序将两阶段串联起来，这不可避免会带来优化效果上的损失。如何同时考虑 RPM 优化、GMV 优化 及 ROI 约束是我们改进工作的任务之一。

此外，原系统的参数调优都是通过离线模拟完成的，也就是说改变计算公式、改变目标或者预估模型升级，参数都需要手动模拟适配。这种迭代方式有着诸多不便

1. 离线模拟结果不代表在线结果，离线模拟最优参数很可能不是线上最优；  
2. 新想法尝试及模型升级都需要手动适配参数，加重了人的负担；  
3. 人工调参的低效率决定了系统可调参数一定是较少的。

总结以上，我们本次改进的目标是，将之前的基于预估值反馈的、多目标分离优化的离线学习，变为基于线上真实效果反馈的、多目标联合优化的在线实时学习。强化学习技术显然和我们的需求很匹配。

# 强化学习建模思考

强化学习有五元组这个概念，状态、动作、奖赏、转移矩阵和折扣系数，建模就是要想清楚它们在我们的实际业务中对应着什么。

**奖赏**：在智能调价系统中就是大盘 RPM、GMV 和 广告主 ROI 约束。然而，奖赏也不简单，一方面因为是多目标优化，另一方面奖赏设计很大程度影响着学习过程和结果，这些我们会在后文谈到。

**动作**：在智能调价系统中就是调价的抓手，而这个抓手就是流量价值的系数。GMV、CTR、CVR、BID 等都是我们关注的价值。

**状态**：一方面要存在自然的状态转移链，另一方面要包含足够的信息让算法去决策动作，进而获得最大的长期回报。智能调价系统中存在两种状态

1. 用户状态：用户购买力、类目偏好、近期行为等有着自然的转移，不同状态下的调价策略似乎应该是不同的，这主要会影响奖赏中的 RPM、GMV；  
2. 广告主状态：这是广告业务中特有的，广告主的账户余额、当前 ROI 等包括当前时间都是自然转移的状态，不同状态下的调价策略也应该是不同的，这主要会影响奖赏中的 ROI。

基于上述分析，我们可以大致描述一个**理想化**的强化学习故事。对于来到广告位的每一个访客，我们根据他们的当前状态去决定如何操作调价抓手，给他们展现特定的广告，引导他们的状态向我们希望的方向上做一步转移。我们的策略会持续引导每一位访客，并最终带来丰厚的长期回报（RPM、GMV）。然而，这个持续引导不是没有代价的，一部分广告主会在这个 “调价-引导” 的过程中损失自己的 ROI。因此，我们在调价中也要考虑到不同广告主的当前状态，基于状态决定他们的实时调价风险偏好，进而获得一个令广告主满意的长期 ROI。

之所以说这个故事比较**理想化**有这么几点原因

1. 广告场景下的行为只是用户全淘行为（淘外因素也可能很大）的一小部分，我们能够对用户施加长期引导可能是一个伪命题，也就是说我们在广告场景观察到的用户状态是全淘行为的随波逐流，而不是我们的引导结果；  
2. 假设用户状态转移在广告场景下是真实的，但转移的并不一定是有效的，那么哪些用户状态是能够用于决策并最终影响奖赏的呢，这需要我们有一种验证方式；  
3. 广告系统相对封闭，所以广告主状态应该是真实的，但状态转移相比用户状态稍有不同，因为在很短的时间间隔就可能有多个请求要处理同一个广告主的广告，所以广告主的状态转移实际上是时间点上的采样；  
4. 我们希望让广告主对长期 ROI 满意，但这个满意度度量有一定的困难：首先 ROI 不像 GMV、RPM 一样是一个分段可加值，直加直减的评估未必合理；其次，ROI 的积累变化很慢，因为购买很稀疏，在非热门广告上更少，实验周期很长；最后，我们要在 ROI 一致的情况下和原方法比 RPM、GMV 提升，但我们很难衡量根据强化学习方法控制的 ROI 相比原方法是松了还是紧了，因此评价就很难做到客观公正。

可以说，我们对于应用强化学习抱有谨慎甚至还有一点怀疑的态度，我们希望通过尽可能严密的推进步骤设计和严谨的实验分析，逐渐摸清强化学习在我们业务中的正确打开方式，这是我们工作的方法论。到真正意义的强化学习，有三步走

* 无用户状态，折扣系数为 0；  
* 有用户状态，折扣系数为 0；  
* 有用户状态，折扣系数非 0。

第一步可以认为是一个进化算法，没有状态，只学一个全局最优的动作；第二步根据状态学习动作，episode 长度为 1；第三步才是典型意义的强化学习。这三步中的状态特指用户状态，尚未考虑广告主状态，因为我们为了让对比更公正，ROI 保障维持原方法不变。

我们现在已经在第一步做出了较大的效果提升，第二步正在实验，第三步是再下一阶段的工作。只有每一步取得相比上一步更大的提升，才说明这一步新加入的因素（如状态，状态转移）是真正有用的。举一个例子，我们如果跳过第一步直接到第二步，一个最简单的方式是按照用户状态（如用户的购买力、长期或实时的偏好类目）划分流量，在每份流量上都执行第一步。根据我们目前的在离线实验经验，这样做也是能获得相对原方法的提升的，但目前还没有发现哪种状态划分使得提升幅度高于不划分的情况。可能是状态确实不行，也可能是我们的用法有问题，还需进一步探索。

接下来，我们主要介绍在第一步方面的一些工作，包括系统设计、奖赏设计和一些实验心得。

# 系统设计


