---
title: OCPC 强化学习技术应用初探
date: 2019-03-01
categories: 计算广告
tags:
- 计算广告
- 机制设计
---

# 问题背景

在广告业务中，广告主会对广告给出一个固定出价，作用在指定的场景和定向类型下。如果广告主能够根据每一条流量的价值进行单独出价，可以带来两点好处：

<!-- more -->

* 广告主可以在各自的高价值（如点击、成交）流量上提高出价，而在普通流量上降低出价，如此容易获得较好的 ROI；
* 流量细分后，平台能够提升广告与访客间的匹配效率，体现为 CTR、GMV 等用户指标提升，而在大盘 ROI 不变的情况下，商业指标 RPM 也能相应提升。

在之前我们有对智能调价方面进行了深入而卓有成效的技术探索，具体详见上一篇文章。本文是在已有的智能调价系统基础上的改进工作，在正式介绍之前，我们很有必要从目标和做法两方面回顾一下原系统。

智能调价系统的目标可以简单概括为：提升大盘 RPM、GMV，保障每个广告主的 ROI。我们的改进工作仍旧以此为目标。

做法上，原系统主要分为三个步骤

1. 每个广告根据预估 GMV 决定自己的调价，结合预估 CTR 得到一次排序；  
2. 计算每个广告在保障 ROI 前提下的最大调价值；  
3. 在保障步骤 1 排序的前提下，将每个广告的出价尽量调节到步骤 2 的最大调价值上。  
这个做法将 GMV、RPM 两指标的优化分成单独的阶段，用保序将两阶段串联起来，这不可避免会带来优化效果上的损失。如何同时考虑 RPM 优化、GMV 优化 及 ROI 约束是我们改进工作的任务之一。

此外，原系统的参数调优都是通过离线模拟完成的，也就是说改变计算公式、改变目标或者预估模型升级，参数都需要手动模拟适配。这种迭代方式有着诸多不便

1. 离线模拟结果不代表在线结果，离线模拟最优参数很可能不是线上最优；  
2. 新想法尝试及模型升级都需要手动适配参数，加重了人的负担；  
3. 人工调参的低效率决定了系统可调参数一定是较少的。

总结以上，我们本次改进的目标是，将之前的基于预估值反馈的、多目标分离优化的离线学习，变为基于线上真实效果反馈的、多目标联合优化的在线实时学习。强化学习技术显然和我们的需求很匹配。

# 强化学习建模思考

强化学习有五元组这个概念，状态、动作、奖赏、转移矩阵和折扣系数，建模就是要想清楚它们在我们的实际业务中对应着什么。

奖赏很简单，在智能调价系统中就是大盘 RPM、GMV 和 广告主 ROI 约束。然而，奖赏也不简单，一方面因为是多目标优化，另一方面奖赏设计很大程度影响着学习过程和结果，这些我们会在后文谈到。

动作，在智能调价系统中就是调价的抓手，而这个抓手就是流量价值的系数。GMV、CTR、CVR、BID 等都是我们关注的价值。

状态，一方面要存在自然的状态转移链，另一方面要包含足够的信息让算法去决策动作，进而获得最大的长期回报。智能调价系统中存在两种状态
