---
title: OCPC_4_多目标策略的演进优化
date: 2019-11-21
categories: 计算广告
tags:
- 计算广告
- 机制设计
---

# 背景介绍

竞价广告市场主体由三方构成：广告主端规模超百万，营销诉求多样；消费者端日请求数十亿，需求各有不同；平台的目标在于通过满足广告主需求以及平衡消费者体验，获得更高的营收。平台为了促成竞价市场的繁荣，主要通过市场竞争和宏观调控来实现。市场竞争是核心的手段，通过促使广告主对流量的竞争实现资源的分配和定价。宏观调控是辅助手段，优化分配效率，提高平台整体价值如GMV，从而提升流量变现的上限。如何通过OCPC出价能力实现以上目标是本文讨论的问题。

<!-- more -->

* 市场竞争：广告主竞价过程中倾向于以更低的成本获得最大的收益，平台为了保证流量不被贱卖，设计了分配和扣费的市场机制，使得广告主以自己的成本价报价为一种占优的策略[1]，从而激励广告主报出更高的价格。但是流量本身高度动态，价值难以量化，广告主难以实现个体理性的报价，OCPC的一个重要任务就是帮助广告主实现个体理性的报价，这构成了CIA(1)的核心，参见**CIA_1_基于OCPC的自动挡出价方案**。以此为基础，CIA(2)仍从竞争角度出发，面向广告主不同诉求实现了计划粒度的出价算法框架，参见**CIA_2_面向诉求的搜索广告出价框架**。为了支持更丰富的营销诉求，CIA(3)通过ES方案实现了客户多种目标的优化出价，参见**普惠营销：搜索直通车的营销自动化实践与机制思考**。

* 宏观调控：广告主之间各自独立报价，这种非合作博弈的均衡状态往往无法实现整体效益的最大化，作为平台希望提升流量变现的上限。尝试推动广告主之间有限的协作，实现集体理性的报价。

* 两者关系：平台不是万能的，大量客户的私有信息无法获取，不可能实现流量资源有计划的高效分配。所以在Ocpc框架中，赋能竞争依然是核心，宏观调控只是辅助的手段。

为此我们提出了基于多智能体出价的营销优化算法，在出价中同时考虑市场竞争与宏观调控，融合广告主个体报价以及集体报价，以满足广告主营销目的的同时提升平台效率，进而促进市场三方长远健康发展。

# 问题分析

OCPC经历过几个版本，主要目的是帮助广告主实现合理的报价，赋能市场竞争。广告主只需要指定营销目标以及单位成本约束，即可实现高效的报价。随着使用OCPC的客户越来越多，之前的方案存在以下问题：

1. 前期使用OCPC的客户占比并不高，对于不同目标策略的训练做了简化，训练某个目标的策略时假设了其它人竞价策略不变。真实的竞价环境下，不同目标的竞价策略是会相互影响的，是一个典型的MAS问题。

2. OCPC能够显著提升客户的竞价效率，但都是基于客户个体自身的利益，容易抢占未使用OCPC客户的优质流量，从而造成零和博弈的局面。然而，平台并不是传统的DSP，我们既要满足客户的诉求，也要提升平台整体的效率。

3. 我们需要灵活的框架来支持客户越来越多的营销诉求，以及在此基础上满足不同时期的平台诉求。灵活体现在，一方面能够快速支持产品方增加不同目标的报价策略，另一方面也能支持平台在不同时间段大盘整体效率目标的切换上。

综上，如何灵活引入不同目标的策略，并且实现多种客户目标的协同和平台效率的优化，是当前直通车多目标出价产品的核心问题，该问题需要在一套完整的出价框架下解决，并考虑对后续业务的兼容性。

# 基于多智能体的出价优化算法

为了解决上述业务问题，我们提出了多智能体的报价框架，该问题存在三个挑战：

* 多智能体策略联合训练，会存在解空间较大，难以收敛；
* 灵活支持不同agent达成其各自不同的营销诉求；
* 如何协同多个agent，并实现对平台效率的改进。

参照多智能体**强化学习MADDPG**的建模方法[2]，将经典的单智能体MDP过程 $(S, S^{\prime}, A, R, P)$ 进行了拓展。在该框架下，相应的动作空间扩展为：$A_1 \times A_2 ... A_n$，状态-动作空间拓展为：$S\times A_1 \times A_2 ... A_n$。可以看出，相对于单智能体强化学习，多智能体建模带来的主要挑战是维度爆炸。为实现多智能体学习，显然单个智能体需要引入其他智能体的特征以及动作描述，从而便于预测下一个状态以及奖励。为实现客户目标与平台效率的同时优化，我们提出以下多智能体的训练架构：

![avatar](/images/计算广告/ad-36.png)

相比于单智能体问题，多智能体学习问题的关键点在于如何实现多智能体之间的通信。相比于MARL，创新点主要体现为：

1. 策略函数由两部分构成，私有网络的输出负责优化代理方的的效果，共享网络的输出则面向平台效率，各司其职。策略生成网络两部分物理意义明确，相当于通过引入先验信息大大减小了网络权重搜索的空间；

2. 完整的竞价上下文信息只输入给共享网络，用于宏观的调控，一定程度上确保了信息隔离，保证了对非代理广告主的公平性，让真正优质的商品脱颖而出，避免agent利用信息差过度损害非代理广告主和平台的利益；

3. 全局reward设计具有很强的灵活性，该接口可以建模多种需求，同时，可以利用MGDA算法)寻找满足帕累托改进的分簇reward权重，实现合理的全局奖励信号设计。

下面将从策略网络设计、奖励信号合成以及参数优化方法三个方面重点介绍。

##  策略网络设计

策略网络由两部分组成，即个体特异（agent-specific）的竞争出价网络以及面向整体效率优化的共享网络，前者以AD自身的状态作为输入，后者则以PV整体的状态作为输入。合并两个网络输出成为出价，定义参数化的策略空间如下：

$$ \text{bid} = \alpha(\mathbf{x}) * v_{ad} + \beta(\mathbf{x})*v_{pf}, $$

其中 $\mathbf{x} \in \mathbb{R}^d$为实时流量的特征，$\alpha(\mathbf{x})$ 和 $\beta(\mathbf{x})$ 合起来构成了参数化的策略空间，$v_{ad}$ 和 $v_{pf}$ 分别表示广告主在该流量上的价值认定以及平台效率在该流量上的价值体现。价值通常由期望表达式计算，即 $v = action\_value * action\_rate$例如，效果广告中通常广告主价值为广告主维度的流量即时 GMV，平台价值为流量总的 GMV。按照价值出价的逻辑，则 $\alpha(\mathbf{x})$ 和 $\beta(\mathbf{x})$ 分别表达了对于两部分价值，广告主愿意付出的出价，其物理意义即为广告的抽成率，分别记为 $\text{tk}_{ad}$ 和 $\text{tk}_{pf}$。该出价形式在给定不同的广告主营销诉求以及平台效率指标的前提下，显式地指定了策略空间。特别地，策略网络的设计主要考虑了以下几个关键点：

* 流量客户价值。广告主个体特异代理网络负责优化广告主价值，通过输出客户价值出价系数 $\alpha(\mathbf{x})$ 以优化 $v_{ad}$ 。例如，如果客户追求的是转化价值，则 $action\_value$ 和 $action\_rate$ 分别对应 $item\_price$ 和 $pcvr$，其他价值诸如点击、若转化价值可以类比实现。理想情况下，每个广告主针对自己的目标单独学习自己的出价策略是最优的，但这样会导致问题参数空间过大。假设有n个待出价AD，则总的策略参数空间为 $d^n$，指数化的参数空间将导致问题不可解。同时，大部分广告主的流量都很稀疏，其规模不足以支撑他们单独训练策略。因此，将AD分簇，实现簇内的策略参数共享成为一种折衷的方法。合理的分簇应该使得簇内的AD具有接近一致的表现。目前我们选择通过营销目标对AD进行分簇，后期可以根据效果进行更细的分簇。

* 流量公允价值。广告主特异的出价，根据广告主自身成本的不同，会产生不同的出价系数 $\alpha(\mathbf{x})$。由于广告主非理性出价存在，对平台效率而言，低价值的广告可能会因为较高的系数而抢占高价值广告的位置，从而导致流量存在错配，损害了平台的整体效率。而广告主们在某一类流量下充分竞争后，对于该流量一致认同的价值 我们称之为流量公允价值。 本文通过引入公允价值，来帮助广告主更合理的出价，进而提升平台效率。具体地，出价系数 $\beta(\mathbf{x})$ 描述广告主共享的流量特异出价系数，例如如果平台目标是创造更多的GMV，则 $v_{pf}$ 类比于 $v_{ad}$，目的是提升高转化商品的报价，进而提升平台GMV，其他种类的平台价值可以类比实现。仅仅依赖客户价值排序，带来的是ECPM最大化，但可能有损平台效率；仅仅依赖公允价值排序会带来对应平台效率最大化，有利于平台长期价值，但是有损ECPM（对应平台短期收益）且削弱了广告的价值。其中，$\text{tk}_{pf}$ 可以通过离线统计query粒度的平均tk或者PV粒度的平均tk计算得到。

* 联合训练。基准 $\text{tk}_{ad}$ 和 $\text{tk}_{pf}$ 构成了 $\alpha(\mathbf{x})$ 和 $\beta(\mathbf{x})$ 的基准成分，策略空间一是针对不同簇的目标结合流量特征优化 $\text{tk}_{ad}$ 的生成方式，二是优化两个 tk 的组合方式。通过两个网络联合优化，可以实现两个出价的组合，从而顾及短期ECPM的同时尽可能优化平台效率。此外，这一部分在出价中的额外意义是，调节平台整体的效率，达到间接释放非代理广告主效果的目的。真实使用中，广告主属于不同的分簇，执行不同的策略。我们对所有的参数进行联合训练，在考虑广告主所选配置的情况下，尽可能复现真实的决策环境，从而最大化确保训练策略效果的可用性。

## 奖励信号合成

从多目标出价业务实际出发，我们将所有智能体分为四个簇，分别为：

* 促进成交：在给定广告主ROI约束下，最大化GMV，即约束c_1最大化r_1。
* 促进收藏加购：给定wROI约束最大化收藏加购量，即约束c_2最大化r_2。
* 促进点击：给定PPC约束最大化点击量，即约束c_3最大化r_3。
* 自主竞价：根据客户自身的出价进行投放。

以上四个策略目的直接由 $\alpha(\mathbf{x})$ 负责实现，此外，我们通过调整 $\beta(\mathbf{x})$ 实现平台效率提升的目的。为实现这个目的，额外定义了全局效果（比如全局GMV）作为全局reward，并将客户单位成本作为软约束。最终，存在两类奖励函数，三个策略簇各自的奖励函数为：

$$ R_i = r_i - w_i \max\{c_i - c, 0\} + R, \: \forall i = 1,2,3, w_i > 0， $$

其中 $R$ 为全局的奖励函数，由平台设定，通常定义为全局的GMV之和。

## 参数优化方法

为进行算法的快速迭代，考虑到PV粒度竞价episode长度过长，每个step对于整体的贡献比较有限，并且PV之间往往相对独立、缺乏明显的状态转移（只有预算算是比较明显的耦合）；其次我们场景下，很多目标(如成交)对应的reward是非常稀疏且波动较大的，这就会导致传统的RL收敛会非常慢且策略泛化性能较差。

相对而言，ES算法具有以下优点[3]：

1. ES评估的是每个策略的结果，而非每个step的结果，能够有效避免因为reward稀疏而造成的影响（这个特性也能帮助算法迁移至在线学习）；  
2. ES对于reward 函数是否对策略参数可微分并不敏感，有助于多种策略目标的实现；  
3. 由于我们有replay模拟系统，且参数空间不大，所以ES收敛较慢的问题在我们的场景下并不严重  
4. ES天然适合参数并行分布式评估，方便工程实现。

因此我们采用ES策略实现参数的训练与更新。在离线训练中遵循以下流程：

```
根据当前最优参数配置，依照一定分布生成候选参数集；
采样竞价记录，通过replay并行评估参数集中的每一组参数；
根据每组参数的奖励值，评估最优的参数，作为下一轮迭代的起点；
重复以上过程直到参数变化小于终止阈值。
```

以上优化方法中存在两个关键点：一是采样评估的竞价记录条数，为保证评估可靠，通过实验确定合理的量；二是奖励信号的合成，通过将约束条件违背程度作为惩罚项加入奖励可以构建一个标量奖励，同时可以通过调整不同簇的奖励权重（参照[MGDA算法](https://hal.inria.fr/inria-00389811v2/document?spm=ata.13261165.0.0.76c853f2MplPzn)确定合理的权重），实现帕累托改进。


# 参考文献
[1] C. A. Wilkens, R. Cavallo, and R. Niazadeh. GSP - The Cinderalla of Mechanism Design. ACM WWW'17, <https://dl.acm.org/citation.cfm?id=3052687>.  
[2] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environmens. NIPS 2017.  
[3] T. Salimans, J. Ho, X. Chen, and I. Sutskever. 2017. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864.
