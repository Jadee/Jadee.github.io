---
title: 迁移学习在搜索多任务多场景的应用
date: 2019-02-10
categories:
- 机器学习
tags:
- 迁移学习
---

# 背景

搜索除了我们通常知道的流量最大的主搜之外，其实还有许多其他的流量不是那么大的场景，包括店铺搜、亲淘、PC、天猫等。这些小场景与主搜的商品和用户集合有很大部分重叠，但样本量却是主搜的几十乃至几百分之一。对主搜场景每天有上百亿的样本，学习一个百亿级别的大规模深度模型是没问题的，但对样本量只有几亿乃至几千万的场景，就有问题了。

<!-- more -->

因此，迁移学习派上了用场。下面我们将从两个方面介绍我们与迁移学习相关的工作：  
* Part1：考虑利用迁移学习同时提升源域和目标域的效果，即考虑用一个统一的模型，实现在提升目标域效果的同时，对源域的效果也不能有影响；  
* Part2：考虑instance based的迁移学习，从源域选出与目标域分布相似、同时对提升目标域的目标有帮助的样本，用来增加目标域的训练样本，以提升目标域的效果。

# Part1：利用迁移学习同时提升多个场景的效果

模型结构如下：

![avatar](/images/ml/ml-1.png)

## embedding layer

我们的模型主要的模型参数都集中在embedding部分，上层的网络部分参数其实只占了一小部分。因此，我们对embedding部分做了重点的分析与优化。每个特征的embedding结果 $e_j $包含两部分：  
1) 共享特征的embedding $e_c^j$  
2) domain-specific的embedding $e_0^j$ 或 $e_1^j$

为了降低目标域学习的复杂度，domain-specific部分的维度要远小于共享部分。在后面试验中我们可以看到，domain-specific部分对刻画场景特有的特性是非常有帮助的。最后将两部分embedding结果拼接起来作为最终的embedding $e_j$。

## attention layer

由于不同场景的用户的行为有不同的偏好，比如天猫的用户对商品的价格不是很敏感，而对商品的品牌更看重，但亲淘则刚好相反，用户更偏好便宜的商品。因此，我们对每个特征embedding结果设计了一个attention层：

$$ h_0=\sigma(W^0e_i^j+b^0), \\
h_1=\sigma(W^1h_0+b^1),  \\
a^j=\sigma(W^2h_1+b^2),  \\
g^j_i=a_i^j*e_i^j $$

最后，将attention之后的 $g_i^j$ 拼接起来，得到输出 $x_i^g$。

## Joint Training

模型的DNN部分，包含了共享网络与domain-specific网络，用来提取domain相似的特征与domain-specific的特征。每个domain的输出包含这两部分的输出之和：

$$ \begin{align}
\hat{y}^s = & \sigma ({W_c^{s}} O^{sc}+ {W^s} {O^s} + {b^s}), \notag\\
\hat{y}^t = & \sigma ({W_c^{t}} O^{tc}+ {W^t} {O^t} + {b^t}), \notag
\end{align} $$

其中 $W_c^s$ 与 $W_c^t$ 是相同的共享网络部分的模型参数，$W^s$ 与 $W^t$ 是源域与目标域的模型参数。与优化目标相关的Loss定义为：

$$ L_{d\in \{s,t\}} = - \frac{1}{n_{{d}}} \sum^{n_{{d}}}_{j=1} \frac{1}{2}[y_j^d \log \hat{y}_j^d + (1-y_j^d) \log (1-\hat{y}_j^d)] $$

同样，我们希望共享网络部分能只学习到与domain无关的相似的特征，而domain-specific部分只学习到本身domain相关的特征。如果我们不对共享网络与specific网络施加任何约束，共享部分可能混入domain-specific相关的特征，domain-specific网络也可能混入share相关的特征。我们采用Wasserstein distance来监督学习domain-invariant特征。$x_s$ 与 $x_t$ 分别表示共享网络来自于两个域特征的输入，对应的共享网络的输出分别为 $O^{sc}$ 与 $O^{st}$，$f_w$代表域鉴别器。它们之间的Wasserstein distance为 $D_c$：

$$ \begin{align}
W(\mathbb{P}_{O^{sc}},\mathbb{P}_{O^{tc}}) = \sup_{||f_w||_L \leq 1} \mathbb{E} [f_w(O^{sc})] - \mathbb{E} [f_w(O^{tc})].   \nonumber
\end{align} $$

$$ \begin{align}
D_c = &\frac{1}{n_s} \sum_{x^s} f_w(O^{sc}) - \frac{1}{n_t} \sum_{x^t} f_w(O^{tc}) \nonumber    \nonumber  \\
= & \frac{1}{n_s} \sum_{x^s} f_w(f_g^{sc} (x^s)) - \frac{1}{n_t} \sum_{x^t} f_w(f_g^{tc} (x^t)).     \nonumber
\end{align} $$

我们希望共享网络部分输出的特征之间的Wasserstein distance为 $D_c$ 越来越小。另一方面，为了使得domain-specific网络只抽取与本身domain相关的特征，令 $D_s，D_t$ 分别代表源域与目标域共享网络输出特征与domain-specific网络输出的特征之间的Wasserstein distance，则我们需要 $D_s$ 与 $D_t$越来越大。

$$ \begin{align}
D_s = & \frac{1}{n_s} \sum_{x^s} f_w(f_g^{sc} (x^s)) - \frac{1}{n_s} \sum_{x^s} f_w(f_g^{c} (x^s)), \nonumber \\
D_t = & \frac{1}{n_t} \sum_{x^t} f_w(f_g^{tc} (x^t)) - \frac{1}{n_t} \sum_{x^t} f_w(f_g^{c} (x^t)).   \nonumber
\end{align} $$

最终联合优化的目标函数为：

$$ \begin{align}
J = \min_{\theta_g,\theta_c} \lbrace L_s + L_t + \lambda \max_{\theta_w} (D_c - D_s - D_t) + L_{reg}\rbrace.   \nonumber
\end{align} $$

整个训练过程是一个极小极大值优化过程。首先固定鉴别器参数 $\theta_w$，通过调整参数 $\theta_g$ 以最小化 $D_c - D_s - D_t$，即最小化不同域共享网络的特征输出，最大化源域的共享特征与specific特征输出和最大化目标域的共享特征与specific特征的输出。固定 $\theta_g$ 
, 则通过调整鉴别器参数 $\theta_w$ 以最大化 $D_c - D_s - D_t$。整个过程与Gan的训练过程类似。

## 实验

我们将我们的模型运用在了(主搜->天猫)与(主搜->亲淘)场景，主搜数据丰富，包含了各类消费人群，天猫主要偏向于更加注重商品品牌与品质的人群，而亲淘则更偏向于低端的消费人群。实验结果如下：

![avatar](/images/ml/ml-2.png)

从实验结果看出，我们的share-specific与联合训练模式，对主搜网络的auc基本没有负向影响，同时对目标域的auc也有显著的提升；同时我们重点考查了不增加对抗loss对模型的影响，可以看出，增加对抗loss之后，有进一步的提升。
另一方面，我们也考查了联合训练中，各个场景不同特征的attention权重:

![avatar](/images/ml/ml-3.png)

中platform A代表主搜场景， platform B代表天猫，platform C代表亲淘。可以看出，主搜在各个维度的特征的attention权重大小更加的平均，而天猫则更加偏向于商品品牌，亲淘则对商品价格与商品销量等统计类特征比较关注。

# Part-2：A Minimax Game for Instance based Selective Transfer Learning

考虑到大场景主搜每天的样本量约在接近百亿级别，小场景亲淘每天的样本量约在接近千万级别，两者数量相差悬殊。此外，由于主搜的样本量充分，所以主搜的模型能够训练到较高的水平，亲淘场景中模型训练比较差。通过分析样本数据我们发现，主搜场景中的人群人均消费水平要高于亲淘场景中人均消费水平，二者的样本数据分布有一定的差异，但主搜中也有低端的消费人群。受迁移学习和对抗学习的启发，我们尝试从主搜场景中选出和亲淘场景中相似度比较高的样本帮助亲淘模型进行训练。不同的地方是，我们考虑的不仅是样本分布上的相似，而且是希望找到在与目标域目标相关的特征分布上相似。如下图：

![avatar](/images/ml/ml-4.png)

样本1，2代表样本分布上不相似的异常点，而3，4代表虽然样本分布相似，但与我们要学习的目标域的目标完全相反，或差异很大。则我们希望通过我们的方法能将这两类样本都踢除出去。
我们的网络结构如下：

![avatar](/images/ml/ml-5.png)

整体优化目标为：

![avatar](/images/ml/ml-6.png)

其中 $p_{\theta}(d)$ 代表selection model, $p_{tgt}(d)$ 表目标域样本分布。$\mathcal{D}(d)$ 代表样本 $d$ 属于目标域的概率。reward项是基于TL model M在validate set $\mathcal{V}$ 得到。最后一项 $J_2$代表TL model的loss。

## discriminative model

鉴别器 \mathcal{D} 主要区分选出来的来自于源域的样本与目标域的样本。它是一个二分类问题，0代表来样本来自于源域，1代表样本来自于目标域。优化的目标函数为：

$$ \begin{align}
\mathcal{D}(d) = \sigma (f_\phi (d)) = \frac{\exp(f_\phi (d))}{1+\exp(f_\phi (d))}.  \nonumber
\end{align} $$

$$ \begin{align}
\phi^{*}=argmax_{\phi} [&\mathbb{E}_{d_{i}\sim p_{tgt}(d)}\log \mathcal{D}(d_{i}) \quad + \nonumber\\
& \mathbb{E}_{d_{i}\thicksim p_{\theta}(d)} \log(1-\mathcal{D}(d_{i}))].   \nonumber
\end{align} $$

## selection model

selection model $p_{\theta}(d)$ 主要有两个目标：  
1、选出与 $p_{tgt}(d)$ 相似的样本；  
2、提升TL model的表现。对目标1，通过选择出discriminative model区分不出来的样本即可，对目标2则同时需要我们选出来的样本使得TLmodel在validate数据集上的loss越来越小：

![avatar](/images/ml/ml-7.png)

因此，我们构造了immediate reward 和delay reward来分别优化这两个目标。

1.**immediate reward**

![avatar](/images/ml/ml-8.png)

如上图，immediate reward主要来自于discriminate model, 通过selection model选出来的样本，输入到discriminate model, discriminate model对每个样本均会给出一个reward $r_i$, 其更新公式如下：

![avatar](/images/ml/ml-9.png)

与policy gradient类似，$log(1 + exp(f_{\phi}(d_i)))$ 代表selection model $p_{\theta}(d)$ 选择样本 $d_i$之后在discriminative model获得的immediate reward $r_i$。

2.**delay reward**

如下图：

![avatar](/images/ml/ml-10.png)

selection model通过 $p_{\theta}(d) \gt \tau$ 选出来的样本更新TL model之后，我们计算更新之前的模型 $L'(y_i，\mathcal{M}(d_i))$ 
与更新之后的模型 $L'(y_i，\mathcal{M}_{p \theta}(d_i))$ 在验证集 $\mathcal{V}$ 上delay reward:

$$ r_b=\sum_{i=1}^\mathcal{V}[L'(y_i, \mathcal{M}(d_i))-L'(y_i, \mathcal{M}_{p_\theta}(d_i)))] $$

与传统的强化学习不同，我们的行为是sample出一个batch的样本，delay reward是在这个batch上得到的。他表示了sample出来的样本是否对TL model有帮助。在模型中，我们采用logloss的变化作为delay reward。对每一个episode的每一个batch, delay reward的计算如下：

$$ \begin{align}
r'_b = r_b + \gamma * r_{b+1} + \gamma^2 * r_{b+2} + \cdots + \gamma^{n-b} * r_{n}.
\end{align} $$

最后，selection model 的更新公式为：

![avatar](/images/ml/ml-11.png)

## TL model

TL model可以为任意的DNN模型，输入为采样的样本与目标域的样本：

$$ \varphi^{*} = \varphi - \beta \frac{1}{N'}
\sum_{i=1}^{N'} \triangledown_{\varphi} L(y_i, \mathcal{M}_{p_\theta}(d_i)). $$

其中 $y_i$ 为label, $\mathcal{M}_{p_\theta}(d_i))$ 为TL model。我们可以通过SGD等方法优化。

## 实验

我们在主搜场景与亲淘场景进行了实验，并与普通的fully-share transfer learning[2,3]以及近期提出的一个利用Bayesian optimization的采样方法Ruder and Plank[1]进行了对比，结果如下：

![avatar](/images/ml/ml-12.png)

从实验结果可以看出，我们的方法在对目标域的目标提升是显著的。另一方面，我们将训练过程discriminative model输出的结果利用t-sne进行可视化输出，结果如下：

![avatar](/images/ml/ml-13.png)

可以看出，随着迭代的进行，discriminative model确实越来越分不开两个来源的数据，也就是说selection model采样出来的数据，与目标域的分布越来越相似。  
同时，在训练结束之后，我们调采样阈值 $\tau$，并同样将discriminative model输出的结果利用t-sne进行可视化输出:

![avatar](/images/ml/ml-14.png)

很明显，当 $\tau$ 选择 $[0，0.1)$ 时，采样出来的样本是很不相似的，而随着阈值的提升，采样出来的样本与目标域越来越相似。

# 总结

迁移学习在搜索的各个场景下有广阔的应用场景，在各个小场景都独自学习部署一个大规模的深度模型是不现实的，小场景的样本量也不足以学习好一个大规模的深度模型。而上面两部分工作只是我们将transfer learning用于实际场景中的一次尝试，如何进一步的提升transfer learning的运用效果，是继续研究的方向。

[1] Sebastian Ruder and Barbara Plank. 2017. Learning to Select Data for Transfer Learning with Bayesian Optimization. In EMNLP.  
[2] Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. 2016. Natural Language Inference by Tree-Based Convolution and Heuristic Matching. In ACL.  
[3] Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2016. How Transferable are Neural Networks in NLP Applications?. In EMNLP.

