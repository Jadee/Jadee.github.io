---
title: Embedding那些事：from word2vec to item2vec
date: 2019-03-05
categories:
- 机器学习
tags:
- DNN
---

# 前言

embedding向量大家可能都用过，但对于NLP领域涉足不深的个人来说，对embedding的认知和理解并不是很深刻。为了进一步理解并且能更好地运用embedding技术，便做了相关的调研，尝试将其汇总总结，理清一条完整的思路，并写下此文，希望对大家理解embedding有所帮助。

<!-- more -->

本文主要介绍了NLP领域embedding的发展，word2vec的经典模型，以及embedding技术推广到其他（推荐／搜索）领域的运用。实践证明，embedding技术对于工业场景来说有着很大的价值和应用前景。

# Word Embedding

自然语言作为一种非结构化的数据，很难被机器处理或学习。自然语言要被机器理解，第一步就需要将自然语言符号化表示。词的向量化表示作为一种很有效的方法，可以定量地度量词之间的关系，挖掘词之间的联系。那么，向量为什么能表示词呢？词向量如何生成呢？

## one-hot representation

one-hot编码是最简单也是最容易理解的一种表示方式，类似于索引的方式。向量的维度为词库（词汇表）的大小，具体的词映射到索引位置，置为1，其余位置为0，生成的向量用来唯一表示词库中的某一个词。

举个例子：  
King、Queen、Man、Woman、Child 组成的词库V，|V|=5，则one-hot编码后的各个词的词向量为：
```
king  = (1,0,0,0,0)
queen = (0,1,0,0,0)
man   = (0,0,1,0,0)
woman = (0,0,0,1,0)
child = (0,0,0,0,1)
```
显然，这种表示方式会带来一些问题：

1）词汇表庞大带来的维度灾难  
2）数据过度稀疏，表达效率低

那么，能不能将词向量映射到较低维度的特征空间呢？

设想一下，有"Royalty"(王权),"Masculinity"(男子气?), "Femininity"(女人味?)和"Age"(年龄)4个维度特征，我们试着将词汇表V里的词用这4个维度来表示：
```
King  = (0.99, 0.99, 0.05, 0.7)
Queen = (0.99, 0.05, 0.93, 0.6)
man   = (0.05,0.95,0.1,0.6)
woman = (0.05,0.1,0.93,0.6)
child = (0.01,0.2,0.4,0.3,0.2)
```

## Dristributed representation

Distributed representation方法可以解决one-hot的问题，通过训练，将词映射成较低维向量。而大部分情况下词向量的各个维度并不能很好地解释，可以理解为隐含特征。有了词向量，我们便可以很好描述词，挖掘词之间的关系了。有一个有趣的研究表明：

$\rightarrow{King} - \rightarrow{Man} + \rightarrow{Woman} = \rightarrow{Queen}$

从直观含义上，不难理解。

那么，如何训练得到word embedding向量呢？  
一种很常见的方法，使用神经网络模型。常用的两种模型结构介绍如下。
