---
title: RNN网络学习总结
date: 2018-11-03
categories:
- 机器学习
tags:
- RNN
---

# Network

# RNN(Recurrent Neural Network)

# GRU(Gated Recurrent Unit Recurrent Neural Networks)

* 序列中不同的位置处的单词对当前的隐藏层的状态的影响不同，越前面的影响越小，即每个前面状态对当前的影响进行了距离加权，距离越远，权值越小。

* 在产生误差error时，误差可能是由某一个或者几个单词而引发的，所以应当仅仅对对应的单词weight进行更新

GRUs首先根据当前输入单词向量word vector以及前一个隐藏层的状态hidden state计算出update gate和reset gate。再根据reset gate、当前word vector以及前一个hidden state计算新的记忆单元内容(new memory content)。当reset gate为1的时候，new memory content忽略之前的所有memory content，最终的memory是之前的hidden state与new memory content的结合

# LSTM(Long Short Term Memory networks)

## LSTM基本介绍

解决RNN长时期依赖问题，本质是能够记住很长时期内的信息。

所有循环神经网络结构都是由完全相同结构的模型进行复制而成的，在普通的RNNs 中，这个模块结构非常简单，比如仅是一个单一的 tanh 层。

## LSTM核心思想

LSTMs 最关键的地方在于 cell（整个绿色的框就是一个 cell） 的状态和结构图上面的那条横穿的水平线。cell 状态的传输就像一条传送带，向量从整个 cell 中穿过，只是做了少量的线性操作。这种结构能够很轻松地实现信息从整个 cell 中穿过而不做改变(实现长时期的信息保留)。若只有上面的那条水平线是没办法实现添加或者删除信息的。而是通过一种叫做 门（gates） 的结构来实现的。gates 可以实现选择性地让信息通过，主要是通过一个 sigmoid 的神经层和一个逐点相乘的操作来实现的。

# 参考
1. <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>  
2. [Tensorflow RNN](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py?spm=ata.13261165.0.0.10ba36ecuMXSE7&file=rnn_cell.py)  
