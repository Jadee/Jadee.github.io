---
title: RNN网络学习总结
date: 2018-11-03
categories:
- 机器学习
tags:
- RNN
---

# Network

# RNN(Recurrent Neural Network)

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)

RNNs 的出现，主要是因为它们能够把以前的信息联系到现在，从而解决现在的问题。但是随着预测信息和相关信息间的间隔增大， RNNs 很难去把它们关联起来了。

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png)

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png)

把上面有W的那个带箭头的圈去掉，它就变成了最普通的**全连接神经网络**。$x$ 是一个向量，它表示**输入层**的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示**隐藏层**的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）。

循环神经网络的**隐藏层**的值 $s$ 不仅仅取决于当前这次的输入 $x$，还取决于上一次**隐藏层**的值 $s$。**权重矩阵** $W$ 就是隐藏层上一次的值作为这一次的输入的权重

# GRU(Gated Recurrent Unit Recurrent Neural Networks)

* 序列中不同的位置处的单词对当前的隐藏层的状态的影响不同，越前面的影响越小，即每个前面状态对当前的影响进行了距离加权，距离越远，权值越小。

* 在产生误差error时，误差可能是由某一个或者几个单词而引发的，所以应当仅仅对对应的单词weight进行更新

GRUs首先根据当前输入单词向量word vector以及前一个隐藏层的状态hidden state计算出update gate和reset gate。再根据reset gate、当前word vector以及前一个hidden state计算新的记忆单元内容(new memory content)。当reset gate为1的时候，new memory content忽略之前的所有memory content，最终的memory是之前的hidden state与new memory content的结合

# LSTM(Long Short Term Memory networks)

## LSTM基本介绍

解决RNN长时期依赖问题，本质是能够记住很长时期内的信息。

所有循环神经网络结构都是由完全相同结构的模型进行复制而成的，在普通的RNNs 中，这个模块结构非常简单，比如仅是一个单一的 tanh 层。

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)

LSTMs 也有类似的结构。但是它们不再只是用一个单一的 tanh 层，而是用了四个相互作用的层。

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)

## LSTM核心思想

LSTMs 最关键的地方在于 cell（整个绿色的框就是一个 cell） 的状态和结构图上面的那条横穿的水平线。cell 状态的传输就像一条传送带，向量从整个 cell 中穿过，只是做了少量的线性操作。这种结构能够很轻松地实现信息从整个 cell 中穿过而不做改变(实现长时期的信息保留)。若只有上面的那条水平线是没办法实现添加或者删除信息的。而是通过一种叫做 门（gates） 的结构来实现的。gates 可以实现选择性地让信息通过，主要是通过一个 sigmoid 的神经层和一个逐点相乘的操作来实现的。

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png)

![avatar](https://img-blog.csdn.net/20180719131152449?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoaW5hZ3JlZW53YWxs/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

LSTM有三种这样的gates，来控制及保护cell的状态。

### 遗忘门

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)

### 传入门

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)

### 输出门

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)

## 改进

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png)

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png)

# GRU

![avatar](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)

LSTMs与GRUs类似，目前非常流行。它与一般的RNNs结构本质上并没有什么不同，只是使用了不同的函数去计算隐藏层的状态。已经证明，该网络结构在对长序列依赖问题中非常有效。


与GRU不同之处

* new memory的计算方法都是根据之前的state及input进行计算，但是GRUs中有一个reset gate控制之前state的进入量，而在LSTMs里没有这个gate；

* 产生新的state的方式不同，LSTMs有两个不同的gate，分别是forget gate (f gate)和input gate(i gate)，而GRUs只有一个update gate(z gate)；

* LSTMs对新产生的state又一个output gate(o gate)可以调节大小，而GRUs直接输出无任何调节。

# Tensorflow 代码分析

## static_rnn
```python
state = cell.zero_state(...)
outputs = []
for input_ in inputs:
    output, state = cell(input_, state)
    outputs.append(output)
return (outputs, state)
```

## GRU Cell
```
def call(self, inputs, state):
    """Gated recurrent unit (GRU) with nunits cells."""
    _check_rnn_cell_input_dtypes([inputs, state])

    gate_inputs = math_ops.matmul(
        array_ops.concat([inputs, state], 1), self._gate_kernel)
    gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias)

    value = math_ops.sigmoid(gate_inputs)
    r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)

    r_state = r * state

    candidate = math_ops.matmul(
        array_ops.concat([inputs, r_state], 1), self._candidate_kernel)
    candidate = nn_ops.bias_add(candidate, self._candidate_bias)

    c = self._activation(candidate)
    new_h = u * state + (1 - u) * c
    return new_h, new_h
```

# 用途

语言模型与文本生成(Language Modeling and Generating Text)、机器翻译(Machine Translation)、语音识别(Speech Recognition)、图像描述生成 (Generating Image Descriptions)

# 参考
1. <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>  
2. [Tensorflow RNN](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py?spm=ata.13261165.0.0.10ba36ecuMXSE7&file=rnn_cell.py)  
