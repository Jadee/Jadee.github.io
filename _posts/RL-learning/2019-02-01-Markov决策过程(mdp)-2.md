---
title: Markov决策过程(mdp)-1
date: 2019-02-01
categories: 强化学习
tags:
- DNN
- 强化学习
- github
---

在基于模型的强化学习中，我们有两种方法可以对策略进行求解，那就是1）策略迭代；2）值迭代

# 举例

https://ieor8100.github.io/rl/docs/Lecture%201%20-MDP.pdf

状态空间：{跌倒， 站立， 移动}<br>
S = {′F′,′ S′,′ M′}

动作空间：{快速， 慢速}<br>
A = {′slow′,′ fast′}

![avatar](/images/RL-learning/mdp-9.png)

## 黑色 fast ：快速<br>
0.6 概率离开保持状态， reward =-1<br>
0.4 概率离开跌倒状态， 进入站立状态， reward =1<br>

离开Fallen的立即收获：<br>
0.6 * -1 + 0.4 * 1 = -0.2

离开Stading 的立即收获：<br>
概率1 * 立即收获1 = 1

离开Moving 的立即收获：<br>
概率1 * 立即收获1 = 1

## 绿色 slow: 安全
Fallen, 不采用slow 动作离开， 收获reward=0

Standing, 离开的立即收获：<br>
0.6 * 2 + 0.4 * -1 = 0.2

Moving, 离开的立即收获：<br>
0.8 * 2, 保持的立即收获 (离开+回来)<br>
0.2 * -1, 离开Moving进入Fallen状态

0.8 * 2 + 0.2 * -1 = 1.4

![avatar](/images/RL-learning/mdp-10.png)

绿色 slow:<br>
P(s,slow,s′): 状态转移概率矩阵

![avatar](/images/RL-learning/mdp-11.png)

f->f 0.6 的转移概率<br>
f->s 0.4 的转移概率

![avatar](/images/RL-learning/mdp-12.png)

s->m 1.0 的转移概率<br>
m->m 1.0 的转移概率

## 值函数
绿色 flow 

v(f) = 0 fast action<br>
fast action / do nothing

v(f) = 1 slow action<br>
0.6 * 2 + 0.4 * -1 = 0.8

只考虑绿色slow动作的立即回报<br>
V(m) = 1.4 = 0.8 * 2 + 0.2 * -1

## 立即reward 计算

![avatar](/images/RL-learning/mdp-13.png)

### 第一迭代，值函数1
初始的值函数，用立即最大回报来计算<br>
2个动作里面，挑选最大的立即回报

![avatar](/images/RL-learning/mdp-14.png)

| 最大立即汇报 | 黑色fast | 绿色slow |
| :-----| ----: | :----: |
| fall | -0.2 | 0 |
| standing | 1 | 0.8 |
| moving | 1 | 1.4 |

v(f, s, m) = (0, 1, 1.4)

### 第二迭代，值函数2
初始值函数， 用立即最大回报来计算<br>
v(f, s, m) = (0, 1, 1.4)

![avatar](/images/RL-learning/mdp-15.png)

## 迭代更新

![avatar](/images/RL-learning/mdp-16.png)

![avatar](/images/RL-learning/mdp-17.png)

### 常量解读
全局常量 s: 状态state不变<br>
全局常量 p: 状态之间的转移概率p不变

策略常量 r: 离开状态s之后的立即回报 reward 不变<br>
不同的策略之间对比，离开s之后的立即回报：r(s)值是不一样<br>
同一个策略之内，离开s之后的立即回报：r(s)值是一样的<br>

### 变量v的更新
v 是变量，每轮迭代都不一样<br>
v 状态值函数，每轮迭代，会更新为最好的策略产生的最大的累积回报

对所有的状态迭代：
分别按照所有的策略，对当前状态计算：累积回报<br>
比较该状态的不同策略返回的累积回报，选用最大的累积回报，去更新v

该状态的值函数，可能是不同策略交织产生的，每轮迭代都选取按最大的回报值

### Infinite无限个状态：v价值函数解读

![avatar](/images/RL-learning/mdp-18.png)

#### 动作：a ~ π(a|s) 
策略：π(a|s) = P[Action = a|State = s]<br>
动作：a ~ π(a|s) <br>
action 符合在状态s时的根据策略π采用a的分布<br>
条件概率：从s这个状态出发<br>
横坐标是：a<br>
纵坐标是：a的概率分布π

#### s' ~ P(s, a)
在状态s, 执行动作a, 转移到状态s'的概率是P

#### R(s, a, s')
在状态s, 执行动作a, 转移到状态s', 在s执行a后，获得的立即回报R

举例：
高三是一个状态s，参加了高考a，立即获得R(分数)

取得R分数，报考院校s' (清华，北京，浙大...)<br>
被录取的状态转移概率P分别是 (0.1, 0.1, 0.8)

解读： V(s) = 立即回报R(s) + r折扣 * P转移概率 * V新状态价值
V(清华)<br>
V(北大)<br>
V(浙大)的价值 = 历史积淀，包括: 品牌+校友+老师

立即回报R(s) = 毕业证(状态s=大学)，cost 浪费了4年<br>
新状态1：硕博连读，转移概率P1 = 0.7, 价值(博士学位), cost 浪费了2 + 4年<br>
新状态2：进入名企，转移概率P2 = 0.3, 价值(工资，奖金，股票，3.25)

#### 总结value函数

* 向前看，不回头，英雄莫问出处<br>
本状态s的价值v，只和后续状态相关，用后续状态s'的价值v', 来计算当前状态s的价值V, 和以前的老状态无关

* 状态的连续爬升，计算当前状态的价值v, 需要连续折现r<br>
V(s) = E[r1 + γr2 + γ2 r3 + γ3r4 + . . . ]

#### 优化策略
每个策略都会产生新的v， 取最大值来更新v

v(s, a1) 状态采用a1的价值：<br>
高中s，选文科a1，一辈子挣钱的总额v是500万

v(s, a2) 状态采用a1的价值：<br>
高中s，选理科a2，一辈子挣钱的总额v是1000万

v(s) = max(v(s, a1), v(s, a2)) <br>
在高中s的价值， 向前看整个人生，能取最大回报值1000万

不同的高中，转移到大学的概率是不同的，所以每个高中有自己的价值。

不同的大学，转移到人生赢家的概率是不同的，所以每个大学有自己的价值。

#### 看懂方程，看懂人生v = max(R + PV)
max是对(R + PV)来取最大值<br>
v = max(R + PV) = R + max(PV)<br>
&nbsp;&nbsp; = 立即回报R + 长期回报的最大值

![avatar](/images/RL-learning/mdp-19.png)


