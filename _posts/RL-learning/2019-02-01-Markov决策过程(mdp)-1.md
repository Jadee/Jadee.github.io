---
title: Markov决策过程(mdp)-1
date: 2019-02-01
categories: 强化学习
tags:
- DNN
- 强化学习
- github
---

# Markov 决策过程(mdp) 解读
MDP 的ppt<br>
http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf

MDP 的 英文教程<br>
https://ieor8100.github.io/rl/docs/Lecture%201%20-MDP.pdf

q-learning<br>
https://www.zhihu.com/question/26408259

<!-- more -->

英文教程 q-learing<br>
http://mnemstudio.org/path-finding-q-learning-tutorial.htm

q-learing 代码实现<br>
https://github.com/JasonQSY/ML-Weekly/blob/master/P5-Reinforcement-Learning/Q-learning/Q-Learning-Get-Started.ipynb

# Markov 属性， 简化运算
Once the state is known, the history may be thrown away<br>
向前看， 莫回头

# 无记忆的随机过程
A Markov process is a memoryless random process

# 状态转移概率矩阵
P is a state transition probability matrix,

Pss′ = P[St+1=s′|St=s]

![avatar](/images/RL-learning/mdp-1.png)

# 奖励 Reward 立即回报
举例：
大学state, 毕业action，立即回报Reward是：<br>
一张毕业证(状态s=大学)+ 生命里耗费4年青春

硕士state, 毕业action，立即回报Reward是：<br>
一张毕业证+ 生命里耗费3年青春-房价翻几倍了

A Markov Reward Process is a tuple ⟨S, P, R, γ⟩

s: 有限状态
p: 状态转移概率矩阵
无记忆

R: 奖励函数
Rs = E[Rt+1 | St = s]

γ： 折扣系数
对远期状态s'

# 回报 Return 长期待遇
Gain = Rt_1+ γRt_2 + γγRt_3

折扣系数 γ [0-1], 代表未来的奖励的折现

立即奖励 + 远期奖励的折现<br>
immediate reward above delayed reward

# V(s) 值函数：
状态（职级) 对应的平均回报(待遇)<br>
v(s) = E[Gt |St =s]

![avatar](/images/RL-learning/mdp-2.png)

折扣r = 0.5

路径 c1 c2 c3 Pass Sleep <br>
立即回报R -2 -2 -2 10 0 <br>
折扣r 1 1/2 1/4 1/8

v1 =−2 -2x1/2 -2x1/4 + 10x1/8

# 折扣系数=0.9的值函数

  ![avatar](/images/RL-learning/mdp-3.png)

# Bellman Equation 方程
值函数（待遇) = 立即奖励R(奖金) + 折现回报(待遇的值，比如长期的股票，期权)

v(s)<br>
=E[Gt |St =s]<br>
=E[Rt+1+γRt+2+γ2Rt+3+...|St =s]<br>
=E[Rt+1+γ(Rt+2+γRt+3+...) |St =s]<br>
=E[Rt+1+γGt+1 |St =s]<br>
=E[Rt+1+γv(St+1)|St =s]<br>

  ![avatar](/images/RL-learning/mdp-4.png)

# 红圈的Value 解读
4.3 = -2 + 0.6x10 + 0.4x0.8<br>
Value=<br>
  * 过本关之后的立即回报 R=-2<br> 
  * 0.6的概率升级pass的 待遇价值10 = 0.6x10<br>
  * 0.4的概率达到pub 的 待遇价值 0.8 = 0.4x0.8<br>

# Bellman Equation的矩阵形式
v = R + γPv <br> 
价值v = 立即回报R + 折现系数γ x 转移概率矩阵P x 价值v <br>

  ![avatar](/images/RL-learning/mdp-5.png)

[P11, P12, ....P1n] : <br>
向量形式的转移概率    : 如果为0，则两个状态之间不可能发生转移

# 求解 Bellman 方程
线性方程组<br>
已知 v = R + γPv<br>
所以 (I − γP) v = R − 1<br>
解 ：v = (I−γP) R

# Markov decision process (MDP)
Markov Decision Process is a tuple ⟨S, A, P, R, γ⟩<br>
S： 有限状态集<br>
A： 有限动作集<br>
P： 状态转移矩阵<br>
无记忆， 马尔可夫性质<br>
R： 平均回报函数， 在状态s的时候， 采用动作a<br>
γ: 折现系数

# 策略 π 的概率分布
策略 policy 的po的读音， 类似于π<br>
π(a|s)=P[At =a|St =s]<br>
在状态s的时候， 采用动作a的概率π<br>
仅对当前状态决策， 来选出最佳动作<br>

选出的动作， 只需要符合策略的概率分布， 和时间无关

## 状态的价值函数 vπ(s)
按照策略π，在状态s时，获得的长短期收益的均值（岗位待遇)

## 动作的价值函数Q: qπ(s,a)
qπ(s,a)=expected_π[Gt |St =s,At =a])<br>
按照策略π，在状态s时, 执行动作a, 获得的长短期收益的均值<br>
(动作价值)

(s1->a1) -> (s2->a2)->s(状态的价值v)-> 根据策略π选择动作-> a

状态价值（回报／待遇)：<br>
v(s) = 平均 ( 立即回报 + 折现待遇v(s+1))<br>
记为：<br>
vπ(s) = Eπ [Rt+1 + γvπ(St+1) | St = s]<br>

动作价值（回报／待遇)：<br>
q(s,a) = 平均 ( 立即回报+ 折现动作回报)<br>
记为：<br>
qπ(s,a)=Eπ[Rt+1+γqπ(St+1,At+1)|St =s,At =a]

状态有价值函数v(s), 动作有价值函数q(s,a)<br>
策略 π(a|s)： 在状态s的时候， 采用动作a的概率<br>
价值 Qπ(s,a)：在状态s的时候， 采用动作a的价值<br>

![avatar](/images/RL-learning/mdp-6.png)

在状态s时:
* 选择好的动作的概率越大，s状态的价值越大<br>
所以要优化策略 : π(a|s)

# 动作价值

![avatar](/images/RL-learning/mdp-7.png)

价值 Qπ(s,a)：在状态s的时候， 采用动作a的价值<br>
转移到新的状态s‘， 获得立即回报r<br>
新的状态s‘， 有价值v(s‘)<br>

从s0, 采用动作a, 转移到s‘, 转移概率矩阵Pa_ss‘<br>

远期价值：<br>
遍历所有的新状态空间S, 对 s‘的价值折现γ求和<br>

近期价值：<br>
R_as: 在状态s，采取行动a, 立即获得的回报 r

当折现=1时的值函数<br>
10-2=8<br>
8-2=6<br>
6-0=6<br>

![avatar](/images/RL-learning/mdp-8.png)

s-> 策略(内因) -> a -> q(s,a)价值->转移概率矩阵(外因)->新的s<br>
s->a到a点之后， 就有价值q(s,a)<br>
高中状态 -> 策略(动作：努力学习理科/努力学习文科)<br>
-> 价值q(s,a)，比如是高考成绩，<br>
有了高考成绩q(s,a), 进入哪所大学， 是由转移概率P_ss'来决定的<br>

用后期的累积回报， 来跨越暂时的低谷, 避免短视的贪婪<br>
选择路径时，v值，肯定会越来越大

