---
title: Markov决策过程(mdp)-3
date: 2019-02-01
categories: 强化学习
tags:
- DNN
- 强化学习
- github
---

# Q-value
Q为动作效用函数（action-utility function），用于评价在特定状态下采取某个动作的优劣，可以将之理解为智能体（Agent，我们聪明的小鸟）的大脑。我们可以把Q当做是一张表。表中的每一行是一个状态，每一列（这个问题中共有两列）表示一个动作（飞与不飞）

https://www.zhihu.com/question/26408259/answer/123230350

| 状态 | 飞 | 不飞 |
| :-----| ----: | :----: |
| dx1,dy1 | 1 | 3 |
| dx1,dy2 | 3 | 4 |
| dxn,dyn | -100 | 1 |

这张表一共 m x n 行，表示 m x n 个状态，每个状态所对应的动作都有一个效用值q

训练之后的小鸟在某个位置处飞与不飞的决策就是通过这张表确定的。小鸟会先去根据当前所在位置查找到对应的行，然后再比较两列的值（飞与不飞）的大小，选择值较大的动作作为当前帧(状态)的动作

```
Initialize Q arbitrarily //随机初始化Q值
Repeat (for each episode): //每一次游戏，从小鸟出生到死亡是一个episode
    Initialize S //小鸟刚开始飞，S为初始位置的状态
    Repeat (for each step of episode):
        根据当前Q和位置S，使用一种策略，得到动作A //这个策略可以是ε-greedy等
        做了动作A，小鸟到达新的位置S'，并获得奖励R //奖励可以是1，50或者-1000
        Q(S,A) ← (1-α)*Q(S,A) + α*[R + γ*maxQ(S',a)] //在Q中更新S
        S ← S'
    until S is terminal //即到小鸟死亡为止
```

# q-learn 如何在探索与经验之间平衡？
ε-greedy方法，即每个状态有ε的概率进行探索（即随机选取飞或不飞），而剩下的1-ε的概率则进行开发（选取当前状态下效用值较大的那个动作）

Q-learning并非每次迭代都沿当前Q值最高的路径前进

# Q-learning的训练公式
Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]

http://mnemstudio.org/path-finding-q-learning-tutorial.htm

算法从历史经验中迭代学习Q(s,a)的价值
The algorithm above is used by the agent to learn from experience.

智能体agent的大脑， 就是迭代训练之后的得到的Q 矩阵
The purpose of the training is to enhance the 'brain' of our agent, represented by matrix Q

每次迭代， 需要坚持跑到终点
Do While the goal state hasn't been reached.

用贪婪greedy+随机探索的方式，选择一个动作

```
if np.random.random() < epsilon:
    # choose random action 随机探索
    action = possible_actions[np.random.randint(0, len(possible_actions))]
else:
    # greedy 沿用历史经验
    action = possible_actions[np.argmax(possible_q)]
```

跑到下一个新状态，

纪录Q(next_state, all_actions)<br>
用下一个新状态的Q值， 来更新这次的状态的Q

Get maximum Q value for this next state based on all possible actions.

α为学习速率（learning rate）<br>
γ为折扣因子（discount factor）<br>
根据公式可以看出，学习速率α越大，保留之前训练的效果就越少。<br>
折扣因子γ越大，max_aQ(S', a)所起到的作用就越大。

小鸟在对状态进行更新时，会考虑到眼前利益（R），和记忆中的利益（max_aQ(S', a)）

记忆中的利益：它是指小鸟记忆里下一个状态S'的动作中效用值的最大值

如果小鸟之前在下一个状态S'的某个动作上吃过甜头（选择了某个动作之后获得了50的奖赏），那么它就更希望提早地得知这个消息，以便下回在状态S可以通过选择正确的动作继续进入这个吃甜头的状态S'。

可以看出，γ越大，小鸟就会越重视以往经验，γ越小，小鸟只重视眼前利益

# q-learning 的演示例子
https://github.com/JasonQSY/ML-Weekly/blob/master/P5-Reinforcement-Learning/Q-learning/Q-Learning-Get-Started.ipynb

# q 和 v 学习 的区别
q(s,a) 的需要学习的参数空间更大

v(s), 750个脓毒症状态，需要学习 750个value

q(s,a)， 750个脓毒症状态 * 5种药 * 5种剂量，需要学习 750 * 5 * 5 = 18759个q value

# google trfl 强化学习库
https://github.com/deepmind/trfl/blob/master/docs/index.md

http://incompleteideas.net/book/ebook/node65.html

# Jacobi迭代
《数值分析》: https://www.jianshu.com/p/b3c6f9310578

理解Jacobi迭代为什么是同步迭代了，因为所有的维度的x必须全部更新完成之后才可以进行下一步的迭代

**求解Ax = b的基本迭代法**<br>
$$ x^(k+1)=r + gamma * x^k $$ <br>
收敛的充分条件为: <br>
折扣因子小于1, 就可以收敛 <br>
$$ ||gamma|| < 1$$ <br>
其中：<br>
$$ || \dot || $$为任意一种矩阵范数 

严格对角占优是指对角线上的元素的绝对值比相同行其他元素的绝对值的和都大，这里不存在等号的条件

弱对角占优是严格对角占优的基础上添加等号的条件,也就是说对角线上的元素的绝对值大于等于相同行其他元素的绝对值的和

对于严格对角占优矩阵和弱对角占优矩阵，我们使用任意的初始向量，构造Jacobi迭代格式或者Gauss-Seidel迭代格式，结果均收敛

**Jacobi迭代求方程组**<br>
4x - 2y + z = 9 <br>
2x + 8y - 3z = 19<br>
x + 2y - 5z = 2

可以写成形式:<br>
x =（9 + 2y - z ）/ 4 <br>
y =（19 - 2x + 3z ）/ 8<br>
z =（x + 2y - 2 ）/ 5<br>

Jacobi迭代过程:<br>
x_k1 =（9 + 2 yķ- zķ）/ 4 <br>
ÿ_k1 =（19 - 2 xķ+ 3 zķ）/ 8 <br>
ž_k1 =（xķ+ 2 yķ- 2 ）/ 5<br>
http://www.cfm.brown.edu/people/dobrush/am34/Matlab/seidel.html

# q-learning 源码示例

![avatar](/images/RL-learning/mdp-20.png)

```
# http://mnemstudio.org/path-finding-q-learning-tutorial.htm
# https://github.com/JasonQSY/ML-Weekly/blob/master/P5-Reinforcement-Learning/Q-learning/Q-Learning-Get-Started.ipynb
import numpy as np

# initial
# q is the tabular representation for q func.
q = np.matrix(np.zeros([6, 6]))

# r is the tabular representation for rewards.
# r is predefined and remains unchanged.
r = np.matrix([[-1, -1, -1, -1,  0,  -1], 
               [-1, -1, -1,  0, -1, 100], 
               [-1, -1, -1,  0, -1,  -1], 
               [-1,  0,  0, -1,  0,  -1], 
               [ 0, -1, -1,  0, -1, 100], 
               [-1,  0, -1, -1,  0, 100]])

# hyperparameter
gamma = 0.8
epsilon = 0.4

```

## 训练部分
```

# (状态, 动作)对的价值q函数
# q 是 有待求解的2d数组， 贯穿于整个：训练迭代+预测 的流程之中
q = np.matrix(np.zeros([6, 6]))

# the main training loop
for episode in range(20):
    # random initial state

    # 随机选一个开始状态，一定要走到出口(打通关)
    state = np.random.randint(0, 6)
    print("random state=", state)

    #一定要走到底，打通关， 获得最终回报后，
    # 才能把回报折现到每个状态，然后才能再新开一局
    while (state != 5): # stop only when state == TERMINAL
        # Filter feasible actions.
        # Even in random case, we cannot choose actions whose r[state, action] = -1.
        # It's not about reinforcement learning. These actions are not feasible physically.
        possible_actions = []
        possible_q = []

        state_action_history=[]

        # 当前状态的视野
        # 遍历当前状态的所有动作
        for action in range(6):
            # 可以走通
            if r[state, action] >= 0:
                # 一维数组 当前状态的可行动作
                possible_actions.append(action)

                # 一维数组 当前状态的q值
                possible_q.append(q[state, action])

        ''' 
        print("state=", state,"possible_actions=", end="")

        # 显示可行的动作
        for pa in possible_actions:
            print(pa, end=" ")
        print()    

        '''    

        # Step next state, here we use epsilon-greedy algorithm.
        action = -1
        if np.random.random() < epsilon:
            # choose random action
            # 随机摸瞎探索 激进派
            action = possible_actions[np.random.randint(0, len(possible_actions))]
        else:
            # greedy 保守派
            # 随大流的最优解
            action = possible_actions[np.argmax(possible_q)]

        # Update Q value
        # 立即回报 + 折现的进阶 q[action] 回报
        # q[action] 的动作action的结果， 本例子是进阶到下一个状态，== q[next_state]
        # 进阶后的最佳出路q[action].max()，有通关的可能性
        # 进阶后的最佳出路, 是根据以往迭代历史计算出来的， 总结了前辈的经验
        # 虽然是别人家孩子曾走出的最佳路线，但也有可能指点迷津，帮助通关，有参考价值
        # 最佳出路q，用gamma折现到本次(state, action)的q
        # 兼顾眼前利益和长远利益
        q[state, action] = r[state, action] + gamma * q[action].max()

        print("s=", state,"a=", action, "r=", r[state, action], 
              "max=", q[action], "=", q[action].max(), "q=", q[state,action])

        # Go to the next state
        # action : 进入到哪间房子
        # state: 更新下一个状态， 是哪间房子
        state = action


    # Display training progress
    # if episode % 10 == 0:
    if episode % 10 >= 0:    
        print("Training episode: %d" % episode)
        print(q)

    print()    
```

## 迭代结果
```
random state= 3
s= 3 a= 2 r= 0 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 0.0
s= 2 a= 3 r= 0 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 0.0
s= 3 a= 1 r= 0 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 0.0
s= 1 a= 3 r= 0 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 0.0
s= 3 a= 1 r= 0 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 0.0
s= 1 a= 5 r= 100 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 100.0
Training episode: 0
[[  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0. 100.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]]

random state= 3
s= 3 a= 2 r= 0 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 0.0
s= 2 a= 3 r= 0 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 0.0
s= 3 a= 1 r= 0 max= [[  0.   0.   0.   0.   0. 100.]] = 100.0 q= 80.0
s= 1 a= 3 r= 0 max= [[ 0. 80.  0.  0.  0.  0.]] = 80.0 q= 64.0
s= 3 a= 1 r= 0 max= [[  0.   0.   0.  64.   0. 100.]] = 100.0 q= 80.0
s= 1 a= 3 r= 0 max= [[ 0. 80.  0.  0.  0.  0.]] = 80.0 q= 64.0
s= 3 a= 1 r= 0 max= [[  0.   0.   0.  64.   0. 100.]] = 100.0 q= 80.0
s= 1 a= 5 r= 100 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 100.0
Training episode: 1
[[  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.  64.   0. 100.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.  80.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]]

random state= 1
s= 1 a= 5 r= 100 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 100.0
Training episode: 2
[[  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.  64.   0. 100.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.  80.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]]

random state= 0
s= 0 a= 4 r= 0 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 0.0
s= 4 a= 3 r= 0 max= [[ 0. 80.  0.  0.  0.  0.]] = 80.0 q= 64.0
s= 3 a= 1 r= 0 max= [[  0.   0.   0.  64.   0. 100.]] = 100.0 q= 80.0
s= 1 a= 5 r= 100 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 100.0
Training episode: 3
[[  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.  64.   0. 100.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.  80.   0.   0.   0.   0.]
 [  0.   0.   0.  64.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]]

random state= 5
Training episode: 4
[[  0.   0.   0.   0.   0.   0.]
 [  0.   0.   0.  64.   0. 100.]
 [  0.   0.   0.   0.   0.   0.]
 [  0.  80.   0.   0.   0.   0.]
 [  0.   0.   0.  64.   0.   0.]
 [  0.   0.   0.   0.   0.   0.]]

### 快结束时的迭代结果

 random state= 2
 s= 2 a= 3 r= 0 max= [[ 0.  80.  51.2  0.  51.2  0. ]] = 80.0 q= 64.0
 s= 3 a= 1 r= 0 max= [[  0.   0.   0.  64.   0. 100.]] = 100.0 q= 80.0
 s= 1 a= 3 r= 0 max= [[ 0.  80.  51.2  0.  51.2  0. ]] = 80.0 q= 64.0
 s= 3 a= 1 r= 0 max= [[  0.   0.   0.  64.   0. 100.]] = 100.0 q= 80.0
 s= 1 a= 5 r= 100 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 100.0
 Training episode: 17
 [[  0.    0.    0.    0.   51.2   0. ]
  [  0.    0.    0.   64.    0.  100. ]
  [  0.    0.    0.   64.    0.    0. ]
  [  0.   80.   51.2   0.   51.2   0. ]
  [  0.    0.    0.   64.    0.    0. ]
  [  0.    0.    0.    0.    0.    0. ]]

 random state= 3
 s= 3 a= 1 r= 0 max= [[  0.   0.   0.  64.   0. 100.]] = 100.0 q= 80.0
 s= 1 a= 5 r= 100 max= [[0. 0. 0. 0. 0. 0.]] = 0.0 q= 100.0
 Training episode: 18
 [[  0.    0.    0.    0.   51.2   0. ]
  [  0.    0.    0.   64.    0.  100. ]
  [  0.    0.    0.   64.    0.    0. ]
  [  0.   80.   51.2   0.   51.2   0. ]
  [  0.    0.    0.   64.    0.    0. ]
  [  0.    0.    0.    0.    0.    0. ]]

 random state= 5
 Training episode: 19
 [[  0.    0.    0.    0.   51.2   0. ]
  [  0.    0.    0.   64.    0.  100. ]
  [  0.    0.    0.   64.    0.    0. ]
  [  0.   80.   51.2   0.   51.2   0. ]
  [  0.    0.    0.   64.    0.    0. ]
  [  0.    0.    0.    0.    0.    0. ]]
```

## 验证（预测下一步的行动)
```
# verify
for i in range(6):
    # one episode
    print("episode: %d" % i) 

    # random initial state
    state = i #np.random.randint(0, 6)
    print("the robot borns in " + str(state) + ".")
    for _ in range(20): # prevent endless loop
        if state == 5:
            break
        action = np.argmax(q[state])
        print("the robot goes to %d." % action)
        state = action
```
